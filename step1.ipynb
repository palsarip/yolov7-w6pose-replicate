{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "647f0433-a684-4278-af6f-f708133c0a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\yolov7\\models\\experimental.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(w, map_location=map_location)  # load\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\anaconda3\\envs\\yolov7_env\\lib\\site-packages\\torch\\functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3596.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 6, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 91\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Check if current frame is within fall annotation\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ann \u001b[38;5;129;01min\u001b[39;00m annotations:\n\u001b[1;32m---> 91\u001b[0m     start_frame, end_frame, height, width, center_x, center_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, ann\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit())\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m start_frame \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m frame_count \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m end_frame:\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;66;03m# Apply fall detection logic\u001b[39;00m\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m detections \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 6, got 1)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from models.experimental import attempt_load\n",
    "from utils.general import non_max_suppression, scale_coords\n",
    "from utils.datasets import letterbox\n",
    "from utils.plots import plot_one_box\n",
    "\n",
    "# Load YOLOv7 model\n",
    "weights = 'yolov7-w6-pose.pt'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = attempt_load(weights, map_location=device)\n",
    "\n",
    "# Function to process each frame\n",
    "def process_frame(frame, model):\n",
    "    # Resize and normalize frame\n",
    "    img = letterbox(frame, 640, stride=64, auto=True)[0]\n",
    "    img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB\n",
    "    img = np.ascontiguousarray(img)\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    img = img.float() / 255.0  # Normalize\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        pred = model(img)[0]\n",
    "    pred = non_max_suppression(pred, 0.25, 0.45)\n",
    "\n",
    "    # Process detections\n",
    "    for det in pred:\n",
    "        if det is not None and len(det):\n",
    "            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], frame.shape).round()\n",
    "            for *xyxy, conf, cls in det:\n",
    "                plot_one_box(xyxy, frame, label=f'{model.names[int(cls)]} {conf:.2f}', color=(255, 0, 0))\n",
    "\n",
    "    return frame, det\n",
    "\n",
    "# Function to calculate Euclidean distance\n",
    "def euclidean_distance(p1, p2):\n",
    "    return np.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)\n",
    "\n",
    "# Function to detect fall\n",
    "def detect_fall(keypoints):\n",
    "    # Extract keypoints\n",
    "    left_shoulder = keypoints[5]\n",
    "    right_shoulder = keypoints[6]\n",
    "    left_hip = keypoints[11]\n",
    "    right_hip = keypoints[12]\n",
    "    left_foot = keypoints[15]\n",
    "    right_foot = keypoints[16]\n",
    "\n",
    "    # Calculate length factor\n",
    "    length_factor = euclidean_distance(left_shoulder, left_hip)\n",
    "\n",
    "    # Check shoulder height relative to feet\n",
    "    if left_shoulder[1] <= left_foot[1] + 0.1 * length_factor:\n",
    "        return True\n",
    "\n",
    "    # Check body dimensions\n",
    "    body_height = abs(left_shoulder[1] - left_foot[1])\n",
    "    body_width = abs(left_shoulder[0] - right_shoulder[0])\n",
    "    if body_height < body_width:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Read video and annotation\n",
    "video_path = 'C:/Users/LENOVO/Documents/A Skripsi/datasets/FallDataset/Dataset/Coffee_room_01/Videos/video (1).avi'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "annotation_path = 'C:/Users/LENOVO/Documents/A Skripsi/datasets/FallDataset/Dataset/Coffee_room_01/Annotation_files/video (1).txt'\n",
    "with open(annotation_path, 'r') as f:\n",
    "    annotations = f.readlines()\n",
    "\n",
    "# Get video FPS and calculate delay between frames\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "desired_fps = 25\n",
    "delay = int(1000 / desired_fps)  # Delay in milliseconds\n",
    "\n",
    "frame_count = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    processed_frame, detections = process_frame(frame, model)\n",
    "\n",
    "    # Check if current frame is within fall annotation\n",
    "    for ann in annotations:\n",
    "        start_frame, end_frame, height, width, center_x, center_y = map(int, ann.strip().split())\n",
    "        if start_frame <= frame_count <= end_frame:\n",
    "            # Apply fall detection logic\n",
    "            if detections is not None:\n",
    "                for det in detections:\n",
    "                    keypoints = det[5:].reshape(-1, 2)\n",
    "                    if detect_fall(keypoints):\n",
    "                        print(f\"Fall detected in frame {frame_count}\")\n",
    "\n",
    "    cv2.imshow('Frame', processed_frame)\n",
    "    if cv2.waitKey(delay) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b11f2bb4-4486-422e-b4df-e5e8810c3679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for dimension 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 79\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m det \u001b[38;5;129;01min\u001b[39;00m detections:\n\u001b[0;32m     78\u001b[0m         keypoints \u001b[38;5;241m=\u001b[39m det[\u001b[38;5;241m5\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdetect_fall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeypoints\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     80\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFall detected in frame \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrame\u001b[39m\u001b[38;5;124m'\u001b[39m, processed_frame)\n",
      "Cell \u001b[1;32mIn[3], line 42\u001b[0m, in \u001b[0;36mdetect_fall\u001b[1;34m(keypoints)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect_fall\u001b[39m(keypoints):\n\u001b[1;32m---> 42\u001b[0m     left_shoulder \u001b[38;5;241m=\u001b[39m \u001b[43mkeypoints\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     43\u001b[0m     right_shoulder \u001b[38;5;241m=\u001b[39m keypoints[\u001b[38;5;241m6\u001b[39m]\n\u001b[0;32m     44\u001b[0m     left_hip \u001b[38;5;241m=\u001b[39m keypoints[\u001b[38;5;241m11\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: index 5 is out of bounds for dimension 0 with size 0"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from models.experimental import attempt_load\n",
    "from utils.general import non_max_suppression, scale_coords\n",
    "from utils.datasets import letterbox\n",
    "from utils.plots import plot_one_box\n",
    "\n",
    "# Load YOLOv7 model\n",
    "weights = 'yolov7-w6-pose.pt'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = attempt_load(weights, map_location=device)\n",
    "\n",
    "# Function to process each frame\n",
    "def process_frame(frame, model):\n",
    "    img = letterbox(frame, 640, stride=64, auto=True)[0]\n",
    "    img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB\n",
    "    img = np.ascontiguousarray(img)\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    img = img.float() / 255.0  # Normalize\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(img)[0]\n",
    "    pred = non_max_suppression(pred, 0.25, 0.45)\n",
    "\n",
    "    for det in pred:\n",
    "        if det is not None and len(det):\n",
    "            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], frame.shape).round()\n",
    "            for *xyxy, conf, cls in det:\n",
    "                plot_one_box(xyxy, frame, label=f'{model.names[int(cls)]} {conf:.2f}', color=(255, 0, 0))\n",
    "\n",
    "    return frame, pred\n",
    "\n",
    "# Function to calculate Euclidean distance\n",
    "def euclidean_distance(p1, p2):\n",
    "    return np.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)\n",
    "\n",
    "# Function to detect fall\n",
    "def detect_fall(keypoints):\n",
    "    left_shoulder = keypoints[5]\n",
    "    right_shoulder = keypoints[6]\n",
    "    left_hip = keypoints[11]\n",
    "    right_hip = keypoints[12]\n",
    "    left_foot = keypoints[15]\n",
    "    right_foot = keypoints[16]\n",
    "    \n",
    "    length_factor = euclidean_distance(left_shoulder, left_hip)\n",
    "    if left_shoulder[1] <= left_foot[1] + 0.1 * length_factor:\n",
    "        return True\n",
    "    \n",
    "    body_height = abs(left_shoulder[1] - left_foot[1])\n",
    "    body_width = abs(left_shoulder[0] - right_shoulder[0])\n",
    "    if body_height < body_width:\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Read video\n",
    "video_path = 'C:/Users/LENOVO/Documents/A Skripsi/datasets/FallDataset/Dataset/Coffee_room_01/Videos/video (1).avi'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "frame_interval = fps // 25  # Process every 25 fps\n",
    "\n",
    "frame_count = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    if frame_count % frame_interval == 0:\n",
    "        processed_frame, detections = process_frame(frame, model)\n",
    "        if detections is not None:\n",
    "            for det in detections:\n",
    "                keypoints = det[5:].reshape(-1, 2)\n",
    "                if detect_fall(keypoints):\n",
    "                    print(f\"Fall detected in frame {frame_count}\")\n",
    "        \n",
    "        cv2.imshow('Frame', processed_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6800d1c1-831f-49ae-990c-c575dd4bb065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\anaconda3\\envs\\yolov7_env\\lib\\site-packages\\torch\\functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from models.experimental import attempt_load\n",
    "from utils.general import non_max_suppression, scale_coords\n",
    "from utils.datasets import letterbox\n",
    "from utils.plots import plot_one_box\n",
    "\n",
    "# Load YOLOv7 model\n",
    "weights = 'yolov7-w6-pose.pt'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = attempt_load(weights, map_location=device)\n",
    "\n",
    "# Function to process each frame\n",
    "def process_frame(frame, model):\n",
    "    # Resize and normalize frame\n",
    "    img = letterbox(frame, 640, stride=64, auto=True)[0]\n",
    "    img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB\n",
    "    img = np.ascontiguousarray(img)\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    img = img.float() / 255.0  # Normalize\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        pred = model(img)[0]\n",
    "    pred = non_max_suppression(pred, 0.25, 0.45)\n",
    "\n",
    "    # Process detections\n",
    "    detections = []\n",
    "    for det in pred:\n",
    "        if det is not None and len(det):\n",
    "            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], frame.shape).round()\n",
    "            for *xyxy, conf, cls, kpts in det:\n",
    "                if int(cls) < len(model.names):  # Check if class index is valid\n",
    "                    label = f'{model.names[int(cls)]} {conf:.2f}'\n",
    "                    plot_one_box(xyxy, frame, label=label, color=(255, 0, 0))\n",
    "                    detections.append(det.cpu().numpy())\n",
    "    \n",
    "    return frame, detections if detections else None\n",
    "\n",
    "# Function to calculate Euclidean distance\n",
    "def euclidean_distance(p1, p2):\n",
    "    return np.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)\n",
    "\n",
    "# Function to detect fall\n",
    "def detect_fall(keypoints):\n",
    "    # Extract keypoints\n",
    "    left_shoulder = keypoints[5]\n",
    "    right_shoulder = keypoints[6]\n",
    "    left_hip = keypoints[11]\n",
    "    right_hip = keypoints[12]\n",
    "    left_foot = keypoints[15]\n",
    "    right_foot = keypoints[16]\n",
    "\n",
    "    # Calculate length factor\n",
    "    length_factor = euclidean_distance(left_shoulder, left_hip)\n",
    "\n",
    "    # Check shoulder height relative to feet\n",
    "    if left_shoulder[1] <= left_foot[1] + 0.1 * length_factor:\n",
    "        return True\n",
    "\n",
    "    # Check body dimensions\n",
    "    body_height = abs(left_shoulder[1] - left_foot[1])\n",
    "    body_width = abs(left_shoulder[0] - right_shoulder[0])\n",
    "    if body_height < body_width:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Read video and annotation\n",
    "video_path = 'C:/Users/LENOVO/Documents/A Skripsi/datasets/FallDataset/Dataset/Coffee_room_01/Videos/video (1).avi'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "annotation_path = 'C:/Users/LENOVO/Documents/A Skripsi/datasets/FallDataset/Dataset/Coffee_room_01/Annotation_files/video (1).txt'\n",
    "\n",
    "# Read annotations and filter invalid lines\n",
    "annotations = []\n",
    "with open(annotation_path, 'r') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 6:  # Ensure there are exactly 6 values\n",
    "            annotations.append(list(map(int, parts)))\n",
    "\n",
    "# Get video FPS and calculate delay between frames\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "desired_fps = 25\n",
    "delay = int(1000 / desired_fps)  # Delay in milliseconds\n",
    "\n",
    "frame_count = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    processed_frame, detections = process_frame(frame, model)\n",
    "\n",
    "    # Check if current frame is within fall annotation\n",
    "    for ann in annotations:\n",
    "        start_frame, end_frame, height, width, center_x, center_y = ann\n",
    "        if start_frame <= frame_count <= end_frame:\n",
    "            # Apply fall detection logic\n",
    "            if detections is not None:\n",
    "                for det in detections:\n",
    "                    keypoints = det[5:].reshape(-1, 2)\n",
    "                    if detect_fall(keypoints):\n",
    "                        print(f\"Fall detected in frame {frame_count}\")\n",
    "\n",
    "    cv2.imshow('Frame', processed_frame)\n",
    "    if cv2.waitKey(delay) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d35f317-95aa-4983-b31b-1fa2b48d9d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from models.experimental import attempt_load\n",
    "from utils.general import non_max_suppression, scale_coords\n",
    "\n",
    "def main():\n",
    "    # Load YOLOv7 Pose Model\n",
    "    model = attempt_load('yolov7-w6-pose.pt', map_location='cpu')  # Change 'cpu' to 'cuda' if available\n",
    "    stride = int(model.stride.max())  # Model stride\n",
    "\n",
    "    # Video Capture\n",
    "    cap = cv2.VideoCapture('C:/Users/LENOVO/Documents/A Skripsi/datasets/FallDataset/Dataset/Coffee_room_01/Videos/video (1).avi')\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file\")\n",
    "        return\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Preprocess the image\n",
    "        img = cv2.resize(frame, (640, 640))\n",
    "        img = img[:, :, ::-1].transpose(2, 0, 1)  # Convert BGR to RGB and reorder dimensions\n",
    "        img = np.ascontiguousarray(img)\n",
    "        img_tensor = torch.from_numpy(img).float().div(255.0).unsqueeze(0).to('cpu')  # Normalize\n",
    "\n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            pred = model(img_tensor)[0]\n",
    "        \n",
    "        # Apply NMS (Non-Maximum Suppression)\n",
    "        pred = non_max_suppression(pred, 0.25, 0.45, classes=0)  # Filter for 'person' class only\n",
    "        \n",
    "        # Process detections\n",
    "        for det in pred:\n",
    "            if det is not None and len(det):\n",
    "                # Rescale boxes to original image size\n",
    "                det[:, :4] = scale_coords(img_tensor.shape[2:], det[:, :4], frame.shape).round()\n",
    "                \n",
    "                for det_item in det:\n",
    "                    xyxy = det_item[:4]  # Bounding box\n",
    "                    conf = det_item[4]  # Confidence score\n",
    "                    cls = det_item[5]  # Class ID\n",
    "                    kpts = det_item[6:].reshape(-1, 3)  # Keypoints (x, y, conf)\n",
    "\n",
    "                    # Draw keypoints\n",
    "                    for x, y, kp_conf in kpts:\n",
    "                        if kp_conf > 0.5:  # Confidence threshold\n",
    "                            cv2.circle(frame, (int(x), int(y)), 5, (0, 255, 0), -1)\n",
    "\n",
    "                    # Draw bounding box\n",
    "                    cv2.rectangle(frame, \n",
    "                                  (int(xyxy[0]), int(xyxy[1])), \n",
    "                                  (int(xyxy[2]), int(xyxy[3])), \n",
    "                                  (255, 0, 0), 2)\n",
    "\n",
    "        # Display result\n",
    "        cv2.imshow('Pose Detection', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "815c1289-97a6-4a48-aec9-f5e4c95c3cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from models.experimental import attempt_load\n",
    "from utils.general import non_max_suppression, scale_coords\n",
    "\n",
    "def main():\n",
    "    # Load YOLOv7 Pose Model\n",
    "    model = attempt_load('yolov7-w6-pose.pt', map_location='cpu')  # Change 'cpu' to 'cuda' if available\n",
    "    stride = int(model.stride.max())  # Model stride\n",
    "\n",
    "    # Video Capture\n",
    "    cap = cv2.VideoCapture('C:/Users/LENOVO/Documents/A Skripsi/datasets/FallDataset/Dataset/Coffee_room_01/Videos/video (1).avi')\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file\")\n",
    "        return\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Preprocess the image\n",
    "        img = cv2.resize(frame, (640, 640))\n",
    "        img = img[:, :, ::-1].transpose(2, 0, 1)  # Convert BGR to RGB and reorder dimensions\n",
    "        img = np.ascontiguousarray(img)\n",
    "        img_tensor = torch.from_numpy(img).float().div(255.0).unsqueeze(0).to('cpu')  # Normalize\n",
    "\n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            pred = model(img_tensor)[0]\n",
    "        \n",
    "        # Apply NMS (Non-Maximum Suppression)\n",
    "        pred = non_max_suppression(pred, 0.25, 0.45, classes=0)  # Filter for 'person' class only\n",
    "        \n",
    "        # Process detections\n",
    "        for det in pred:\n",
    "            if det is not None and len(det):\n",
    "                # Rescale boxes to original image size\n",
    "                det[:, :4] = scale_coords(img_tensor.shape[2:], det[:, :4], frame.shape).round()\n",
    "                \n",
    "                for det_item in det:\n",
    "                    xyxy = det_item[:4]  # Bounding box\n",
    "                    conf = det_item[4]  # Confidence score\n",
    "                    cls = det_item[5]  # Class ID\n",
    "                    kpts = det_item[6:].reshape(-1, 3)  # Keypoints (x, y, conf)\n",
    "\n",
    "                    # Draw keypoints\n",
    "                    for x, y, kp_conf in kpts:\n",
    "                        if kp_conf > 0.5:  # Confidence threshold\n",
    "                            cv2.circle(frame, (int(x), int(y)), 5, (0, 255, 0), -1)\n",
    "\n",
    "                    # Draw bounding box\n",
    "                    cv2.rectangle(frame, \n",
    "                                  (int(xyxy[0]), int(xyxy[1])), \n",
    "                                  (int(xyxy[2]), int(xyxy[3])), \n",
    "                                  (255, 0, 0), 2)\n",
    "\n",
    "        # Display result\n",
    "        cv2.imshow('Pose Detection', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac36e712-3138-4416-a60b-8e47d11811bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
