{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eaea521-cbcf-4c71-84cd-49d97367987a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 124\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(label_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    123\u001b[0m     label_content \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadline()\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m--> 124\u001b[0m     ground_truth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[43mlabel_content\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)  \u001b[38;5;66;03m# 0 = Fall, 1 = No Fall\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# Preprocess the image\u001b[39;00m\n\u001b[0;32m    127\u001b[0m image \u001b[38;5;241m=\u001b[39m letterbox(image, \u001b[38;5;241m960\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, auto\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "from models.yolo import Model\n",
    "from PIL import Image, ImageEnhance\n",
    "import random\n",
    "\n",
    "# Add the custom class to the safe globals list\n",
    "torch.serialization.add_safe_globals([Model])\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load YOLOv7-pose model\n",
    "weights = torch.load('yolov7-w6-pose.pt', map_location=device, weights_only=False)\n",
    "model = weights['model']\n",
    "_ = model.float().eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.half().to(device)\n",
    "\n",
    "def augment_image(image):\n",
    "    \"\"\"\n",
    "    Apply random augmentation to the image (brightness, contrast, noise, etc.).\n",
    "    :param image: Input image (numpy array in BGR format).\n",
    "    :return: Augmented image (numpy array in BGR format).\n",
    "    \"\"\"\n",
    "    # Convert to PIL Image for easier augmentation (PIL uses RGB format)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "    image = Image.fromarray(image)\n",
    "\n",
    "    # Random brightness adjustment\n",
    "    brightness_factor = random.uniform(0.8, 1.2)  # Adjust brightness randomly\n",
    "    enhancer = ImageEnhance.Brightness(image)\n",
    "    image = enhancer.enhance(brightness_factor)\n",
    "\n",
    "    # Random contrast adjustment\n",
    "    contrast_factor = random.uniform(0.8, 1.2)  # Adjust contrast randomly\n",
    "    enhancer = ImageEnhance.Contrast(image)\n",
    "    image = enhancer.enhance(contrast_factor)\n",
    "\n",
    "    # Convert back to numpy array (RGB format)\n",
    "    image = np.array(image)\n",
    "\n",
    "    # Add Gaussian noise\n",
    "    mean = 0\n",
    "    var = random.uniform(0, 0.005)  # Random noise variance (reduced to avoid extreme changes)\n",
    "    sigma = var ** 0.5\n",
    "    gaussian = np.random.normal(mean, sigma, image.shape).reshape(image.shape)\n",
    "    image = image + gaussian * 255\n",
    "    image = np.clip(image, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # Convert back to BGR format for OpenCV\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image\n",
    "\n",
    "def apply_clahe(image):\n",
    "    \"\"\"\n",
    "    Apply CLAHE to the image to enhance contrast.\n",
    "    :param image: Input image (numpy array in BGR format).\n",
    "    :return: Image with CLAHE applied (numpy array in BGR format).\n",
    "    \"\"\"\n",
    "    # Convert to LAB color space\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "\n",
    "    # Apply CLAHE to the L channel\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    l_clahe = clahe.apply(l)\n",
    "\n",
    "    # Merge the channels back\n",
    "    lab_clahe = cv2.merge((l_clahe, a, b))\n",
    "    image_clahe = cv2.cvtColor(lab_clahe, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    return image_clahe\n",
    "\n",
    "def preprocess_image(image, apply_augmentation=True, apply_clahe_flag=True):\n",
    "    \"\"\"\n",
    "    Preprocess the image with augmentation and CLAHE.\n",
    "    :param image: Input image (numpy array in BGR format).\n",
    "    :param apply_augmentation: Whether to apply augmentation.\n",
    "    :param apply_clahe_flag: Whether to apply CLAHE.\n",
    "    :return: Preprocessed image (numpy array in BGR format).\n",
    "    \"\"\"\n",
    "    if apply_augmentation:\n",
    "        image = augment_image(image)\n",
    "\n",
    "    if apply_clahe_flag:\n",
    "        image = apply_clahe(image)\n",
    "\n",
    "    return image\n",
    "\n",
    "def detect_fall(keypoints, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Detect fall based on keypoints.\n",
    "    :param keypoints: Array of keypoints (17 keypoints, each with x, y, confidence).\n",
    "    :param threshold: Confidence threshold for keypoints.\n",
    "    :return: True if fall is detected, False otherwise.\n",
    "    \"\"\"\n",
    "    # Indices for keypoints (COCO format)\n",
    "    LEFT_SHOULDER = 5\n",
    "    RIGHT_SHOULDER = 6\n",
    "    LEFT_HIP = 11\n",
    "    RIGHT_HIP = 12\n",
    "    LEFT_KNEE = 13\n",
    "    RIGHT_KNEE = 14\n",
    "\n",
    "    # Get keypoints and confidence scores\n",
    "    left_shoulder = keypoints[LEFT_SHOULDER * 3: (LEFT_SHOULDER + 1) * 3]\n",
    "    right_shoulder = keypoints[RIGHT_SHOULDER * 3: (RIGHT_SHOULDER + 1) * 3]\n",
    "    left_hip = keypoints[LEFT_HIP * 3: (LEFT_HIP + 1) * 3]\n",
    "    right_hip = keypoints[RIGHT_HIP * 3: (RIGHT_HIP + 1) * 3]\n",
    "    left_knee = keypoints[LEFT_KNEE * 3: (LEFT_KNEE + 1) * 3]\n",
    "    right_knee = keypoints[RIGHT_KNEE * 3: (RIGHT_KNEE + 1) * 3]\n",
    "\n",
    "    # Check confidence scores\n",
    "    if (left_shoulder[2] < threshold or right_shoulder[2] < threshold or\n",
    "        left_hip[2] < threshold or right_hip[2] < threshold or\n",
    "        left_knee[2] < threshold or right_knee[2] < threshold):\n",
    "        return False  # Skip if any keypoint is not confident\n",
    "\n",
    "    # Calculate average y positions\n",
    "    shoulder_y = (left_shoulder[1] + right_shoulder[1]) / 2\n",
    "    hip_y = (left_hip[1] + right_hip[1]) / 2\n",
    "    knee_y = (left_knee[1] + right_knee[1]) / 2\n",
    "\n",
    "    # Check if hip and knee are below shoulders (fall condition)\n",
    "    if hip_y > shoulder_y and knee_y > shoulder_y:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def calculate_metrics(ground_truth, predictions):\n",
    "    \"\"\"\n",
    "    Calculate precision, recall, F1-score, and accuracy manually.\n",
    "    :param ground_truth: List of ground truth labels (1 = Fall, 0 = No Fall).\n",
    "    :param predictions: List of predicted labels (1 = Fall, 0 = No Fall).\n",
    "    :return: Precision, recall, F1-score, and accuracy.\n",
    "    \"\"\"\n",
    "    TP = 0  # True Positives\n",
    "    FP = 0  # False Positives\n",
    "    TN = 0  # True Negatives\n",
    "    FN = 0  # False Negatives\n",
    "\n",
    "    for gt, pred in zip(ground_truth, predictions):\n",
    "        if gt == 1 and pred == 1:\n",
    "            TP += 1\n",
    "        elif gt == 0 and pred == 1:\n",
    "            FP += 1\n",
    "        elif gt == 0 and pred == 0:\n",
    "            TN += 1\n",
    "        elif gt == 1 and pred == 0:\n",
    "            FN += 1\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1_score, accuracy\n",
    "\n",
    "# Real-time fall detection\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Lists to store ground truth and predictions\n",
    "ground_truth_list = []\n",
    "predictions_list = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocess the frame\n",
    "    preprocessed_frame = preprocess_image(frame, apply_augmentation=True, apply_clahe_flag=True)\n",
    "\n",
    "    # Resize and normalize the frame for YOLOv7-pose\n",
    "    image = letterbox(preprocessed_frame, 960, stride=64, auto=True)[0]\n",
    "    image = transforms.ToTensor()(image)\n",
    "    image = torch.tensor(np.array([image.numpy()]))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        image = image.half().to(device)\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        output, _ = model(image)\n",
    "        output = non_max_suppression_kpt(output, 0.25, 0.65, nc=model.yaml['nc'], nkpt=model.yaml['nkpt'], kpt_label=True)\n",
    "        output = output_to_keypoint(output)\n",
    "\n",
    "    nimg = image[0].permute(1, 2, 0) * 255\n",
    "    nimg = nimg.cpu().numpy().astype(np.uint8)\n",
    "    nimg = cv2.cvtColor(nimg, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    fall_detected = False\n",
    "    for idx in range(output.shape[0]):\n",
    "        keypoints = output[idx, 7:].T\n",
    "        plot_skeleton_kpts(nimg, keypoints, 3)\n",
    "\n",
    "        # Detect fall\n",
    "        if detect_fall(keypoints):\n",
    "            fall_detected = True\n",
    "            cv2.putText(nimg, \"Fall Detected!\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    # Save prediction\n",
    "    predictions_list.append(1 if fall_detected else 0)\n",
    "\n",
    "    # Display instructions for manual ground truth input\n",
    "    cv2.putText(nimg, \"Press 'f' for Fall, 'n' for No Fall, 'q' to Quit\", (50, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "\n",
    "    # Display the result\n",
    "    cv2.imshow(\"Fall Detection\", nimg)\n",
    "\n",
    "    # Wait for key press to input ground truth\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('f'):  # Fall\n",
    "        ground_truth_list.append(1)\n",
    "    elif key == ord('n'):  # No Fall\n",
    "        ground_truth_list.append(0)\n",
    "    elif key == ord('q'):  # Quit\n",
    "        break\n",
    "\n",
    "    # Calculate metrics if ground truth is available\n",
    "    if len(ground_truth_list) == len(predictions_list):\n",
    "        precision, recall, f1_score, accuracy = calculate_metrics(ground_truth_list, predictions_list)\n",
    "\n",
    "        # Display metrics on the frame\n",
    "        cv2.putText(nimg, f\"Precision: {precision:.2f}\", (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        cv2.putText(nimg, f\"Recall: {recall:.2f}\", (50, 130), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        cv2.putText(nimg, f\"F1-score: {f1_score:.2f}\", (50, 160), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        cv2.putText(nimg, f\"Accuracy: {accuracy:.2f}\", (50, 190), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "    # Clear GPU cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Print final metrics\n",
    "if len(ground_truth_list) == len(predictions_list):\n",
    "    print(\"Final Metrics:\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1-score: {f1_score:.2f}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae332188-226a-4e8e-bf91-87b29395e952",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
