{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa3c1e9a-e650-48c2-bbcc-717e4b451032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\anaconda3\\envs\\yolov7_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.serialization' has no attribute 'add_safe_globals'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01myolo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Add the custom class to the safe globals list\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_safe_globals\u001b[49m([Model])\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Initialize device\u001b[39;00m\n\u001b[0;32m     14\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch.serialization' has no attribute 'add_safe_globals'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "from models.yolo import Model\n",
    "\n",
    "# Add the custom class to the safe globals list\n",
    "torch.serialization.add_safe_globals([Model])\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load YOLOv7-pose model\n",
    "weights = torch.load('yolov7-w6-pose.pt', map_location=device, weights_only=False)\n",
    "model = weights['model']\n",
    "_ = model.float().eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.half().to(device)\n",
    "\n",
    "def calculate_length_factor(shoulder, torso):\n",
    "\n",
    "    # Calculate the length factor using Euclidean distance.\n",
    "    \n",
    "    return np.sqrt((shoulder[0] - torso[0])**2 + (shoulder[1] - torso[1])**2)\n",
    "\n",
    "def detect_fall(keypoints, threshold=0.5, alpha=0.5):\n",
    "    \n",
    "    # Indices for keypoints (COCO format)\n",
    "    LEFT_SHOULDER = 5\n",
    "    RIGHT_SHOULDER = 6\n",
    "    LEFT_HIP = 11\n",
    "    RIGHT_HIP = 12\n",
    "    LEFT_ANKLE = 13\n",
    "    RIGHT_ANKLE = 14\n",
    "\n",
    "    # Get keypoints and confidence scores\n",
    "    left_shoulder = keypoints[LEFT_SHOULDER * 3: (LEFT_SHOULDER + 1) * 3]\n",
    "    right_shoulder = keypoints[RIGHT_SHOULDER * 3: (RIGHT_SHOULDER + 1) * 3]\n",
    "    left_hip = keypoints[LEFT_HIP * 3: (LEFT_HIP + 1) * 3]\n",
    "    right_hip = keypoints[RIGHT_HIP * 3: (RIGHT_HIP + 1) * 3]\n",
    "    left_ankle = keypoints[LEFT_ANKLE * 3: (LEFT_ANKLE + 1) * 3]\n",
    "    right_ankle = keypoints[RIGHT_ANKLE * 3: (RIGHT_ANKLE + 1) * 3]\n",
    "\n",
    "    # Check confidence scores\n",
    "    if (left_shoulder[2] < threshold or right_shoulder[2] < threshold or\n",
    "        left_hip[2] < threshold or right_hip[2] < threshold or\n",
    "        left_ankle[2] < threshold or right_ankle[2] < threshold):\n",
    "        return False  # Skip if any keypoint is not confident\n",
    "\n",
    "    # Calculate length factor\n",
    "    length_factor = calculate_length_factor(left_shoulder[:2], left_hip[:2])\n",
    "\n",
    "    # Check if shoulders are below ankles (fall condition)\n",
    "    if (left_shoulder[1] <= left_ankle[1] + alpha * length_factor and\n",
    "        right_shoulder[1] <= right_ankle[1] + alpha * length_factor):\n",
    "        # Calculate body dimensions\n",
    "        body_height = abs(left_shoulder[1] - left_ankle[1])\n",
    "        body_width = abs(left_shoulder[0] - right_shoulder[0])\n",
    "\n",
    "        # Check if body height is less than body width\n",
    "        if body_height < body_width:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "# Real-time fall detection\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocess the frame\n",
    "    image = letterbox(frame, 960, stride=64, auto=True)[0]\n",
    "    image_ = image.copy()\n",
    "    image = transforms.ToTensor()(image)\n",
    "    image = torch.tensor(np.array([image.numpy()]))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        image = image.half().to(device)\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        output, _ = model(image)\n",
    "        output = non_max_suppression_kpt(output, 0.25, 0.65, nc=model.yaml['nc'], nkpt=model.yaml['nkpt'], kpt_label=True)\n",
    "        output = output_to_keypoint(output)\n",
    "\n",
    "    nimg = image[0].permute(1, 2, 0) * 255\n",
    "    nimg = nimg.cpu().numpy().astype(np.uint8)\n",
    "    nimg = cv2.cvtColor(nimg, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    for idx in range(output.shape[0]):\n",
    "        keypoints = output[idx, 7:].T\n",
    "        plot_skeleton_kpts(nimg, keypoints, 3)\n",
    "\n",
    "        # Detect fall\n",
    "        if detect_fall(keypoints):\n",
    "            cv2.putText(nimg, \"Fall Detected!\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    # Display the result\n",
    "    cv2.imshow(\"Fall Detection\", nimg)\n",
    "\n",
    "    # Break if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    # Clear GPU cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8bd505e-032b-4840-ae55-597c7b927d6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.serialization' has no attribute 'add_safe_globals'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplots\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m output_to_keypoint, plot_skeleton_kpts\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01myolo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[1;32m---> 11\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_safe_globals\u001b[49m([Model])\n\u001b[0;32m     13\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Load YOLOv7-pose model\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch.serialization' has no attribute 'add_safe_globals'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import time\n",
    "from torchvision import transforms\n",
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "from models.yolo import Model\n",
    "\n",
    "torch.serialization.add_safe_globals([Model])\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load YOLOv7-pose model\n",
    "weights = torch.load('yolov7-w6-pose.pt', map_location=device, weights_only=False)\n",
    "model = weights['model']\n",
    "_ = model.float().eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.half().to(device)\n",
    "\n",
    "def calculate_speed(prev_y, curr_y, time_diff):\n",
    "    return abs(curr_y - prev_y) / time_diff if time_diff > 0 else 0\n",
    "\n",
    "def calculate_angle(a, b, c):\n",
    "    ab = np.array(a) - np.array(b)\n",
    "    bc = np.array(c) - np.array(b)\n",
    "    cos_angle = np.dot(ab, bc) / (np.linalg.norm(ab) * np.linalg.norm(bc) + 1e-6)\n",
    "    angle = np.arccos(np.clip(cos_angle, -1.0, 1.0))\n",
    "    return np.degrees(angle)\n",
    "\n",
    "def detect_fall(prev_positions, keypoints, threshold=0.5, alpha=0.5, speed_thresh=5, angle_thresh=45):\n",
    "    LEFT_SHOULDER, RIGHT_SHOULDER, LEFT_HIP, RIGHT_HIP, LEFT_ANKLE, RIGHT_ANKLE = 5, 6, 11, 12, 13, 14\n",
    "    \n",
    "    # Get keypoints\n",
    "    left_shoulder, right_shoulder = keypoints[LEFT_SHOULDER * 3:(LEFT_SHOULDER + 1) * 3], keypoints[RIGHT_SHOULDER * 3:(RIGHT_SHOULDER + 1) * 3]\n",
    "    left_hip, right_hip = keypoints[LEFT_HIP * 3:(LEFT_HIP + 1) * 3], keypoints[RIGHT_HIP * 3:(RIGHT_HIP + 1) * 3]\n",
    "    left_ankle, right_ankle = keypoints[LEFT_ANKLE * 3:(LEFT_ANKLE + 1) * 3], keypoints[RIGHT_ANKLE * 3:(RIGHT_ANKLE + 1) * 3]\n",
    "\n",
    "    # Confidence check\n",
    "    if any(kp[2] < threshold for kp in [left_shoulder, right_shoulder, left_hip, right_hip, left_ankle, right_ankle]):\n",
    "        return False  \n",
    "    \n",
    "    # Calculate body height and width\n",
    "    body_height = abs(left_shoulder[1] - left_ankle[1])\n",
    "    body_width = abs(left_shoulder[0] - right_shoulder[0])\n",
    "    \n",
    "    # Check if body height < width (lying down condition)\n",
    "    if body_height < body_width:\n",
    "        time_now = time.time()\n",
    "        speed = calculate_speed(prev_positions.get('shoulder_y', left_shoulder[1]), left_shoulder[1], time_now - prev_positions.get('time', time_now))\n",
    "        angle = calculate_angle(left_shoulder[:2], left_hip[:2], left_ankle[:2])\n",
    "        \n",
    "        if speed > speed_thresh and angle < angle_thresh:\n",
    "            prev_positions.update({'shoulder_y': left_shoulder[1], 'time': time_now})\n",
    "            return True\n",
    "    \n",
    "    prev_positions.update({'shoulder_y': left_shoulder[1], 'time': time.time()})\n",
    "    return False\n",
    "\n",
    "# Process video\n",
    "cap = cv2.VideoCapture('fall_video.mp4')  # Replace with video path\n",
    "cap.set(cv2.CAP_PROP_FPS, 25)\n",
    "prev_positions = {}\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    image = letterbox(frame, 960, stride=64, auto=True)[0]\n",
    "    image = transforms.ToTensor()(image)\n",
    "    image = torch.tensor(np.array([image.numpy()]))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        image = image.half().to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output, _ = model(image)\n",
    "        output = non_max_suppression_kpt(output, 0.25, 0.65, nc=model.yaml['nc'], nkpt=model.yaml['nkpt'], kpt_label=True)\n",
    "        output = output_to_keypoint(output)\n",
    "    \n",
    "    nimg = image[0].permute(1, 2, 0) * 255\n",
    "    nimg = nimg.cpu().numpy().astype(np.uint8)\n",
    "    nimg = cv2.cvtColor(nimg, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    for idx in range(output.shape[0]):\n",
    "        keypoints = output[idx, 7:].T\n",
    "        plot_skeleton_kpts(nimg, keypoints, 3)\n",
    "\n",
    "        if detect_fall(prev_positions, keypoints):\n",
    "            cv2.putText(nimg, \"Fall Detected!\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "    \n",
    "    cv2.imshow(\"Fall Detection\", nimg)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd8140c6-5bce-476b-b91a-9c3d835b4349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "from models.experimental import attempt_load\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model\n",
    "model = attempt_load('yolov7-w6-pose.pt', map_location=device)\n",
    "_ = model.float().eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.half().to(device)\n",
    "\n",
    "def calculate_length_factor(shoulder, hip):\n",
    "    return np.sqrt((shoulder[0] - hip[0])**2 + (shoulder[1] - hip[1])**2)\n",
    "\n",
    "def detect_fall(keypoints, threshold=0.5, alpha=0.5):\n",
    "    LEFT_SHOULDER, RIGHT_SHOULDER = 5, 6\n",
    "    LEFT_HIP, RIGHT_HIP = 11, 12\n",
    "    LEFT_ANKLE, RIGHT_ANKLE = 13, 14\n",
    "    \n",
    "    # Extract keypoints with confidence scores\n",
    "    left_shoulder = keypoints[LEFT_SHOULDER * 3: (LEFT_SHOULDER + 1) * 3]\n",
    "    right_shoulder = keypoints[RIGHT_SHOULDER * 3: (RIGHT_SHOULDER + 1) * 3]\n",
    "    left_hip = keypoints[LEFT_HIP * 3: (LEFT_HIP + 1) * 3]\n",
    "    right_hip = keypoints[RIGHT_HIP * 3: (RIGHT_HIP + 1) * 3]\n",
    "    left_ankle = keypoints[LEFT_ANKLE * 3: (LEFT_ANKLE + 1) * 3]\n",
    "    right_ankle = keypoints[RIGHT_ANKLE * 3: (RIGHT_ANKLE + 1) * 3]\n",
    "    \n",
    "    # Check if keypoints are detected with sufficient confidence\n",
    "    if any(kp[2] < threshold for kp in [left_shoulder, right_shoulder, left_hip, right_hip, left_ankle, right_ankle]):\n",
    "        return False\n",
    "    \n",
    "    # Calculate body orientation\n",
    "    shoulder_avg_y = (left_shoulder[1] + right_shoulder[1]) / 2\n",
    "    hip_avg_y = (left_hip[1] + right_hip[1]) / 2\n",
    "    ankle_avg_y = (left_ankle[1] + right_ankle[1]) / 2\n",
    "    \n",
    "    # Calculate body dimensions\n",
    "    torso_length = calculate_length_factor(\n",
    "        [(left_shoulder[0] + right_shoulder[0]) / 2, shoulder_avg_y],\n",
    "        [(left_hip[0] + right_hip[0]) / 2, hip_avg_y]\n",
    "    )\n",
    "    \n",
    "    body_height = abs(shoulder_avg_y - ankle_avg_y)\n",
    "    body_width = abs(left_shoulder[0] - right_shoulder[0])\n",
    "    \n",
    "    # Fall condition 1: Shoulders below hips with significant horizontal orientation\n",
    "    if shoulder_avg_y > hip_avg_y + 0.2 * torso_length:\n",
    "        # Fall condition 2: Body width greater than height\n",
    "        if body_width > body_height * 0.9:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "cap = cv2.VideoCapture('C:/Users/LENOVO/Documents/A Skripsi/datasets/FallDataset/Dataset/Coffee_room_01/Videos/video (8).avi')\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video file\")\n",
    "    exit()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocess image\n",
    "    image = letterbox(frame, 960, stride=64, auto=True)[0]\n",
    "    image = transforms.ToTensor()(image)\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        image = image.half()\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        output, _ = model(image)\n",
    "        output = non_max_suppression_kpt(\n",
    "            output, \n",
    "            0.25,   # Confidence threshold\n",
    "            0.65,   # IoU threshold\n",
    "            nc=model.yaml['nc'], \n",
    "            nkpt=model.yaml['nkpt'], \n",
    "            kpt_label=True\n",
    "        )\n",
    "        output = output_to_keypoint(output)\n",
    "    \n",
    "    # Prepare image for display\n",
    "    nimg = image[0].permute(1, 2, 0).cpu().numpy()\n",
    "    nimg = (nimg * 255).astype(np.uint8)\n",
    "    nimg = cv2.cvtColor(nimg, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Process each detected person\n",
    "    for idx in range(output.shape[0]):\n",
    "        keypoints = output[idx, 7:].T\n",
    "        plot_skeleton_kpts(nimg, keypoints, 3)\n",
    "\n",
    "        if detect_fall(keypoints):\n",
    "            cv2.putText(nimg, \"Fall Detected!\", (50, 50), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow(\"Fall Detection\", nimg)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd398cd3-1710-4107-80e0-5bc1d820e69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from collections import deque\n",
    "\n",
    "class FallDetector:\n",
    "    def __init__(self):\n",
    "        # Keypoint indices\n",
    "        self.LEFT_SHOULDER = 5\n",
    "        self.RIGHT_SHOULDER = 6\n",
    "        self.LEFT_HIP = 11\n",
    "        self.RIGHT_HIP = 12\n",
    "        self.LEFT_ANKLE = 13\n",
    "        self.RIGHT_ANKLE = 14\n",
    "        \n",
    "        # Parameters from the paper\n",
    "        self.ALPHA = 0.3  # Adjustment factor for height threshold\n",
    "        self.ANGLE_THRESHOLD = 45  # Degrees for torso-leg angle\n",
    "        self.VELOCITY_THRESHOLD = 0.2  # Normalized velocity threshold\n",
    "        self.CONFIRMATION_FRAMES = 5  # Minimum consecutive frames for fall confirmation\n",
    "        \n",
    "        # State tracking\n",
    "        self.previous_positions = deque(maxlen=5)  # Track positions for velocity calculation\n",
    "        self.fall_frames = 0\n",
    "        self.is_fallen = False\n",
    "\n",
    "    def calculate_length_factor(self, shoulder, hip):\n",
    "        \"\"\"Calculate the normalization factor as per equation (1)\"\"\"\n",
    "        return np.sqrt((shoulder[0] - hip[0])**2 + (shoulder[1] - hip[1])**2)\n",
    "\n",
    "    def calculate_body_dimensions(self, keypoints):\n",
    "        \"\"\"Calculate body height and width as per equations (3) and (4)\"\"\"\n",
    "        ls = keypoints[self.LEFT_SHOULDER*3:(self.LEFT_SHOULDER+1)*3]\n",
    "        rs = keypoints[self.RIGHT_SHOULDER*3:(self.RIGHT_SHOULDER+1)*3]\n",
    "        la = keypoints[self.LEFT_ANKLE*3:(self.LEFT_ANKLE+1)*3]\n",
    "        \n",
    "        if ls[2] > 0.5 and rs[2] > 0.5 and la[2] > 0.5:\n",
    "            body_height = abs((ls[1] + rs[1])/2 - la[1])\n",
    "            body_width = abs(ls[0] - rs[0])\n",
    "            return body_height, body_width\n",
    "        return None, None\n",
    "\n",
    "    def calculate_velocity(self, current_shoulder_pos):\n",
    "        \"\"\"Calculate normalized velocity of shoulder movement\"\"\"\n",
    "        if len(self.previous_positions) >= 2:\n",
    "            prev_pos = self.previous_positions[-1]\n",
    "            dx = current_shoulder_pos[0] - prev_pos[0]\n",
    "            dy = current_shoulder_pos[1] - prev_pos[1]\n",
    "            distance = np.sqrt(dx**2 + dy**2)\n",
    "            time_elapsed = len(self.previous_positions)\n",
    "            return distance / time_elapsed\n",
    "        return 0\n",
    "\n",
    "    def calculate_torso_leg_angle(self, keypoints):\n",
    "        \"\"\"Calculate angle between torso and legs in degrees\"\"\"\n",
    "        ls = keypoints[self.LEFT_SHOULDER*3:(self.LEFT_SHOULDER+1)*3]\n",
    "        lh = keypoints[self.LEFT_HIP*3:(self.LEFT_HIP+1)*3]\n",
    "        la = keypoints[self.LEFT_ANKLE*3:(self.LEFT_ANKLE+1)*3]\n",
    "        \n",
    "        if all(kp[2] > 0.5 for kp in [ls, lh, la]):\n",
    "            # Vector from shoulder to hip (torso)\n",
    "            torso_vec = np.array([lh[0] - ls[0], lh[1] - ls[1]])\n",
    "            # Vector from hip to ankle (leg)\n",
    "            leg_vec = np.array([la[0] - lh[0], la[1] - lh[1]])\n",
    "            \n",
    "            # Calculate angle between vectors\n",
    "            unit_torso = torso_vec / np.linalg.norm(torso_vec)\n",
    "            unit_leg = leg_vec / np.linalg.norm(leg_vec)\n",
    "            dot_product = np.dot(unit_torso, unit_leg)\n",
    "            angle = np.degrees(np.arccos(np.clip(dot_product, -1.0, 1.0)))\n",
    "            return angle\n",
    "        return None\n",
    "\n",
    "    def detect_fall(self, keypoints):\n",
    "        \"\"\"Main fall detection function following the paper's algorithm\"\"\"\n",
    "        # Extract keypoints with confidence\n",
    "        ls = keypoints[self.LEFT_SHOULDER*3:(self.LEFT_SHOULDER+1)*3]\n",
    "        rs = keypoints[self.RIGHT_SHOULDER*3:(self.RIGHT_SHOULDER+1)*3]\n",
    "        lh = keypoints[self.LEFT_HIP*3:(self.LEFT_HIP+1)*3]\n",
    "        rh = keypoints[self.RIGHT_HIP*3:(self.RIGHT_HIP+1)*3]\n",
    "        la = keypoints[self.LEFT_ANKLE*3:(self.LEFT_ANKLE+1)*3]\n",
    "        ra = keypoints[self.RIGHT_ANKLE*3:(self.RIGHT_ANKLE+1)*3]\n",
    "        \n",
    "        # Check if we have enough valid keypoints\n",
    "        if any(kp[2] < 0.5 for kp in [ls, rs, lh, rh, la, ra]):\n",
    "            self.fall_frames = max(0, self.fall_frames - 1)\n",
    "            return False\n",
    "        \n",
    "        # Calculate current shoulder position for velocity\n",
    "        shoulder_center = ((ls[0] + rs[0])/2, (ls[1] + rs[1])/2)\n",
    "        self.previous_positions.append(shoulder_center)\n",
    "        \n",
    "        # 1. Calculate length factor as per equation (1)\n",
    "        length_factor = self.calculate_length_factor(ls, lh)\n",
    "        \n",
    "        # 2. Check shoulder height condition as per equation (2)\n",
    "        height_condition = (ls[1] <= la[1] + self.ALPHA * length_factor) and \\\n",
    "                          (rs[1] <= ra[1] + self.ALPHA * length_factor)\n",
    "        \n",
    "        # 3. Check body dimensions as per equations (3)-(5)\n",
    "        body_height, body_width = self.calculate_body_dimensions(keypoints)\n",
    "        dimension_condition = (body_height is not None) and (body_height < body_width)\n",
    "        \n",
    "        # 4. Calculate velocity and angle for differentiation\n",
    "        velocity = self.calculate_velocity(shoulder_center)\n",
    "        angle = self.calculate_torso_leg_angle(keypoints)\n",
    "        \n",
    "        # Differentiate between fall and lying down\n",
    "        is_fall = False\n",
    "        if height_condition and dimension_condition:\n",
    "            if angle is not None and angle < self.ANGLE_THRESHOLD:\n",
    "                if velocity > self.VELOCITY_THRESHOLD:\n",
    "                    is_fall = True  # Sudden fall\n",
    "                else:\n",
    "                    is_fall = True  # Consider as fall even without velocity (per paper)\n",
    "        \n",
    "        # Update fall state with confirmation frames\n",
    "        if is_fall:\n",
    "            self.fall_frames += 1\n",
    "            if self.fall_frames >= self.CONFIRMATION_FRAMES:\n",
    "                self.is_fallen = True\n",
    "                return True\n",
    "        else:\n",
    "            self.fall_frames = max(0, self.fall_frames - 1)\n",
    "            if self.fall_frames == 0:\n",
    "                self.is_fallen = False\n",
    "        \n",
    "        return self.is_fallen\n",
    "\n",
    "        # Initialize detector\n",
    "        fall_detector = FallDetector()\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "        \n",
    "            # [Your existing frame processing code...]\n",
    "            \n",
    "            for idx in range(output.shape[0]):\n",
    "                keypoints = output[idx, 7:].T\n",
    "                plot_skeleton_kpts(nimg, keypoints, 3)\n",
    "        \n",
    "                if fall_detector.detect_fall(keypoints):\n",
    "                    cv2.putText(nimg, \"FALL DETECTED!\", (50, 80), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 3, cv2.LINE_AA)\n",
    "                    \n",
    "                    # Add debug information\n",
    "                    body_height, body_width = fall_detector.calculate_body_dimensions(keypoints)\n",
    "                    angle = fall_detector.calculate_torso_leg_angle(keypoints)\n",
    "                    \n",
    "                    cv2.putText(nimg, f\"Body H/W: {body_height:.1f}/{body_width:.1f}\", (50, 120),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)\n",
    "                    cv2.putText(nimg, f\"Torso-Leg Angle: {angle:.1f}Â°\", (50, 160),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)\n",
    "        \n",
    "            cv2.imshow(\"Fall Detection\", nimg)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed5d8222-96b2-4d2a-9e93-c41cd0020567",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\anaconda3\\envs\\yolov7_env\\lib\\site-packages\\torch\\functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:2895.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MIN_CONFIDENCE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 241\u001b[0m\n\u001b[0;32m    238\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m    240\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:/Users/LENOVO/Documents/A Skripsi/datasets/FallDataset/Dataset/Coffee_room_01/Videos/video (1).avi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 220\u001b[0m, in \u001b[0;36mprocess_video\u001b[1;34m(video_path, output_path, rotation)\u001b[0m\n\u001b[0;32m    217\u001b[0m         plot_skeleton_kpts(display_img, keypoints, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;66;03m# Fall detection logic\u001b[39;00m\n\u001b[1;32m--> 220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mfall_detector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_fall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeypoints\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    221\u001b[0m             cv2\u001b[38;5;241m.\u001b[39mputText(display_img, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFALL DETECTED!\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m80\u001b[39m), \n\u001b[0;32m    222\u001b[0m                        cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m1.5\u001b[39m, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m    224\u001b[0m \u001b[38;5;66;03m# Resize back to original dimensions\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 111\u001b[0m, in \u001b[0;36mFallDetector.detect_fall\u001b[1;34m(self, keypoints)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# Get required keypoints with validation\u001b[39;00m\n\u001b[0;32m    104\u001b[0m required_kps \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    105\u001b[0m     keypoints[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKEYPOINT_IDS[k]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m:(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKEYPOINT_IDS[k]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m] \n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLEFT_SHOULDER\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRIGHT_SHOULDER\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m    107\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLEFT_HIP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRIGHT_HIP\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    108\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLEFT_ANKLE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRIGHT_ANKLE\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    109\u001b[0m ]\n\u001b[1;32m--> 111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43many\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mMIN_CONFIDENCE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrequired_kps\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfall_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfall_frames \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 111\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# Get required keypoints with validation\u001b[39;00m\n\u001b[0;32m    104\u001b[0m required_kps \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    105\u001b[0m     keypoints[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKEYPOINT_IDS[k]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m:(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mKEYPOINT_IDS[k]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m] \n\u001b[0;32m    106\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLEFT_SHOULDER\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRIGHT_SHOULDER\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m    107\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLEFT_HIP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRIGHT_HIP\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    108\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLEFT_ANKLE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRIGHT_ANKLE\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    109\u001b[0m ]\n\u001b[1;32m--> 111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(kp[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[43mMIN_CONFIDENCE\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m kp \u001b[38;5;129;01min\u001b[39;00m required_kps):\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfall_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfall_frames \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MIN_CONFIDENCE' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from collections import deque\n",
    "from models.experimental import attempt_load\n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.datasets import letterbox\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = torch.load('yolov7-w6-pose.pt', map_location=device)\n",
    "model = weights['model'].float().eval()\n",
    "if torch.cuda.is_available():\n",
    "    model.half().to(device)\n",
    "\n",
    "class FallDetector:\n",
    "    def __init__(self, fps=30):\n",
    "        self.KEYPOINT_IDS = {\n",
    "            'LEFT_SHOULDER': 5, \n",
    "            'RIGHT_SHOULDER': 6,\n",
    "            'LEFT_HIP': 11,\n",
    "            'RIGHT_HIP': 12,\n",
    "            'LEFT_ANKLE': 13,\n",
    "            'RIGHT_ANKLE': 14\n",
    "        }\n",
    "        \n",
    "        self.THRESHOLDS = {\n",
    "            'ALPHA': 0.4,\n",
    "            'ANGLE': 120,\n",
    "            'VELOCITY': 0.5,\n",
    "            'CONFIRMATION_FRAMES': 10\n",
    "        }\n",
    "        \n",
    "        self.previous_positions = deque(maxlen=5)\n",
    "        self.fall_frames = 0\n",
    "        self.is_fallen = False\n",
    "        self.fps = fps\n",
    "\n",
    "    def get_valid_keypoints(self, keypoints):\n",
    "        \"\"\"Filter and validate keypoints with proper confidence\"\"\"\n",
    "        MIN_CONFIDENCE = 0.5\n",
    "        SAFE_COORD_THRESH = 1e6\n",
    "        kps = []\n",
    "        for i in range(0, len(keypoints), 3):\n",
    "            x, y, conf = keypoints[i:i+3]\n",
    "            if conf > MIN_CONFIDENCE and abs(x) < SAFE_COORD_THRESH and abs(y) < SAFE_COORD_THRESH:\n",
    "                kps.extend([x, y, conf])\n",
    "            else:\n",
    "                kps.extend([0.0, 0.0, 0.0])\n",
    "        return np.array(kps)\n",
    "\n",
    "    def calculate_length_factor(self, shoulder, hip):\n",
    "        EPSILON = 1e-6\n",
    "        return np.sqrt((shoulder[0] - hip[0])**2 + (shoulder[1] - hip[1])**2 + EPSILON)\n",
    "\n",
    "    def calculate_body_dimensions(self, keypoints):\n",
    "        MIN_CONFIDENCE = 0.5\n",
    "        EPSILON = 1e-6\n",
    "        ls = keypoints[self.KEYPOINT_IDS['LEFT_SHOULDER']*3:(self.KEYPOINT_IDS['LEFT_SHOULDER']+1)*3]\n",
    "        rs = keypoints[self.KEYPOINT_IDS['RIGHT_SHOULDER']*3:(self.KEYPOINT_IDS['RIGHT_SHOULDER']+1)*3]\n",
    "        la = keypoints[self.KEYPOINT_IDS['LEFT_ANKLE']*3:(self.KEYPOINT_IDS['LEFT_ANKLE']+1)*3]\n",
    "        \n",
    "        if all(kp[2] > MIN_CONFIDENCE for kp in [ls, rs, la]):\n",
    "            body_height = abs(((ls[1] + rs[1])/2) - la[1])\n",
    "            body_width = abs(ls[0] - rs[0])\n",
    "            return max(body_height, EPSILON), max(body_width, EPSILON)\n",
    "        return None, None\n",
    "\n",
    "    # Rest of the methods remain with consistent 4-space indentation...\n",
    "\n",
    "    def calculate_velocity(self, current_pos):\n",
    "        if len(self.previous_positions) >= 2:\n",
    "            prev_pos = self.previous_positions[-1]\n",
    "            dx = current_pos[0] - prev_pos[0]\n",
    "            dy = current_pos[1] - prev_pos[1]\n",
    "            distance = np.sqrt(dx**2 + dy**2 + EPSILON)\n",
    "            return distance * self.fps  # pixels/second\n",
    "        return 0\n",
    "\n",
    "    def calculate_torso_leg_angle(self, keypoints):\n",
    "        ls = keypoints[self.KEYPOINT_IDS['LEFT_SHOULDER']*3:(self.KEYPOINT_IDS['LEFT_SHOULDER']+1)*3]\n",
    "        lh = keypoints[self.KEYPOINT_IDS['LEFT_HIP']*3:(self.KEYPOINT_IDS['LEFT_HIP']+1)*3]\n",
    "        la = keypoints[self.KEYPOINT_IDS['LEFT_ANKLE']*3:(self.KEYPOINT_IDS['LEFT_ANKLE']+1)*3]\n",
    "        \n",
    "        if all(kp[2] > MIN_CONFIDENCE for kp in [ls, lh, la]):\n",
    "            torso_vec = np.array([lh[0] - ls[0], lh[1] - ls[1]])\n",
    "            leg_vec = np.array([la[0] - lh[0], la[1] - lh[1]])\n",
    "            \n",
    "            if np.linalg.norm(torso_vec) < EPSILON or np.linalg.norm(leg_vec) < EPSILON:\n",
    "                return None\n",
    "                \n",
    "            unit_torso = torso_vec / np.linalg.norm(torso_vec)\n",
    "            unit_leg = leg_vec / np.linalg.norm(leg_vec)\n",
    "            angle = np.degrees(np.arccos(np.clip(np.dot(unit_torso, unit_leg), -1.0, 1.0)))\n",
    "            return angle\n",
    "        return None\n",
    "\n",
    "    def detect_fall(self, keypoints):\n",
    "        keypoints = self.get_valid_keypoints(keypoints)\n",
    "        \n",
    "        # Get required keypoints with validation\n",
    "        required_kps = [\n",
    "            keypoints[self.KEYPOINT_IDS[k]*3:(self.KEYPOINT_IDS[k]+1)*3] \n",
    "            for k in ['LEFT_SHOULDER', 'RIGHT_SHOULDER', \n",
    "                    'LEFT_HIP', 'RIGHT_HIP',\n",
    "                    'LEFT_ANKLE', 'RIGHT_ANKLE']\n",
    "        ]\n",
    "        \n",
    "        if any(kp[2] < MIN_CONFIDENCE for kp in required_kps):\n",
    "            self.fall_frames = max(0, self.fall_frames - 1)\n",
    "            return False\n",
    "        \n",
    "        ls, rs, lh, rh, la, ra = required_kps\n",
    "        shoulder_center = ((ls[0] + rs[0])/2, (ls[1] + rs[1])/2)\n",
    "        self.previous_positions.append(shoulder_center)\n",
    "        \n",
    "        length_factor = self.calculate_length_factor(ls, lh)\n",
    "        body_height, body_width = self.calculate_body_dimensions(keypoints)\n",
    "        \n",
    "        vertical_diff_left = abs(ls[1] - la[1])\n",
    "        vertical_diff_right = abs(rs[1] - ra[1])\n",
    "        height_condition = (vertical_diff_left > self.THRESHOLDS['ALPHA'] * length_factor) or \\\n",
    "                          (vertical_diff_right > self.THRESHOLDS['ALPHA'] * length_factor)\n",
    "        \n",
    "        dimension_condition = (body_height is not None) and (body_height < body_width * 1.2)\n",
    "        \n",
    "        velocity = self.calculate_velocity(shoulder_center)\n",
    "        angle = self.calculate_torso_leg_angle(keypoints)\n",
    "        \n",
    "        is_fall = False\n",
    "        if height_condition and dimension_condition:\n",
    "            angle_condition = (angle is not None) and (angle < self.THRESHOLDS['ANGLE'])\n",
    "            velocity_condition = velocity > self.THRESHOLDS['VELOCITY']\n",
    "            is_fall = angle_condition or velocity_condition\n",
    "        \n",
    "        if is_fall:\n",
    "            self.fall_frames += 1\n",
    "            if self.fall_frames >= self.THRESHOLDS['CONFIRMATION_FRAMES']:\n",
    "                self.is_fallen = True\n",
    "                return True\n",
    "        else:\n",
    "            self.fall_frames = max(0, self.fall_frames - 2)\n",
    "            if self.fall_frames == 0:\n",
    "                self.is_fallen = False\n",
    "        \n",
    "        return self.is_fallen\n",
    "\n",
    "def process_video(video_path, output_path=None, rotation=0):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Rotation handling\n",
    "    if rotation in [90, 270]:\n",
    "        width, height = original_height, original_width\n",
    "    else:\n",
    "        width, height = original_width, original_height\n",
    "    \n",
    "    # Video writer setup\n",
    "    if output_path:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    fall_detector = FallDetector(fps=fps)\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret or frame is None:\n",
    "            break\n",
    "        \n",
    "        # Rotate frame if needed\n",
    "        if rotation == 90:\n",
    "            frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)\n",
    "        elif rotation == 180:\n",
    "            frame = cv2.rotate(frame, cv2.ROTATE_180)\n",
    "        elif rotation == 270:\n",
    "            frame = cv2.rotate(frame, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "        \n",
    "        # Preprocess frame (same as photo example)\n",
    "        img = letterbox(frame, 960, stride=64, auto=True)[0]\n",
    "        img_tensor = transforms.ToTensor()(img)\n",
    "        img_tensor = img_tensor.unsqueeze(0)\n",
    "        if torch.cuda.is_available():\n",
    "            img_tensor = img_tensor.half().to(device)\n",
    "        \n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            output, _ = model(img_tensor)\n",
    "            output = non_max_suppression_kpt(\n",
    "                output, \n",
    "                0.25, 0.65, \n",
    "                nc=model.yaml['nc'], \n",
    "                nkpt=model.yaml['nkpt'], \n",
    "                kpt_label=True\n",
    "            )\n",
    "            output = output_to_keypoint(output)\n",
    "        \n",
    "        # Prepare display image (same as photo example)\n",
    "        display_img = img_tensor[0].permute(1, 2, 0).cpu().numpy()\n",
    "        display_img = (display_img * 255).astype(np.uint8)\n",
    "        display_img = cv2.cvtColor(display_img, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Process detections\n",
    "        if output is not None and len(output) > 0:\n",
    "            for idx in range(output.shape[0]):\n",
    "                keypoints = output[idx, 7:].T\n",
    "                \n",
    "                # Plot skeleton directly on processed image\n",
    "                plot_skeleton_kpts(display_img, keypoints, 3)\n",
    "                \n",
    "                # Fall detection logic\n",
    "                if fall_detector.detect_fall(keypoints):\n",
    "                    cv2.putText(display_img, \"FALL DETECTED!\", (50, 80), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 3)\n",
    "        \n",
    "        # Resize back to original dimensions\n",
    "        display_img = cv2.resize(display_img, (width, height))\n",
    "        \n",
    "        # Show and save results\n",
    "        cv2.imshow(\"Fall Detection\", display_img)\n",
    "        if output_path:\n",
    "            out.write(display_img)\n",
    "            \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    if output_path:\n",
    "        out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "process_video('C:/Users/LENOVO/Documents/A Skripsi/datasets/FallDataset/Dataset/Coffee_room_01/Videos/video (1).avi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d288f7d1-149e-454d-bb43-c87f7e9374d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
