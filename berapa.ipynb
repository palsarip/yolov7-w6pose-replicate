{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02e731b0-1ed1-4e93-b074-d344fe2a705a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import time\n",
    "from torchvision import transforms\n",
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "from models.yolo import Model\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load YOLOv7-pose model\n",
    "weights = torch.load('yolov7-w6-pose.pt', map_location=device, weights_only=False)\n",
    "model = weights['model'].float().eval()\n",
    "if torch.cuda.is_available():\n",
    "    model.half().to(device)\n",
    "\n",
    "# Parameters from journal\n",
    "ALPHA = 0.5            # Height adjustment factor\n",
    "SPEED_THRESHOLD = 100  # Speed threshold (pixels/second)\n",
    "ANGLE_THRESHOLD = 45   # Angle threshold (degrees)\n",
    "TARGET_FPS = 25        # Target processing FPS\n",
    "KP_CONFIDENCE = 0.5    # Keypoint confidence threshold\n",
    "\n",
    "# Tracking variables\n",
    "prev_shoulder_y = None\n",
    "prev_time = None\n",
    "fall_counter = 0       # To prevent temporary false positives\n",
    "FALL_FRAMES_THRESHOLD = 3  # Minimum consecutive frames to confirm fall\n",
    "\n",
    "def calculate_length_factor(shoulder, hip):\n",
    "    \"\"\"Calculate L factor (Euclidean distance between shoulder and hip)\"\"\"\n",
    "    return np.sqrt((shoulder[0] - hip[0])**2 + (shoulder[1] - hip[1])**2)\n",
    "\n",
    "def calculate_angle(a, b, c):\n",
    "    \"\"\"Calculate angle between three points (in degrees)\"\"\"\n",
    "    ba = np.array(a) - np.array(b)\n",
    "    bc = np.array(c) - np.array(b)\n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    angle = np.arccos(np.clip(cosine_angle, -1, 1))\n",
    "    return np.degrees(angle)\n",
    "\n",
    "def detect_fall(keypoints, vertical_speed):\n",
    "    \"\"\"Implement fall detection algorithm from the journal\"\"\"\n",
    "    global fall_counter\n",
    "    \n",
    "    # Keypoint indices (YOLOv7-pose format)\n",
    "    LEFT_SHOULDER, RIGHT_SHOULDER = 5, 6\n",
    "    LEFT_HIP, RIGHT_HIP = 11, 12\n",
    "    LEFT_ANKLE, RIGHT_ANKLE = 15, 16\n",
    "    \n",
    "    # Extract keypoints with confidence check\n",
    "    kpts = {\n",
    "        'left_shoulder': keypoints[LEFT_SHOULDER*3:(LEFT_SHOULDER+1)*3],\n",
    "        'right_shoulder': keypoints[RIGHT_SHOULDER*3:(RIGHT_SHOULDER+1)*3],\n",
    "        'left_hip': keypoints[LEFT_HIP*3:(LEFT_HIP+1)*3],\n",
    "        'right_hip': keypoints[RIGHT_HIP*3:(RIGHT_HIP+1)*3],\n",
    "        'left_ankle': keypoints[LEFT_ANKLE*3:(LEFT_ANKLE+1)*3],\n",
    "        'right_ankle': keypoints[RIGHT_ANKLE*3:(RIGHT_ANKLE+1)*3]\n",
    "    }\n",
    "    \n",
    "    # Filter keypoints with low confidence\n",
    "    if any(kp[2] < KP_CONFIDENCE for kp in kpts.values()):\n",
    "        fall_counter = max(0, fall_counter - 1)\n",
    "        return False\n",
    "    \n",
    "    # Calculate length factor (average of left and right)\n",
    "    L_left = calculate_length_factor(kpts['left_shoulder'][:2], kpts['left_hip'][:2])\n",
    "    L_right = calculate_length_factor(kpts['right_shoulder'][:2], kpts['right_hip'][:2])\n",
    "    L_factor = (L_left + L_right) / 2\n",
    "    \n",
    "    # Criterion 1: Shoulder low relative to feet\n",
    "    shoulder_low = (\n",
    "        (kpts['left_shoulder'][1] <= kpts['left_ankle'][1] + ALPHA * L_factor) or\n",
    "        (kpts['right_shoulder'][1] <= kpts['right_ankle'][1] + ALPHA * L_factor)\n",
    "    )\n",
    "    \n",
    "    # Criterion 2: Hip low relative to feet\n",
    "    hip_low = (\n",
    "        (kpts['left_hip'][1] <= kpts['left_ankle'][1] + ALPHA * L_factor) or\n",
    "        (kpts['right_hip'][1] <= kpts['right_ankle'][1] + ALPHA * L_factor)\n",
    "    )\n",
    "    \n",
    "    # Criterion 3: Horizontal position (height < width)\n",
    "    avg_shoulder_y = (kpts['left_shoulder'][1] + kpts['right_shoulder'][1]) / 2\n",
    "    avg_ankle_y = (kpts['left_ankle'][1] + kpts['right_ankle'][1]) / 2\n",
    "    body_height = abs(avg_shoulder_y - avg_ankle_y)\n",
    "    body_width = abs(kpts['left_shoulder'][0] - kpts['right_shoulder'][0])\n",
    "    horizontal_position = body_height < body_width\n",
    "    \n",
    "    # Criterion 4: Sharp angles or high speed\n",
    "    left_angle = calculate_angle(\n",
    "        kpts['left_shoulder'][:2],\n",
    "        kpts['left_hip'][:2],\n",
    "        kpts['left_ankle'][:2]\n",
    "    )\n",
    "    right_angle = calculate_angle(\n",
    "        kpts['right_shoulder'][:2],\n",
    "        kpts['right_hip'][:2],\n",
    "        kpts['right_ankle'][:2]\n",
    "    )\n",
    "    min_angle = min(left_angle, right_angle)\n",
    "    \n",
    "    # Combine all criteria\n",
    "    fall_conditions = (shoulder_low and hip_low and horizontal_position and \n",
    "                      (abs(vertical_speed) > SPEED_THRESHOLD or min_angle < ANGLE_THRESHOLD))\n",
    "    \n",
    "    # Require multiple consecutive frames to confirm fall\n",
    "    if fall_conditions:\n",
    "        fall_counter += 1\n",
    "    else:\n",
    "        fall_counter = max(0, fall_counter - 1)\n",
    "    \n",
    "    return fall_counter >= FALL_FRAMES_THRESHOLD\n",
    "\n",
    "def process_video(video_path):\n",
    "    \"\"\"Process video stream for fall detection\"\"\"\n",
    "    global prev_shoulder_y, prev_time, fall_counter\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "    \n",
    "    # Frame rate adjustment\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    if original_fps <= 0:\n",
    "        original_fps = 25  # Default if cannot read FPS\n",
    "    skip_frames = max(1, int(round(original_fps / TARGET_FPS)))\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Skip frames to maintain target FPS\n",
    "        if int(cap.get(cv2.CAP_PROP_POS_FRAMES)) % skip_frames != 0:\n",
    "            continue\n",
    "        \n",
    "        # Preprocess frame\n",
    "        image = letterbox(frame, 640, stride=64, auto=True)[0]\n",
    "        image_tensor = transforms.ToTensor()(image).unsqueeze(0)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            image_tensor = image_tensor.half().to(device)\n",
    "        \n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            output, _ = model(image_tensor)\n",
    "            output = non_max_suppression_kpt(output, 0.25, 0.65, \n",
    "                                           nc=model.yaml['nc'], \n",
    "                                           nkpt=model.yaml['nkpt'], \n",
    "                                           kpt_label=True)\n",
    "            output = output_to_keypoint(output)\n",
    "        \n",
    "        vertical_speed = 0\n",
    "        current_time = time.perf_counter()\n",
    "        fall_detected = False\n",
    "        \n",
    "        if output.shape[0] > 0:\n",
    "            keypoints = output[0, 7:].T\n",
    "            \n",
    "            # Calculate vertical speed (shoulder movement)\n",
    "            left_shoulder_y = keypoints[5*3+1]\n",
    "            right_shoulder_y = keypoints[6*3+1]\n",
    "            current_shoulder_y = (left_shoulder_y + right_shoulder_y) / 2\n",
    "            \n",
    "            if prev_shoulder_y is not None and prev_time is not None:\n",
    "                time_diff = current_time - prev_time\n",
    "                if time_diff > 0:\n",
    "                    vertical_speed = (current_shoulder_y - prev_shoulder_y) / time_diff\n",
    "            \n",
    "            prev_shoulder_y = current_shoulder_y\n",
    "            prev_time = current_time\n",
    "            \n",
    "            # Detect fall\n",
    "            fall_detected = detect_fall(keypoints, vertical_speed)\n",
    "            \n",
    "            # Prepare visualization\n",
    "            nimg = image_tensor[0].permute(1, 2, 0).cpu().numpy() * 255\n",
    "            nimg = cv2.cvtColor(nimg.astype(np.uint8), cv2.COLOR_RGB2BGR)\n",
    "            plot_skeleton_kpts(nimg, keypoints, 3)\n",
    "            \n",
    "            # Calculate angles for display\n",
    "            left_angle = calculate_angle(\n",
    "                keypoints[5*3:5*3+2],  # left shoulder\n",
    "                keypoints[11*3:11*3+2], # left hip\n",
    "                keypoints[15*3:15*3+2]  # left ankle\n",
    "            )\n",
    "            right_angle = calculate_angle(\n",
    "                keypoints[6*3:6*3+2],  # right shoulder\n",
    "                keypoints[12*3:12*3+2], # right hip\n",
    "                keypoints[16*3:16*3+2]  # right ankle\n",
    "            )\n",
    "            min_angle = min(left_angle, right_angle)\n",
    "            \n",
    "            # Display information\n",
    "            status_text = \"Normal\"\n",
    "            color = (0, 255, 0)  # Green\n",
    "            if fall_detected:\n",
    "                status_text = \"FALL DETECTED!\"\n",
    "                color = (0, 0, 255)  # Red\n",
    "            \n",
    "            cv2.putText(nimg, status_text, (50, 50), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "            cv2.putText(nimg, f\"Speed: {vertical_speed:.1f} px/s\", (50, 90), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)\n",
    "            cv2.putText(nimg, f\"Angle: {min_angle:.1f}°\", (50, 130), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)\n",
    "            \n",
    "            # Show result\n",
    "            cv2.imshow(\"Fall Detection\", nimg)\n",
    "        else:\n",
    "            prev_shoulder_y = None\n",
    "            prev_time = None\n",
    "            fall_counter = 0\n",
    "            cv2.imshow(\"Fall Detection\", frame)\n",
    "        \n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_path = \"C:\\\\Users\\\\LENOVO\\\\Documents\\\\A Skripsi\\\\datasets\\\\FallDataset\\\\Dataset\\\\Coffee_room_01\\\\Videos\\\\video (1).avi\"\n",
    "    process_video(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f031773-7028-4476-8d96-e4b6ca809c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video at target FPS: 25 (skipping 0 frames)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\anaconda3\\envs\\yolov7_env\\lib\\site-packages\\torch\\functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3638.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import time\n",
    "from torchvision import transforms\n",
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "from models.yolo import Model\n",
    "\n",
    "class FallDetector:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.load_model()\n",
    "        self.initialize_parameters()\n",
    "        self.reset_tracking_variables()\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load YOLOv7-pose model with safety checks\"\"\"\n",
    "        torch.serialization.add_safe_globals([Model])\n",
    "        weights = torch.load('yolov7-w6-pose.pt', map_location=self.device, weights_only=False)\n",
    "        self.model = weights['model'].float().eval()\n",
    "        if torch.cuda.is_available():\n",
    "            self.model.half().to(self.device)\n",
    "    \n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"Set parameters according to the journal\"\"\"\n",
    "        self.ALPHA = 0.5            # Height adjustment factor\n",
    "        self.SPEED_THRESHOLD = 100   # Speed threshold (pixels/second)\n",
    "        self.ANGLE_THRESHOLD = 45    # Angle threshold (degrees)\n",
    "        self.TARGET_FPS = 25         # Target processing FPS\n",
    "        self.KP_CONFIDENCE = 0.5    # Keypoint confidence threshold\n",
    "        self.FALL_FRAMES_THRESHOLD = 3  # Minimum consecutive frames to confirm fall\n",
    "    \n",
    "    def reset_tracking_variables(self):\n",
    "        \"\"\"Reset variables for new detection sequence\"\"\"\n",
    "        self.prev_shoulder_y = None\n",
    "        self.prev_time = None\n",
    "        self.fall_counter = 0\n",
    "        self.frame_count = 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_length_factor(shoulder, hip):\n",
    "        \"\"\"Calculate L factor (Euclidean distance between shoulder and hip)\"\"\"\n",
    "        return np.sqrt((shoulder[0] - hip[0])**2 + (shoulder[1] - hip[1])**2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def calculate_angle(a, b, c):\n",
    "        \"\"\"Calculate angle between three points (in degrees)\"\"\"\n",
    "        ba = np.array(a) - np.array(b)\n",
    "        bc = np.array(c) - np.array(b)\n",
    "        cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "        angle = np.arccos(np.clip(cosine_angle, -1, 1))\n",
    "        return np.degrees(angle)\n",
    "    \n",
    "    def detect_fall(self, keypoints):\n",
    "        \"\"\"\n",
    "        Implement complete fall detection algorithm from the journal\n",
    "        with all four criteria and knee points integration\n",
    "        \"\"\"\n",
    "        # Keypoint indices (YOLOv7-pose format)\n",
    "        LEFT_SHOULDER, RIGHT_SHOULDER = 5, 6\n",
    "        LEFT_HIP, RIGHT_HIP = 11, 12\n",
    "        LEFT_KNEE, RIGHT_KNEE = 13, 14\n",
    "        LEFT_ANKLE, RIGHT_ANKLE = 15, 16\n",
    "        \n",
    "        # Extract keypoints with confidence check\n",
    "        kpts = {\n",
    "            'left_shoulder': keypoints[LEFT_SHOULDER*3:(LEFT_SHOULDER+1)*3],\n",
    "            'right_shoulder': keypoints[RIGHT_SHOULDER*3:(RIGHT_SHOULDER+1)*3],\n",
    "            'left_hip': keypoints[LEFT_HIP*3:(LEFT_HIP+1)*3],\n",
    "            'right_hip': keypoints[RIGHT_HIP*3:(RIGHT_HIP+1)*3],\n",
    "            'left_knee': keypoints[LEFT_KNEE*3:(LEFT_KNEE+1)*3],\n",
    "            'right_knee': keypoints[RIGHT_KNEE*3:(RIGHT_KNEE+1)*3],\n",
    "            'left_ankle': keypoints[LEFT_ANKLE*3:(LEFT_ANKLE+1)*3],\n",
    "            'right_ankle': keypoints[RIGHT_ANKLE*3:(RIGHT_ANKLE+1)*3]\n",
    "        }\n",
    "        \n",
    "        # Filter keypoints with low confidence\n",
    "        if any(kp[2] < self.KP_CONFIDENCE for kp in kpts.values()):\n",
    "            self.fall_counter = max(0, self.fall_counter - 1)\n",
    "            return False\n",
    "        \n",
    "        # Calculate vertical speed (shoulder movement)\n",
    "        current_shoulder_y = (kpts['left_shoulder'][1] + kpts['right_shoulder'][1]) / 2\n",
    "        vertical_speed = 0\n",
    "        \n",
    "        if self.prev_shoulder_y is not None and self.prev_time is not None:\n",
    "            time_diff = time.perf_counter() - self.prev_time\n",
    "            if time_diff > 0:\n",
    "                vertical_speed = (current_shoulder_y - self.prev_shoulder_y) / time_diff\n",
    "        \n",
    "        self.prev_shoulder_y = current_shoulder_y\n",
    "        self.prev_time = time.perf_counter()\n",
    "        \n",
    "        # Calculate length factor (average of left and right)\n",
    "        L_left = self.calculate_length_factor(kpts['left_shoulder'][:2], kpts['left_hip'][:2])\n",
    "        L_right = self.calculate_length_factor(kpts['right_shoulder'][:2], kpts['right_hip'][:2])\n",
    "        L_factor = (L_left + L_right) / 2\n",
    "        \n",
    "        # Criterion 1: Shoulder low relative to feet (with length factor adjustment)\n",
    "        shoulder_low = (\n",
    "            (kpts['left_shoulder'][1] <= kpts['left_ankle'][1] + self.ALPHA * L_factor) or\n",
    "            (kpts['right_shoulder'][1] <= kpts['right_ankle'][1] + self.ALPHA * L_factor)\n",
    "        )\n",
    "        \n",
    "        # Criterion 2: Hip low relative to feet\n",
    "        hip_low = (\n",
    "            (kpts['left_hip'][1] <= kpts['left_ankle'][1] + self.ALPHA * L_factor) or\n",
    "            (kpts['right_hip'][1] <= kpts['right_ankle'][1] + self.ALPHA * L_factor)\n",
    "        )\n",
    "        \n",
    "        # Criterion 3: Horizontal position (height < width)\n",
    "        avg_shoulder_y = (kpts['left_shoulder'][1] + kpts['right_shoulder'][1]) / 2\n",
    "        avg_ankle_y = (kpts['left_ankle'][1] + kpts['right_ankle'][1]) / 2\n",
    "        body_height = abs(avg_shoulder_y - avg_ankle_y)\n",
    "        body_width = abs(kpts['left_shoulder'][0] - kpts['right_shoulder'][0])\n",
    "        horizontal_position = body_height < body_width\n",
    "        \n",
    "        # Criterion 4: Sharp angles (using knee points) or high speed\n",
    "        left_leg_angle = self.calculate_angle(\n",
    "            kpts['left_hip'][:2],\n",
    "            kpts['left_knee'][:2],\n",
    "            kpts['left_ankle'][:2]\n",
    "        )\n",
    "        right_leg_angle = self.calculate_angle(\n",
    "            kpts['right_hip'][:2],\n",
    "            kpts['right_knee'][:2],\n",
    "            kpts['right_ankle'][:2]\n",
    "        )\n",
    "        min_angle = min(left_leg_angle, right_leg_angle)\n",
    "        \n",
    "        # Combine all criteria exactly as in the journal\n",
    "        fall_conditions = (\n",
    "            shoulder_low and \n",
    "            hip_low and \n",
    "            horizontal_position and\n",
    "            (abs(vertical_speed) > self.SPEED_THRESHOLD or min_angle < self.ANGLE_THRESHOLD)\n",
    "        )\n",
    "        \n",
    "        # Require multiple consecutive frames to confirm fall\n",
    "        if fall_conditions:\n",
    "            self.fall_counter += 1\n",
    "        else:\n",
    "            self.fall_counter = max(0, self.fall_counter - 1)\n",
    "        \n",
    "        return self.fall_counter >= self.FALL_FRAMES_THRESHOLD\n",
    "    \n",
    "    def process_frame(self, frame):\n",
    "        \"\"\"Process a single frame for fall detection\"\"\"\n",
    "        # Preprocess frame\n",
    "        image = letterbox(frame, 640, stride=64, auto=True)[0]\n",
    "        image_tensor = transforms.ToTensor()(image).unsqueeze(0)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            image_tensor = image_tensor.half().to(self.device)\n",
    "        \n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            output, _ = self.model(image_tensor)\n",
    "            output = non_max_suppression_kpt(\n",
    "                output, 0.25, 0.65, \n",
    "                nc=self.model.yaml['nc'], \n",
    "                nkpt=self.model.yaml['nkpt'], \n",
    "                kpt_label=True\n",
    "            )\n",
    "            output = output_to_keypoint(output)\n",
    "        \n",
    "        fall_detected = False\n",
    "        visualization_img = frame.copy()\n",
    "        \n",
    "        if output.shape[0] > 0:\n",
    "            keypoints = output[0, 7:].T\n",
    "            plot_skeleton_kpts(visualization_img, keypoints, 3)\n",
    "            \n",
    "            # Detect fall with all journal criteria\n",
    "            fall_detected = self.detect_fall(keypoints)\n",
    "            \n",
    "            # Calculate angles for display\n",
    "            left_angle = self.calculate_angle(\n",
    "                keypoints[11*3:11*3+2],  # left hip\n",
    "                keypoints[13*3:13*3+2],   # left knee\n",
    "                keypoints[15*3:15*3+2]    # left ankle\n",
    "            )\n",
    "            right_angle = self.calculate_angle(\n",
    "                keypoints[12*3:12*3+2],   # right hip\n",
    "                keypoints[14*3:14*3+2],   # right knee\n",
    "                keypoints[16*3:16*3+2]    # right ankle\n",
    "            )\n",
    "            min_angle = min(left_angle, right_angle)\n",
    "            \n",
    "            # Display information\n",
    "            status_text = \"Normal\"\n",
    "            color = (0, 255, 0)  # Green\n",
    "            if fall_detected:\n",
    "                status_text = \"FALL DETECTED!\"\n",
    "                color = (0, 0, 255)  # Red\n",
    "            \n",
    "            cv2.putText(visualization_img, status_text, (50, 50), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "            cv2.putText(visualization_img, f\"Min Angle: {min_angle:.1f}°\", (50, 90), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)\n",
    "        \n",
    "        return visualization_img, fall_detected\n",
    "\n",
    "def main():\n",
    "    detector = FallDetector()\n",
    "    \n",
    "    video_path = \"C:\\\\Users\\\\LENOVO\\\\Documents\\\\A Skripsi\\\\datasets\\\\FallDataset\\\\Dataset\\\\Coffee_room_01\\\\Videos\\\\video (1).avi\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "    \n",
    "    # Frame rate adjustment\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    if original_fps <= 0:\n",
    "        original_fps = 25  # Default if cannot read FPS\n",
    "    skip_frames = max(1, int(round(original_fps / detector.TARGET_FPS)))\n",
    "    \n",
    "    print(f\"Processing video at target FPS: {detector.TARGET_FPS} (skipping {skip_frames-1} frames)\")\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Skip frames to maintain target FPS\n",
    "        if int(cap.get(cv2.CAP_PROP_POS_FRAMES)) % skip_frames != 0:\n",
    "            continue\n",
    "        \n",
    "        # Process frame\n",
    "        processed_frame, fall_detected = detector.process_frame(frame)\n",
    "        \n",
    "        # Display result\n",
    "        cv2.imshow(\"Fall Detection\", processed_frame)\n",
    "        \n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d684b71-b8f8-4720-97d7-a2b5f1b996a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
