{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f72a11a9-9c33-4353-90ab-23c323bd8f63",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "707a1d00-f913-4688-be6c-76233bd17281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Model file found at models/yolov7-w6-pose.pt\n",
      "Camera 0 is available\n",
      "System setup completed!\n"
     ]
    }
   ],
   "source": [
    "# 1. System Setup for Fall Detection Application\n",
    "\n",
    "# Import necessary standard libraries\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import argparse\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Import PyTorch libraries\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set up GPU if available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define constants and configuration\n",
    "CONFIG = {\n",
    "    'model_path': 'models/yolov7-w6-pose.pt',  # Path to the pre-trained YOLOv7-w6-pose model\n",
    "    'input_size': (960, 960),           # Input size for the model (width, height)\n",
    "    'confidence_threshold': 0.25,       # Confidence threshold for detection\n",
    "    'iou_threshold': 0.45,              # IoU threshold for NMS\n",
    "    'device': device,                   # Device to run inference on\n",
    "}\n",
    "\n",
    "# Check if the model file exists\n",
    "if not os.path.exists(CONFIG['model_path']):\n",
    "    print(f\"Model file not found at {CONFIG['model_path']}. Please download it first.\")\n",
    "    print(\"You can download it from: https://github.com/WongKinYiu/yolov7/releases\")\n",
    "else:\n",
    "    print(f\"Model file found at {CONFIG['model_path']}\")\n",
    "\n",
    "# Function to check camera availability\n",
    "def check_camera(camera_id=0):\n",
    "    cap = cv2.VideoCapture(camera_id)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Camera {camera_id} is not available\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"Camera {camera_id} is available\")\n",
    "        cap.release()\n",
    "        return True\n",
    "\n",
    "# Check camera\n",
    "camera_available = check_camera()\n",
    "\n",
    "print(\"System setup completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2574d478-1ce3-46de-b345-cc45f376a3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Video Processing Pipeline\n",
    "\n",
    "def preprocess_frame(frame, input_size=(960, 960)):\n",
    "    \"\"\"\n",
    "    Preprocess a frame for input to YOLOv7-W6-Pose.\n",
    "    \n",
    "    Args:\n",
    "        frame (numpy.ndarray): Input frame from video/camera\n",
    "        input_size (tuple): Target size for model input (width, height)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Preprocessed frame tensor ready for model input\n",
    "    \"\"\"\n",
    "    # Convert BGR to RGB (OpenCV uses BGR by default)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Create a PIL image from the numpy array\n",
    "    image = Image.fromarray(rgb_frame)\n",
    "    \n",
    "    # Original dimensions\n",
    "    orig_width, orig_height = image.size\n",
    "    \n",
    "    # Calculate the letterbox dimensions to maintain aspect ratio\n",
    "    ratio = min(input_size[0] / orig_width, input_size[1] / orig_height)\n",
    "    new_width = int(orig_width * ratio)\n",
    "    new_height = int(orig_height * ratio)\n",
    "    \n",
    "    # Resize the image\n",
    "    resized_image = image.resize((new_width, new_height), Image.LANCZOS)\n",
    "    \n",
    "    # Create a new image with the target size and paste the resized image\n",
    "    letterboxed_image = Image.new(\"RGB\", input_size, (114, 114, 114))\n",
    "    letterboxed_image.paste(resized_image, ((input_size[0] - new_width) // 2, \n",
    "                                           (input_size[1] - new_height) // 2))\n",
    "    \n",
    "    # Convert to tensor and normalize\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Apply transforms\n",
    "    tensor = transform(letterboxed_image)\n",
    "    \n",
    "    # Add batch dimension\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    \n",
    "    return tensor, ratio, (input_size[0] - new_width) // 2, (input_size[1] - new_height) // 2\n",
    "\n",
    "\n",
    "def create_video_capture(source=0):\n",
    "    \"\"\"\n",
    "    Create a video capture object for the specified source.\n",
    "    \n",
    "    Args:\n",
    "        source: Camera index or video file path\n",
    "        \n",
    "    Returns:\n",
    "        cv2.VideoCapture: VideoCapture object\n",
    "    \"\"\"\n",
    "    # Create video capture object\n",
    "    cap = cv2.VideoCapture(video_path, cv2.CAP_FFMPEG)\n",
    "    \n",
    "    # Check if camera/video opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video source {source}\")\n",
    "        return None\n",
    "    \n",
    "    # Get frame dimensions\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    print(f\"Video source opened: {frame_width}x{frame_height} at {fps} FPS\")\n",
    "    \n",
    "    return cap\n",
    "\n",
    "\n",
    "def process_video_source(source=0, process_frame_func=None, display=True, output_file=None):\n",
    "    \"\"\"\n",
    "    Process frames from a video source (camera or file).\n",
    "    \n",
    "    Args:\n",
    "        source: Camera index or video file path\n",
    "        process_frame_func: Function to process each frame\n",
    "        display: Whether to display the output frame\n",
    "        output_file: Path to save output video (if None, no saving)\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Create video capture\n",
    "    cap = create_video_capture(source)\n",
    "    if cap is None:\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Create video writer if output file is specified\n",
    "    out = None\n",
    "    if output_file is not None:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_file, fourcc, fps, (frame_width, frame_height))\n",
    "    \n",
    "    # Process video frames\n",
    "    frame_count = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while True:\n",
    "        # Read frame\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Break if end of video or error\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        \n",
    "        # Process the frame if a processing function is provided\n",
    "        if process_frame_func is not None:\n",
    "            processed_frame, fall_detected = process_frame_func(frame)\n",
    "        else:\n",
    "            processed_frame = frame\n",
    "            fall_detected = False\n",
    "        \n",
    "        # Write frame to output video if specified\n",
    "        if out is not None:\n",
    "            out.write(processed_frame)\n",
    "        \n",
    "        # Display the frame\n",
    "        if display:\n",
    "            # Add fall detection status to the frame\n",
    "            if fall_detected:\n",
    "                cv2.putText(processed_frame, \"FALL DETECTED\", (50, 50), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            \n",
    "            cv2.imshow('Video Processing', processed_frame)\n",
    "            \n",
    "            # Break if 'q' is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    \n",
    "    # Calculate and print processing stats\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if frame_count > 0 and elapsed_time > 0:\n",
    "        fps_processing = frame_count / elapsed_time\n",
    "        print(f\"Processed {frame_count} frames in {elapsed_time:.2f} seconds ({fps_processing:.2f} FPS)\")\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    if out is not None:\n",
    "        out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Test the video processing pipeline\n",
    "# if __name__ == \"__main__\":\n",
    "#     # This will just display the camera feed without any processing\n",
    "#     process_video_source(0, None, True, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78184261-8595-46b4-96f5-cb11c4a5e555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Pose Estimation using YOLOv7-W6-Pose\n",
    "\n",
    "# Load the YOLOv7-W6-Pose model\n",
    "def load_model(model_path, device):\n",
    "    \"\"\"\n",
    "    Load the YOLOv7-W6-Pose model from file.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the model weights file\n",
    "        device (torch.device): Device to load the model on\n",
    "        \n",
    "    Returns:\n",
    "        model: Loaded YOLOv7 model\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    model = torch.load(model_path, map_location=device)['model']\n",
    "    # Extract the model\n",
    "    if isinstance(model, dict):\n",
    "        model = model.float().to(device)  # FP32 model\n",
    "    else:\n",
    "        model.float().to(device)\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    \n",
    "    if device.type != 'cpu':\n",
    "        model = model.half()  # Convert to FP16 if on GPU\n",
    "    \n",
    "    print(f\"Model {model_path} loaded to {device}\")\n",
    "    return model\n",
    "\n",
    "# Function for non-maximum suppression to filter pose detections\n",
    "def non_max_suppression_pose(prediction, conf_thres=0.25, iou_thres=0.45, classes=None, max_det=300):\n",
    "    \"\"\"\n",
    "    Performs Non-Maximum Suppression (NMS) on the outputs of YOLOv7-W6-Pose.\n",
    "    \n",
    "    Args:\n",
    "        prediction (torch.Tensor): Model predictions\n",
    "        conf_thres (float): Confidence threshold\n",
    "        iou_thres (float): IoU threshold\n",
    "        classes (list): Filter by class\n",
    "        max_det (int): Maximum number of detections\n",
    "        \n",
    "    Returns:\n",
    "        list: List of detections with pose keypoints\n",
    "    \"\"\"\n",
    "    # Handle empty prediction case\n",
    "    if prediction.numel() == 0:\n",
    "        return [torch.zeros((0, 57), device=prediction.device)] * prediction.shape[0]\n",
    "    \n",
    "    # Print prediction shape for debugging\n",
    "    print(f\"Prediction shape: {prediction.shape}\")\n",
    "    \n",
    "    # Try to determine the number of classes and keypoints\n",
    "    try:\n",
    "        # For YOLOv7-W6-Pose standard format\n",
    "        nc = prediction.shape[2] - 57  # Number of classes\n",
    "        nkpt = 17  # Number of keypoints (COCO keypoints)\n",
    "    except IndexError:\n",
    "        # If prediction shape is not as expected, use default values\n",
    "        print(\"Warning: Unable to determine classes from prediction shape. Using defaults.\")\n",
    "        nc = 1  # Default: single class (person)\n",
    "        nkpt = 17  # Default: 17 keypoints (COCO format)\n",
    "\n",
    "# Helper function to convert [cx, cy, w, h] to [x1, y1, x2, y2]\n",
    "def xywh2xyxy(x):\n",
    "    \"\"\"\n",
    "    Convert bounding box coordinates from [cx, cy, w, h] to [x1, y1, x2, y2].\n",
    "    \n",
    "    Args:\n",
    "        x (torch.Tensor): Bounding box coordinates in [cx, cy, w, h] format\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Bounding box coordinates in [x1, y1, x2, y2] format\n",
    "    \"\"\"\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[:, 0] = x[:, 0] - x[:, 2] / 2  # x1 = cx - w/2\n",
    "    y[:, 1] = x[:, 1] - x[:, 3] / 2  # y1 = cy - h/2\n",
    "    y[:, 2] = x[:, 0] + x[:, 2] / 2  # x2 = cx + w/2\n",
    "    y[:, 3] = x[:, 1] + x[:, 3] / 2  # y2 = cy + h/2\n",
    "    return y\n",
    "\n",
    "# Function to perform pose estimation on a frame\n",
    "def detect_pose(model, img, device, conf_thres=0.25, iou_thres=0.45):\n",
    "    \"\"\"\n",
    "    Perform pose estimation on an image.\n",
    "    \n",
    "    Args:\n",
    "        model: YOLOv7-W6-Pose model\n",
    "        img (torch.Tensor): Preprocessed image tensor\n",
    "        device (torch.device): Device to run inference on\n",
    "        conf_thres (float): Confidence threshold\n",
    "        iou_thres (float): IoU threshold\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (Detections with pose keypoints, inference time)\n",
    "    \"\"\"\n",
    "    # Transfer to device\n",
    "    img = img.to(device)\n",
    "    \n",
    "    # Half precision\n",
    "    half = device.type != 'cpu'\n",
    "    if half:\n",
    "        img = img.half()\n",
    "    \n",
    "    # Get model output\n",
    "    try:\n",
    "        with torch.no_grad():  # Inference\n",
    "            start_time = time.time()\n",
    "            # The model might return a tuple or a single tensor\n",
    "            output = model(img)\n",
    "            end_time = time.time()\n",
    "        \n",
    "        # Handle different output types\n",
    "        if isinstance(output, tuple):\n",
    "            # If output is a tuple, use the first element\n",
    "            # This assumes the first element contains the detections\n",
    "            output = output[0]\n",
    "        \n",
    "        # Apply NMS\n",
    "        output = non_max_suppression_pose(output, conf_thres, iou_thres)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during detection: {e}\")\n",
    "        output = None\n",
    "        end_time = time.time()\n",
    "    \n",
    "    inference_time = end_time - start_time\n",
    "    \n",
    "    return output, inference_time\n",
    "\n",
    "# Extract pose keypoints from detections\n",
    "def extract_keypoints(detections, orig_shape, ratio, pad_x, pad_y):\n",
    "    \"\"\"\n",
    "    Extract and format pose keypoints from detections.\n",
    "    \n",
    "    Args:\n",
    "        detections (list): Detections from model after NMS\n",
    "        orig_shape (tuple): Original image shape (height, width)\n",
    "        ratio (float): Ratio from preprocessing\n",
    "        pad_x (int): Padding in x direction\n",
    "        pad_y (int): Padding in y direction\n",
    "        \n",
    "    Returns:\n",
    "        list: List of dictionaries containing keypoints and bounding boxes\n",
    "    \"\"\"\n",
    "    people = []\n",
    "    \n",
    "    # Check if detections is None or empty\n",
    "    if detections is None or len(detections) == 0 or len(detections[0]) == 0:\n",
    "        return people\n",
    "    \n",
    "    # Process each detection\n",
    "    for detection in detections[0]:\n",
    "        # Get confidence score\n",
    "        conf = detection[4].cpu().numpy()\n",
    "        \n",
    "        # Skip if confidence is below threshold\n",
    "        if conf < 0.25:\n",
    "            continue\n",
    "        \n",
    "        # Extract bounding box\n",
    "        box = detection[:4].cpu().numpy()\n",
    "        \n",
    "        # Adjust box coordinates for original image\n",
    "        box[0] = (box[0] - pad_x) / ratio\n",
    "        box[1] = (box[1] - pad_y) / ratio\n",
    "        box[2] = (box[2] - pad_x) / ratio\n",
    "        box[3] = (box[3] - pad_y) / ratio\n",
    "        \n",
    "        # Extract keypoints\n",
    "        keypoints = []\n",
    "        num_keypoints = 17\n",
    "        for i in range(num_keypoints):\n",
    "            # Extract x, y, confidence for each keypoint\n",
    "            kp_x = detection[6 + i * 3].cpu().numpy()\n",
    "            kp_y = detection[6 + i * 3 + 1].cpu().numpy()\n",
    "            kp_conf = detection[6 + i * 3 + 2].cpu().numpy()\n",
    "            \n",
    "            # Adjust coordinates for original image\n",
    "            kp_x = (kp_x - pad_x) / ratio\n",
    "            kp_y = (kp_y - pad_y) / ratio\n",
    "            \n",
    "            keypoints.append((kp_x, kp_y, kp_conf))\n",
    "        \n",
    "        # Store person data\n",
    "        person = {\n",
    "            'bbox': box,\n",
    "            'keypoints': keypoints,\n",
    "            'confidence': conf\n",
    "        }\n",
    "        \n",
    "        people.append(person)\n",
    "    \n",
    "    return people\n",
    "\n",
    "# Define the keypoint indices according to COCO format\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "# Function to draw pose skeleton on image\n",
    "def draw_pose(img, people):\n",
    "    \"\"\"\n",
    "    Draw pose skeleton and bounding boxes on the image.\n",
    "    \n",
    "    Args:\n",
    "        img (numpy.ndarray): Original image\n",
    "        people (list): List of dictionaries containing keypoints and bounding boxes\n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Image with pose skeleton drawn\n",
    "    \"\"\"\n",
    "    # Create a copy of the image\n",
    "    img_with_pose = img.copy()\n",
    "    \n",
    "    # Define connections between keypoints\n",
    "    skeleton = [\n",
    "        ('nose', 'left_eye'), ('nose', 'right_eye'), ('left_eye', 'left_ear'),\n",
    "        ('right_eye', 'right_ear'), ('left_shoulder', 'right_shoulder'),\n",
    "        ('left_shoulder', 'left_hip'), ('right_shoulder', 'right_hip'),\n",
    "        ('left_hip', 'right_hip'), ('left_shoulder', 'left_elbow'),\n",
    "        ('left_elbow', 'left_wrist'), ('right_shoulder', 'right_elbow'),\n",
    "        ('right_elbow', 'right_wrist'), ('left_hip', 'left_knee'),\n",
    "        ('left_knee', 'left_ankle'), ('right_hip', 'right_knee'),\n",
    "        ('right_knee', 'right_ankle')\n",
    "    ]\n",
    "    \n",
    "    # Colors for visualization\n",
    "    colors = {\n",
    "        'bbox': (0, 255, 0),  # Green\n",
    "        'keypoints': (0, 0, 255),  # Red\n",
    "        'skeleton': (255, 0, 0)  # Blue\n",
    "    }\n",
    "    \n",
    "    # Draw each person\n",
    "    for person in people:\n",
    "        # Draw bounding box\n",
    "        box = person['bbox'].astype(int)\n",
    "        cv2.rectangle(img_with_pose, (box[0], box[1]), (box[2], box[3]), colors['bbox'], 2)\n",
    "        \n",
    "        keypoints = person['keypoints']\n",
    "        \n",
    "        # Draw keypoints\n",
    "        for i, (x, y, conf) in enumerate(keypoints):\n",
    "            if conf > 0.25:  # Only draw keypoints with sufficient confidence\n",
    "                cv2.circle(img_with_pose, (int(x), int(y)), 5, colors['keypoints'], -1)\n",
    "        \n",
    "        # Draw skeleton\n",
    "        for kp1_name, kp2_name in skeleton:\n",
    "            kp1_idx = KEYPOINT_DICT[kp1_name]\n",
    "            kp2_idx = KEYPOINT_DICT[kp2_name]\n",
    "            \n",
    "            x1, y1, conf1 = keypoints[kp1_idx]\n",
    "            x2, y2, conf2 = keypoints[kp2_idx]\n",
    "            \n",
    "            if conf1 > 0.25 and conf2 > 0.25:  # Only draw connections with confident keypoints\n",
    "                cv2.line(img_with_pose, (int(x1), int(y1)), (int(x2), int(y2)), colors['skeleton'], 2)\n",
    "    \n",
    "    return img_with_pose\n",
    "\n",
    "# Function to process a frame with pose estimation\n",
    "def process_frame_with_pose(frame, model, device):\n",
    "    \"\"\"\n",
    "    Process a frame with pose estimation.\n",
    "    \n",
    "    Args:\n",
    "        frame (numpy.ndarray): Input frame\n",
    "        model: YOLOv7-W6-Pose model\n",
    "        device (torch.device): Device to run inference on\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (Processed frame with pose visualization, people detected with keypoints)\n",
    "    \"\"\"\n",
    "    # Preprocess the frame\n",
    "    input_tensor, ratio, pad_x, pad_y = preprocess_frame(frame, CONFIG['input_size'])\n",
    "    \n",
    "    # Perform pose detection\n",
    "    detections, inference_time = detect_pose(\n",
    "        model, \n",
    "        input_tensor, \n",
    "        device, \n",
    "        CONFIG['confidence_threshold'], \n",
    "        CONFIG['iou_threshold']\n",
    "    )\n",
    "    \n",
    "    # Extract keypoints\n",
    "    people = extract_keypoints(detections, frame.shape, ratio, pad_x, pad_y)\n",
    "    \n",
    "    # Draw pose skeleton on the frame\n",
    "    frame_with_pose = draw_pose(frame, people)\n",
    "    \n",
    "    # Add inference time to the frame\n",
    "    cv2.putText(\n",
    "        frame_with_pose, \n",
    "        f'Inference: {inference_time*1000:.1f}ms', \n",
    "        (10, 30), \n",
    "        cv2.FONT_HERSHEY_SIMPLEX, \n",
    "        1, \n",
    "        (0, 255, 0), \n",
    "        2\n",
    "    )\n",
    "    \n",
    "    return frame_with_pose, people\n",
    "\n",
    "# Test pose estimation if running this cell directly\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Only import if this file is run directly\n",
    "#     from video_processing import process_video_source\n",
    "    \n",
    "#     # Load the model\n",
    "#     model = load_model(CONFIG['model_path'], CONFIG['device'])\n",
    "    \n",
    "#     # Define a function to process each frame with the model\n",
    "#     def process_frame_func(frame):\n",
    "#         frame_with_pose, people = process_frame_with_pose(frame, model, CONFIG['device'])\n",
    "#         return frame_with_pose, False  # No fall detection yet\n",
    "    \n",
    "#     # Process video from webcam\n",
    "#     process_video_source(0, process_frame_func, True, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2089854b-eaa6-4fb0-b41e-0ac58de8b78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Fall Detection Algorithm - Including Fall vs. Lying Down Differentiation\n",
    "\n",
    "class FallDetector:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the fall detector with tracking for speed calculation.\"\"\"\n",
    "        self.prev_keypoints = None\n",
    "    \n",
    "    def calculate_length_factor(self, keypoints):\n",
    "        \"\"\"\n",
    "        Calculate length factor based on the shoulder-to-torso distance.\n",
    "        \n",
    "        This implements Equation 1 from page 9 of the journal:\n",
    "        Lfactor = √(xl - xTl)² + (yl - yTl)²\n",
    "        \n",
    "        Args:\n",
    "            keypoints (list): List of keypoint tuples (x, y, confidence)\n",
    "            \n",
    "        Returns:\n",
    "            float: Length factor for distance calculations\n",
    "        \"\"\"\n",
    "        # Get left shoulder and left hip (torso) keypoints\n",
    "        left_shoulder = keypoints[KEYPOINT_DICT['left_shoulder']]\n",
    "        left_hip = keypoints[KEYPOINT_DICT['left_hip']]\n",
    "        \n",
    "        # Calculate Euclidean distance as in Equation 1\n",
    "        shoulder_x, shoulder_y = left_shoulder[0], left_shoulder[1]\n",
    "        hip_x, hip_y = left_hip[0], left_hip[1]\n",
    "        \n",
    "        length_factor = math.sqrt((shoulder_x - hip_x)**2 + (shoulder_y - hip_y)**2)\n",
    "        return length_factor\n",
    "    \n",
    "    def calculate_vertical_speed(self, prev_keypoints, curr_keypoints):\n",
    "        \"\"\"\n",
    "        Calculate vertical speed of movement to differentiate falls from lying down.\n",
    "        \n",
    "        As described on page 10: \"The speed of key body points is calculated by\n",
    "        measuring the displacement between their positions in consecutive frames.\"\n",
    "        \n",
    "        Args:\n",
    "            prev_keypoints (list): Previous frame keypoints\n",
    "            curr_keypoints (list): Current frame keypoints\n",
    "            \n",
    "        Returns:\n",
    "            float: Vertical speed (displacement between frames)\n",
    "        \"\"\"\n",
    "        if prev_keypoints is None:\n",
    "            return 0\n",
    "        \n",
    "        # Use shoulders to calculate vertical speed as mentioned in the paper\n",
    "        prev_left_shoulder = prev_keypoints[KEYPOINT_DICT['left_shoulder']]\n",
    "        curr_left_shoulder = curr_keypoints[KEYPOINT_DICT['left_shoulder']]\n",
    "        \n",
    "        # Calculate vertical displacement\n",
    "        vertical_displacement = abs(curr_left_shoulder[1] - prev_left_shoulder[1])\n",
    "        return vertical_displacement\n",
    "    \n",
    "    def calculate_torso_angle(self, keypoints):\n",
    "        \"\"\"\n",
    "        Calculate angle between torso and vertical.\n",
    "        \n",
    "        As mentioned on page 10: \"A threshold of 45 degrees is used in the code.\n",
    "        If the angle between the torso and legs drops below this value, it indicates\n",
    "        that the person's body is approaching a horizontal position...\"\n",
    "        \n",
    "        Args:\n",
    "            keypoints (list): List of keypoint tuples (x, y, confidence)\n",
    "            \n",
    "        Returns:\n",
    "            float: Angle in degrees\n",
    "        \"\"\"\n",
    "        # Get shoulder and hip keypoints to define torso\n",
    "        left_shoulder = keypoints[KEYPOINT_DICT['left_shoulder']]\n",
    "        right_shoulder = keypoints[KEYPOINT_DICT['right_shoulder']]\n",
    "        left_hip = keypoints[KEYPOINT_DICT['left_hip']]\n",
    "        right_hip = keypoints[KEYPOINT_DICT['right_hip']]\n",
    "        \n",
    "        # Calculate midpoints\n",
    "        mid_shoulder_x = (left_shoulder[0] + right_shoulder[0]) / 2\n",
    "        mid_shoulder_y = (left_shoulder[1] + right_shoulder[1]) / 2\n",
    "        mid_hip_x = (left_hip[0] + right_hip[0]) / 2\n",
    "        mid_hip_y = (left_hip[1] + right_hip[1]) / 2\n",
    "        \n",
    "        # Calculate torso vector\n",
    "        torso_x = mid_shoulder_x - mid_hip_x\n",
    "        torso_y = mid_shoulder_y - mid_hip_y\n",
    "        \n",
    "        # Calculate angle with vertical (y-axis in image coordinates)\n",
    "        # Note: In image coordinates, y increases downward\n",
    "        angle_rad = math.atan2(torso_x, torso_y)  # Angle with vertical\n",
    "        angle_deg = math.degrees(abs(angle_rad))\n",
    "        \n",
    "        return angle_deg\n",
    "    \n",
    "    def detect_fall(self, people):\n",
    "        \"\"\"\n",
    "        Detect if a fall has occurred based on pose keypoints.\n",
    "        \n",
    "        This implements the fall detection algorithm described in the paper\n",
    "        on pages 9-10, including the differentiation between falls and lying down.\n",
    "        \n",
    "        Args:\n",
    "            people (list): List of dictionaries containing keypoints and bounding boxes\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if fall detected, False otherwise\n",
    "        \"\"\"\n",
    "        # If no people detected, return False\n",
    "        if not people:\n",
    "            self.prev_keypoints = None\n",
    "            return False\n",
    "        \n",
    "        # Use the first person detected\n",
    "        person = people[0]\n",
    "        keypoints = person['keypoints']\n",
    "        \n",
    "        # Get relevant keypoints\n",
    "        left_shoulder = keypoints[KEYPOINT_DICT['left_shoulder']]\n",
    "        right_shoulder = keypoints[KEYPOINT_DICT['right_shoulder']]\n",
    "        left_hip = keypoints[KEYPOINT_DICT['left_hip']]\n",
    "        right_hip = keypoints[KEYPOINT_DICT['right_hip']]\n",
    "        left_ankle = keypoints[KEYPOINT_DICT['left_ankle']]\n",
    "        right_ankle = keypoints[KEYPOINT_DICT['right_ankle']]\n",
    "        \n",
    "        # 1. Calculate the length factor (Equation 1, page 9)\n",
    "        length_factor = self.calculate_length_factor(keypoints)\n",
    "        \n",
    "        # 2. Check if shoulders are lower than feet with adjustment (Equation 2, page 9)\n",
    "        # yl ≤ yFl + α·Lfactor\n",
    "        # Note: In image coordinates, y increases downward, so we flip the inequality\n",
    "        alpha = 0.1  # Small adjustment factor as mentioned in the paper\n",
    "        shoulder_below_feet = (\n",
    "            left_shoulder[1] >= left_ankle[1] - alpha * length_factor or\n",
    "            right_shoulder[1] >= right_ankle[1] - alpha * length_factor\n",
    "        )\n",
    "        \n",
    "        # 3. Calculate body height and width (Equations 3 & 4, page 10)\n",
    "        # Hbody = |yl - yFl|\n",
    "        body_height = abs(left_shoulder[1] - left_ankle[1])\n",
    "        # Wbody = |xl - xr|\n",
    "        body_width = abs(left_shoulder[0] - right_shoulder[0])\n",
    "        \n",
    "        # 4. Check fall condition based on body dimensions (Equation 5, page 10)\n",
    "        # Hbody < Wbody\n",
    "        orientation_fallen = body_height < body_width\n",
    "        \n",
    "        # 5. Differentiate between fall and lying down (as described on page 10)\n",
    "        # Calculate vertical speed between frames\n",
    "        vertical_speed = 0\n",
    "        if self.prev_keypoints is not None:\n",
    "            vertical_speed = self.calculate_vertical_speed(self.prev_keypoints, keypoints)\n",
    "        \n",
    "        # Calculate torso angle\n",
    "        torso_angle = self.calculate_torso_angle(keypoints)\n",
    "        \n",
    "        # From page 10: \"If the vertical speed exceeds a specific threshold, it indicates a fall...\"\n",
    "        # From page 10: \"A threshold of 45 degrees is used in the code.\"\n",
    "        is_rapid_movement = vertical_speed > 15  # Threshold for rapid movement\n",
    "        is_horizontal_position = torso_angle > 45  # Angle threshold as mentioned in paper\n",
    "        \n",
    "        # Fall is detected if basic conditions are met AND\n",
    "        # either the movement is rapid OR the body is in a horizontal position\n",
    "        fall_detected = (\n",
    "            shoulder_below_feet and \n",
    "            orientation_fallen and \n",
    "            (is_rapid_movement or is_horizontal_position)\n",
    "        )\n",
    "        \n",
    "        # Store current keypoints for next frame's speed calculation\n",
    "        self.prev_keypoints = keypoints\n",
    "        \n",
    "        return fall_detected\n",
    "    \n",
    "    def annotate_frame(self, frame, fall_detected):\n",
    "        \"\"\"\n",
    "        Annotate the frame with fall detection status.\n",
    "        \n",
    "        Args:\n",
    "            frame (numpy.ndarray): Frame to annotate\n",
    "            fall_detected (bool): Whether a fall is detected\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Annotated frame\n",
    "        \"\"\"\n",
    "        if fall_detected:\n",
    "            # Draw red text for fall detection\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                \"FALL DETECTED!\",\n",
    "                (50, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                1,\n",
    "                (0, 0, 255),  # Red color\n",
    "                2\n",
    "            )\n",
    "        \n",
    "        return frame\n",
    "\n",
    "# Function to process a frame with fall detection\n",
    "def process_frame_with_fall_detection(frame, model, device, fall_detector):\n",
    "    \"\"\"\n",
    "    Process a frame with pose estimation and fall detection.\n",
    "    \n",
    "    Args:\n",
    "        frame (numpy.ndarray): Input frame\n",
    "        model: YOLOv7-W6-Pose model\n",
    "        device (torch.device): Device to run inference on\n",
    "        fall_detector (FallDetector): Fall detector instance\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (Processed frame with annotations, fall detection status)\n",
    "    \"\"\"\n",
    "    # Perform pose estimation\n",
    "    frame_with_pose, people = process_frame_with_pose(frame, model, device)\n",
    "    \n",
    "    # Detect falls using the algorithm from the journal\n",
    "    fall_detected = fall_detector.detect_fall(people)\n",
    "    \n",
    "    # Annotate frame with fall detection status\n",
    "    annotated_frame = fall_detector.annotate_frame(frame_with_pose, fall_detected)\n",
    "    \n",
    "    return annotated_frame, fall_detected\n",
    "\n",
    "# # Test fall detection if running this cell directly\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Only import if this file is run directly\n",
    "#     from video_processing import process_video_source\n",
    "    \n",
    "#     # Load the model\n",
    "#     model = load_model(CONFIG['model_path'], CONFIG['device'])\n",
    "    \n",
    "#     # Create fall detector\n",
    "#     fall_detector = FallDetector()\n",
    "    \n",
    "#     # Define a function to process each frame with the model and fall detection\n",
    "#     def process_frame_func(frame):\n",
    "#         return process_frame_with_fall_detection(frame, model, device, fall_detector)\n",
    "    \n",
    "#     # Process video from webcam\n",
    "#     process_video_source(0, process_frame_func, True, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb8c42ce-d6f2-4a6f-b652-a48b965c0f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Main Program Integration\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function that integrates all components of the fall detection system.\n",
    "    \"\"\"\n",
    "    print(\"Fall Detection System using YOLOv7-W6-Pose\")\n",
    "    print(\"------------------------------------------\")\n",
    "    \n",
    "    # Parse command line arguments\n",
    "    parser = argparse.ArgumentParser(description='Fall Detection System using YOLOv7-W6-Pose')\n",
    "    parser.add_argument('--source', type=str, default='0', \n",
    "                        help='Source for video input (0 for webcam, or path to video file)')\n",
    "    parser.add_argument('--output', type=str, default=None, \n",
    "                        help='Path to save processed video (None for no saving)')\n",
    "    parser.add_argument('--display', action='store_true', \n",
    "                        help='Display video processing in real-time')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Convert source to int if it's a digit (camera index)\n",
    "    if args.source.isdigit():\n",
    "        args.source = int(args.source)\n",
    "    \n",
    "    # Check if model file exists\n",
    "    if not os.path.exists(CONFIG['model_path']):\n",
    "        print(f\"Error: Model file not found at {CONFIG['model_path']}\")\n",
    "        print(\"Please download it from: https://github.com/WongKinYiu/yolov7/releases\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Load YOLOv7-W6-Pose model\n",
    "        print(f\"Loading model from {CONFIG['model_path']}...\")\n",
    "        model = load_model(CONFIG['model_path'], CONFIG['device'])\n",
    "        print(\"Model loaded successfully.\")\n",
    "        \n",
    "        # Create fall detector\n",
    "        fall_detector = FallDetector()\n",
    "        \n",
    "        # Define frame processing function\n",
    "        def process_frame_func(frame):\n",
    "            return process_frame_with_fall_detection(frame, model, CONFIG['device'], fall_detector)\n",
    "        \n",
    "        # Process video source\n",
    "        print(f\"Processing video from source: {args.source}\")\n",
    "        if args.output:\n",
    "            print(f\"Output will be saved to: {args.output}\")\n",
    "        \n",
    "        process_video_source(args.source, process_frame_func, args.display, args.output)\n",
    "        \n",
    "        print(\"Processing completed.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# # Run the main program if this script is executed directly\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26604664-5430-4923-a112-43dc4f31b71c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on Le2i dataset: datasets/le2i\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\naufa\\AppData\\Local\\Temp\\ipykernel_19120\\3066927419.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(model_path, map_location=device)['model']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model models/yolov7-w6-pose.pt loaded to cuda:0\n",
      "\n",
      "Processing environment: All_Rooms\n",
      "Found 187 videos in All_Rooms\n",
      "Processing video (1).avi...\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "✗ False negative: Failed to detect fall in video (1).avi\n",
      "Processing video (10).avi...\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "✗ False negative: Failed to detect fall in video (10).avi\n",
      "Processing video (100).avi...\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "✗ False negative: Failed to detect fall in video (100).avi\n",
      "Processing video (101).avi...\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "✗ False negative: Failed to detect fall in video (101).avi\n",
      "Processing video (102).avi...\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n",
      "Prediction shape: torch.Size([1, 57375, 57])\n"
     ]
    }
   ],
   "source": [
    "def evaluate_on_le2i_dataset():\n",
    "    \"\"\"\n",
    "    Evaluate the fall detection system on the Le2i dataset with the specific directory structure.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Evaluation metrics\n",
    "    \"\"\"\n",
    "    # Hard-coded path to Le2i dataset\n",
    "    base_path = \"datasets/le2i\"\n",
    "    print(f\"Evaluating on Le2i dataset: {base_path}\")\n",
    "    \n",
    "    # Load the model\n",
    "    model = load_model(CONFIG['model_path'], CONFIG['device'])\n",
    "    \n",
    "    # Create fall detector\n",
    "    fall_detector = FallDetector()\n",
    "    \n",
    "    # Results tracking\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    \n",
    "    # Define the environment folders and their structures\n",
    "    environments = {\n",
    "        # \"All_Rooms\": {\n",
    "        #     \"videos_path\": os.path.join(base_path, \"All_Rooms\", \"Videos\"),\n",
    "        #     \"annotations_path\": os.path.join(base_path, \"All_Rooms\", \"Annotation_files\")\n",
    "        # },\n",
    "        \"Coffee_room_01\": {\n",
    "            \"videos_path\": os.path.join(base_path, \"Coffee_room_01\", \"Coffee_room_01\", \"Videos\"),\n",
    "            \"annotations_path\": os.path.join(base_path, \"Coffee_room_01\", \"Coffee_room_01\", \"Annotation_files\")\n",
    "        },\n",
    "        \"Coffee_room_02\": {\n",
    "            \"videos_path\": os.path.join(base_path, \"Coffee_room_02\", \"Coffee_room_02\", \"Videos\"),\n",
    "            \"annotations_path\": os.path.join(base_path, \"Coffee_room_02\", \"Coffee_room_02\", \"Annotations_files\")\n",
    "        },\n",
    "        \"Home_01\": {\n",
    "            \"videos_path\": os.path.join(base_path, \"Home_01\", \"Home_01\", \"Videos\"),\n",
    "            \"annotations_path\": os.path.join(base_path, \"Home_01\", \"Home_01\", \"Annotation_files\")\n",
    "        },\n",
    "        \"Home_02\": {\n",
    "            \"videos_path\": os.path.join(base_path, \"Home_02\", \"Home_02\", \"Videos\"),\n",
    "            \"annotations_path\": os.path.join(base_path, \"Home_02\", \"Home_02\", \"Annotation_files\")\n",
    "        },\n",
    "        \"Office\": {\n",
    "            \"videos_path\": os.path.join(base_path, \"Office\", \"Office\", \"Videos\"),\n",
    "            \"annotations_path\": os.path.join(base_path, \"Office\", \"Office\", \"Annotation_files\")\n",
    "        },\n",
    "        \"Lecture_room\": {\n",
    "            \"videos_path\": os.path.join(base_path, \"Lecture_room\", \"Lecture room\", \"Videos\"),\n",
    "            \"annotations_path\": os.path.join(base_path, \"Lecture_room\", \"Lecture room\", \"Annotation_files\")\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Process each environment\n",
    "    for env_name, paths in environments.items():\n",
    "        videos_path = paths[\"videos_path\"]\n",
    "        annotations_path = paths[\"annotations_path\"]\n",
    "        \n",
    "        print(f\"\\nProcessing environment: {env_name}\")\n",
    "        \n",
    "        # Check if the paths exist\n",
    "        if not os.path.exists(videos_path):\n",
    "            print(f\"Warning: Videos path does not exist: {videos_path}\")\n",
    "            continue\n",
    "            \n",
    "        if not os.path.exists(annotations_path):\n",
    "            print(f\"Warning: Annotations path does not exist: {annotations_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Get all video files\n",
    "        video_files = [f for f in os.listdir(videos_path) if f.endswith(('.mp4', '.avi'))]\n",
    "        print(f\"Found {len(video_files)} videos in {env_name}\")\n",
    "        \n",
    "        # Add counter for periodic model reloading\n",
    "        video_counter = 0\n",
    "        \n",
    "        for video_file in video_files:\n",
    "            video_counter += 1\n",
    "            \n",
    "            # Every 20 videos, reload the model to clear accumulated memory\n",
    "            if video_counter % 20 == 0:\n",
    "                print(f\"Processed {video_counter} videos, reloading model to clear memory...\")\n",
    "                del model\n",
    "                del fall_detector\n",
    "                \n",
    "                # Force garbage collection\n",
    "                if CONFIG['device'].type != 'cpu':\n",
    "                    torch.cuda.empty_cache()\n",
    "                import gc\n",
    "                gc.collect()\n",
    "                \n",
    "                # Reload model and fall detector\n",
    "                model = load_model(CONFIG['model_path'], CONFIG['device'])\n",
    "                fall_detector = FallDetector()\n",
    "            \n",
    "            video_path = os.path.join(videos_path, video_file)\n",
    "            print(f\"Processing {video_file}...\")\n",
    "            \n",
    "            # Find corresponding annotation file (same name with .txt extension)\n",
    "            annotation_file = os.path.splitext(video_file)[0] + '.txt'\n",
    "            annotation_path = os.path.join(annotations_path, annotation_file)\n",
    "            \n",
    "            if not os.path.exists(annotation_path):\n",
    "                print(f\"Warning: No annotation file found for {video_file}\")\n",
    "                continue\n",
    "            \n",
    "            # Parse the annotation file\n",
    "            fall_frames = []\n",
    "            try:\n",
    "                with open(annotation_path, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                    # Skip the first two lines (metadata)\n",
    "                    for line in lines[2:]:\n",
    "                        parts = line.strip().split(',')\n",
    "                        if len(parts) >= 2:\n",
    "                            frame_num = int(parts[0])\n",
    "                            action_code = int(parts[1])\n",
    "                            # Action codes 7 and 8 represent falling and fallen states\n",
    "                            if action_code in [7, 8]:\n",
    "                                fall_frames.append(frame_num)\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing annotation file {annotation_file}: {str(e)}\")\n",
    "                continue\n",
    "            \n",
    "            # Process the video\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            if not cap.isOpened():\n",
    "                print(f\"Error: Could not open video {video_path}\")\n",
    "                continue\n",
    "            \n",
    "            # Process frames and detect falls\n",
    "            frame_num = 0\n",
    "            system_detected_falls = []\n",
    "            \n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                frame_num += 1\n",
    "                \n",
    "                # Process frame for fall detection\n",
    "                _, people = process_frame_with_pose(frame, model, CONFIG['device'])\n",
    "                frame_fall_detected = fall_detector.detect_fall(people)\n",
    "                \n",
    "                if frame_fall_detected:\n",
    "                    system_detected_falls.append(frame_num)\n",
    "            \n",
    "            # Clean up\n",
    "            cap.release()\n",
    "            \n",
    "            # Memory cleanup after each video\n",
    "            if CONFIG['device'].type != 'cpu':\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            import gc\n",
    "            gc.collect()\n",
    "            \n",
    "            # Compare results with ground truth\n",
    "            # A fall is correctly detected if the system detected a fall in any frame\n",
    "            # where the ground truth indicates a fall\n",
    "            has_fall_in_ground_truth = len(fall_frames) > 0\n",
    "            system_detected_fall = len(system_detected_falls) > 0\n",
    "            \n",
    "            if has_fall_in_ground_truth and system_detected_fall:\n",
    "                true_positives += 1\n",
    "                print(f\"✓ True positive: Fall correctly detected in {video_file}\")\n",
    "            elif not has_fall_in_ground_truth and system_detected_fall:\n",
    "                false_positives += 1\n",
    "                print(f\"✗ False positive: Fall incorrectly detected in {video_file}\")\n",
    "            elif has_fall_in_ground_truth and not system_detected_fall:\n",
    "                false_negatives += 1\n",
    "                print(f\"✗ False negative: Failed to detect fall in {video_file}\")\n",
    "            else:  # not has_fall_in_ground_truth and not system_detected_fall\n",
    "                true_negatives += 1\n",
    "                print(f\"✓ True negative: Correctly identified no fall in {video_file}\")\n",
    "        \n",
    "        # Memory cleanup after each environment\n",
    "        if CONFIG['device'].type != 'cpu':\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        import gc\n",
    "        gc.collect()\n",
    "        print(f\"Completed environment: {env_name}, clearing memory...\")\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {}\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    total = true_positives + true_negatives + false_positives + false_negatives\n",
    "    if total > 0:\n",
    "        metrics['accuracy'] = (true_positives + true_negatives) / total * 100\n",
    "    \n",
    "    # Calculate precision\n",
    "    if true_positives + false_positives > 0:\n",
    "        metrics['precision'] = true_positives / (true_positives + false_positives) * 100\n",
    "    \n",
    "    # Calculate recall (sensitivity)\n",
    "    if true_positives + false_negatives > 0:\n",
    "        metrics['recall'] = true_positives / (true_positives + false_negatives) * 100\n",
    "    \n",
    "    # Calculate specificity\n",
    "    if true_negatives + false_positives > 0:\n",
    "        metrics['specificity'] = true_negatives / (true_negatives + false_positives) * 100\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    if 'precision' in metrics and 'recall' in metrics:\n",
    "        metrics['f1_score'] = 2 * (metrics['precision'] * metrics['recall']) / (metrics['precision'] + metrics['recall'])\n",
    "    \n",
    "    # Print confusion matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(f\"True Positives: {true_positives}\")\n",
    "    print(f\"False Positives: {false_positives}\")\n",
    "    print(f\"True Negatives: {true_negatives}\")\n",
    "    print(f\"False Negatives: {false_negatives}\")\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\nEvaluation Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric.capitalize()}: {value:.2f}%\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "metrics = evaluate_on_le2i_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
