{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74b5631e-6b29-44cd-a473-a5adea5e9986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original FPS: 25.0, Processing at 25 FPS, skipping 0 frames between processed frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\anaconda3\\envs\\yolov7_env\\lib\\site-packages\\torch\\functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3638.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Actual FPS: 19.24\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "from models.yolo import Model\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Add the custom class to the safe globals list\n",
    "torch.serialization.add_safe_globals([Model])\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load YOLOv7-pose model\n",
    "weights = torch.load('yolov7-w6-pose.pt', map_location=device, weights_only=False)\n",
    "model = weights['model']\n",
    "_ = model.float().eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.half().to(device)\n",
    "\n",
    "# Constants from the journal\n",
    "ALPHA = 0.5  # Adjustment factor for height threshold\n",
    "SPEED_THRESHOLD = 0.5  # Vertical speed threshold for fall detection (pixels/frame)\n",
    "ANGLE_THRESHOLD = 45  # Degrees threshold between torso and legs\n",
    "TARGET_FPS = 25  # Target frames per second for processing\n",
    "\n",
    "# Previous frame data for speed calculation\n",
    "prev_shoulder_y = None\n",
    "frame_count = 0\n",
    "\n",
    "def calculate_length_factor(shoulder, torso):\n",
    "    return np.sqrt((shoulder[0] - torso[0])**2 + (shoulder[1] - torso[1])**2)\n",
    "\n",
    "def calculate_angle(a, b, c):\n",
    "    \"\"\"Calculate angle between points a, b, c in degrees\"\"\"\n",
    "    ba = np.array(a) - np.array(b)\n",
    "    bc = np.array(c) - np.array(b)\n",
    "    \n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    angle = np.arccos(cosine_angle)\n",
    "    return np.degrees(angle)\n",
    "\n",
    "def detect_fall(keypoints, threshold=0.5):\n",
    "    global prev_shoulder_y, frame_count\n",
    "    \n",
    "    LEFT_SHOULDER, RIGHT_SHOULDER, LEFT_HIP, RIGHT_HIP, LEFT_KNEE, RIGHT_KNEE, LEFT_ANKLE, RIGHT_ANKLE = 5, 6, 11, 12, 13, 14, 15, 16\n",
    "    \n",
    "    keypoints_conf = [keypoints[i * 3: (i + 1) * 3] for i in [LEFT_SHOULDER, RIGHT_SHOULDER, LEFT_HIP, RIGHT_HIP, LEFT_KNEE, RIGHT_KNEE, LEFT_ANKLE, RIGHT_ANKLE]]\n",
    "    \n",
    "    # Check if keypoints are detected with sufficient confidence\n",
    "    if any(kp[2] < threshold for kp in keypoints_conf[:4] + keypoints_conf[6:8]):\n",
    "        return False\n",
    "    \n",
    "    # Current shoulder y position (average of left and right)\n",
    "    current_shoulder_y = (keypoints_conf[0][1] + keypoints_conf[1][1]) / 2\n",
    "    \n",
    "    # Calculate vertical speed if we have previous data\n",
    "    vertical_speed = 0\n",
    "    if prev_shoulder_y is not None and frame_count > 0:\n",
    "        vertical_speed = abs(current_shoulder_y - prev_shoulder_y) / frame_count\n",
    "    \n",
    "    prev_shoulder_y = current_shoulder_y\n",
    "    frame_count = 0 \n",
    "    \n",
    "    # Calculate length factor (from shoulder to hip)\n",
    "    length_factor = calculate_length_factor(keypoints_conf[0][:2], keypoints_conf[2][:2])\n",
    "    \n",
    "    # Check shoulder height relative to feet (with length factor adjustment)\n",
    "    shoulder_low = (keypoints_conf[0][1] <= keypoints_conf[6][1] + ALPHA * length_factor and\n",
    "                    keypoints_conf[1][1] <= keypoints_conf[7][1] + ALPHA * length_factor)\n",
    "    \n",
    "    # Calculate body dimensions\n",
    "    body_height = abs(keypoints_conf[0][1] - keypoints_conf[6][1])\n",
    "    body_width = abs(keypoints_conf[0][0] - keypoints_conf[1][0])\n",
    "    horizontal_position = body_height < body_width\n",
    "    \n",
    "    # Calculate angle between hip, knee and ankle (for left and right legs)\n",
    "    left_leg_angle = calculate_angle(keypoints_conf[2][:2], keypoints_conf[4][:2], keypoints_conf[6][:2])\n",
    "    right_leg_angle = calculate_angle(keypoints_conf[3][:2], keypoints_conf[5][:2], keypoints_conf[7][:2])\n",
    "    leg_angle = min(left_leg_angle, right_leg_angle)\n",
    "    \n",
    "    # Fall conditions from journal:\n",
    "    # 1. Shoulders low relative to feet (tapi ini berdasarkan bahu kanan dan kiri kalau lebih rendah dari vertikal dari kaki kanan dan kaki kiri)\n",
    "    # 2. Body in horizontal position (perbedaan lebar dan tinggi badan, algoritmanya kalkulasi kalau si height \n",
    "    # ini menjadi lebih kecil dibandingkan dengan width, ini menandakan kalau orangnya tuh jatuh)\n",
    "    # 3. Either rapid movement (speed) or acute angle between torso and legs\n",
    "    if shoulder_low and horizontal_position:\n",
    "        if vertical_speed > SPEED_THRESHOLD or leg_angle < ANGLE_THRESHOLD:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Path to video file\n",
    "video_path = \"C:\\\\Users\\\\LENOVO\\\\Documents\\\\A Skripsi\\\\datasets\\\\FallDataset\\\\Dataset\\\\Coffee_room_01\\\\Videos\\\\video (20).avi\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties\n",
    "original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Calculate skip frames to achieve 25 FPS\n",
    "if original_fps > 0:\n",
    "    skip_frames = max(1, int(round(original_fps / TARGET_FPS)))\n",
    "else:\n",
    "    skip_frames = 1  # Default if FPS info not available\n",
    "\n",
    "print(f\"Original FPS: {original_fps}, Processing at {TARGET_FPS} FPS, skipping {skip_frames-1} frames between processed frames\")\n",
    "\n",
    "frame_counter = 0\n",
    "start_time = time.time()\n",
    "processed_frames = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame_counter += 1\n",
    "    \n",
    "    # Skip frames to achieve target FPS\n",
    "    if frame_counter % skip_frames != 0:\n",
    "        continue\n",
    "    \n",
    "    processed_frames += 1\n",
    "    frame_count += 1\n",
    "    \n",
    "    image = letterbox(frame, 640, stride=64, auto=True)[0]\n",
    "    image_ = image.copy()\n",
    "    image = transforms.ToTensor()(image)\n",
    "    image = torch.tensor(np.array([image.numpy()]))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        image = image.half().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output, _ = model(image)\n",
    "        output = non_max_suppression_kpt(output, 0.25, 0.65, nc=model.yaml['nc'], nkpt=model.yaml['nkpt'], kpt_label=True)\n",
    "        output = output_to_keypoint(output)\n",
    "\n",
    "    nimg = image[0].permute(1, 2, 0) * 255\n",
    "    nimg = nimg.cpu().numpy().astype(np.uint8)\n",
    "    nimg = cv2.cvtColor(nimg, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    for idx in range(output.shape[0]):\n",
    "        keypoints = output[idx, 7:].T\n",
    "        plot_skeleton_kpts(nimg, keypoints, 3)\n",
    "\n",
    "        if detect_fall(keypoints):\n",
    "            cv2.putText(nimg, \"Fall Detected!\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    # Calculate and display actual processing FPS\n",
    "    elapsed_time = time.time() - start_time\n",
    "    current_fps = processed_frames / elapsed_time if elapsed_time > 0 else 0\n",
    "    cv2.putText(nimg, f\"FPS: {current_fps:.1f}\", (50, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow(\"Fall Detection\", nimg)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Calculate final processing statistics\n",
    "total_time = time.time() - start_time\n",
    "actual_fps = processed_frames / total_time if total_time > 0 else 0\n",
    "print(f\"Processing complete. Actual FPS: {actual_fps:.2f}\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fff2de4a-5dd2-48b2-88bb-6a7d377619ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original FPS: 25.0, Processing at 25 FPS, skipping 0 frames between processed frames\n",
      "Processing complete. Actual FPS: 20.65\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "from models.yolo import Model\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Add the custom class to the safe globals list\n",
    "torch.serialization.add_safe_globals([Model])\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load YOLOv7-pose model\n",
    "weights = torch.load('yolov7-w6-pose.pt', map_location=device, weights_only=False)\n",
    "model = weights['model']\n",
    "_ = model.float().eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.half().to(device)\n",
    "\n",
    "# Constants from the journal\n",
    "ALPHA = 0.5  # Adjustment factor for height threshold\n",
    "SPEED_THRESHOLD = 0.5  # Vertical speed threshold for fall detection (pixels/frame)\n",
    "ANGLE_THRESHOLD = 45  # Degrees threshold between torso and legs\n",
    "TARGET_FPS = 25  # Target frames per second for processing\n",
    "\n",
    "# Previous frame data for speed calculation\n",
    "prev_shoulder_y = None\n",
    "frame_count = 0\n",
    "\n",
    "def calculate_length_factor(shoulder, torso):\n",
    "    return np.sqrt((shoulder[0] - torso[0])**2 + (shoulder[1] - torso[1])**2)\n",
    "\n",
    "def calculate_angle(a, b, c):\n",
    "    # Calculate angle between points a, b, c in degrees\n",
    "    ba = np.array(a) - np.array(b)\n",
    "    bc = np.array(c) - np.array(b)\n",
    "    \n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    angle = np.arccos(cosine_angle)\n",
    "    return np.degrees(angle)\n",
    "\n",
    "def detect_fall(keypoints, threshold=0.5):\n",
    "    global prev_shoulder_y, prev_frame_time, fall_start_time\n",
    "    \n",
    "    # Keypoint indices\n",
    "    NOSE = 0\n",
    "    LEFT_SHOULDER, RIGHT_SHOULDER = 5, 6\n",
    "    LEFT_HIP, RIGHT_HIP = 11, 12\n",
    "    LEFT_ANKLE, RIGHT_ANKLE = 15, 16\n",
    "    LEFT_KNEE, RIGHT_KNEE = 13, 14\n",
    "\n",
    "    # Initialize default return values\n",
    "    is_fall = False\n",
    "    current_state = \"normal\"\n",
    "    conditions = []\n",
    "    \n",
    "    try:\n",
    "        # Extract keypoints with confidence check\n",
    "        kp = {}\n",
    "        for name, idx in [('nose', NOSE), ('left_shoulder', LEFT_SHOULDER),\n",
    "                         ('right_shoulder', RIGHT_SHOULDER), ('left_hip', LEFT_HIP),\n",
    "                         ('right_hip', RIGHT_HIP), ('left_knee', LEFT_KNEE),\n",
    "                         ('right_knee', RIGHT_KNEE), ('left_ankle', LEFT_ANKLE),\n",
    "                         ('right_ankle', RIGHT_ANKLE)]:\n",
    "            kp[name] = keypoints[idx*3:(idx+1)*3]\n",
    "            if kp[name][2] < threshold:\n",
    "                return False, \"low_confidence\", [\"Low confidence in keypoints\"]\n",
    "\n",
    "        # 1. Calculate critical distances\n",
    "        torso_length = math.sqrt((kp['left_shoulder'][0]-kp['left_hip'][0])**2 + \n",
    "                              (kp['left_shoulder'][1]-kp['left_hip'][1])**2)\n",
    "        shoulder_height = (kp['left_shoulder'][1] + kp['right_shoulder'][1]) / 2\n",
    "        feet_height = (kp['left_ankle'][1] + kp['right_ankle'][1]) / 2\n",
    "        head_height = kp['nose'][1]\n",
    "\n",
    "        # 2. Normalized height ratios\n",
    "        head_to_feet = abs(head_height - feet_height)\n",
    "        shoulder_to_feet = abs(shoulder_height - feet_height)\n",
    "        normalized_ratio = shoulder_to_feet / (head_to_feet + 1e-5)\n",
    "\n",
    "        # 3. Vertical speed calculation\n",
    "        current_time = time.time()\n",
    "        vertical_speed = 0\n",
    "        if prev_shoulder_y is not None and prev_frame_time is not None:\n",
    "            time_elapsed = current_time - prev_frame_time\n",
    "            if time_elapsed > 0:\n",
    "                vertical_speed = (shoulder_height - prev_shoulder_y) / time_elapsed\n",
    "        \n",
    "        # 4. Body orientation\n",
    "        body_width = abs(kp['left_shoulder'][0] - kp['right_shoulder'][0])\n",
    "        body_height = head_to_feet\n",
    "        orientation_ratio = body_width / (body_height + 1e-5)\n",
    "\n",
    "        # 5. Leg angles\n",
    "        def calculate_angle(a, b, c):\n",
    "            ba = np.array(a[:2]) - np.array(b[:2])\n",
    "            bc = np.array(c[:2]) - np.array(b[:2])\n",
    "            cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "            return np.degrees(np.arccos(np.clip(cosine_angle, -1, 1)))\n",
    "\n",
    "        left_leg_angle = calculate_angle(kp['left_hip'], kp['left_knee'], kp['left_ankle'])\n",
    "        right_leg_angle = calculate_angle(kp['right_hip'], kp['right_knee'], kp['right_ankle'])\n",
    "        min_leg_angle = min(left_leg_angle, right_leg_angle)\n",
    "\n",
    "        # Fall Detection Conditions\n",
    "        conditions = []\n",
    "        \n",
    "        # Shoulders near feet\n",
    "        shoulder_foot_threshold = 0.8 * torso_length\n",
    "        if shoulder_height > feet_height - shoulder_foot_threshold:\n",
    "            conditions.append(\"shoulders_near_feet\")\n",
    "        \n",
    "        # Rapid downward movement\n",
    "        if vertical_speed > SPEED_THRESHOLD:\n",
    "            conditions.append(f\"rapid_downward_{vertical_speed:.1f}px/s\")\n",
    "        \n",
    "        # Body orientation\n",
    "        if orientation_ratio > 1.2:\n",
    "            conditions.append(\"horizontal_posture\")\n",
    "        \n",
    "        # Leg collapse\n",
    "        if min_leg_angle < ANGLE_THRESHOLD:\n",
    "            conditions.append(f\"legs_collapsed_{min_leg_angle:.0f}deg\")\n",
    "\n",
    "        # Fall state determination\n",
    "        if \"shoulders_near_feet\" in conditions:\n",
    "            if any(\"rapid_downward\" in cond for cond in conditions):\n",
    "                current_state = \"falling\"\n",
    "                fall_start_time = current_time\n",
    "            elif \"horizontal_posture\" in conditions and current_time - fall_start_time < 1.0:\n",
    "                current_state = \"fallen\"\n",
    "        \n",
    "        # Final decision\n",
    "        if current_state == \"fallen\":\n",
    "            is_fall = True\n",
    "        elif len(conditions) >= 2 and \"shoulders_near_feet\" in conditions:\n",
    "            is_fall = True\n",
    "        \n",
    "        # Update tracking variables\n",
    "        prev_shoulder_y = shoulder_height\n",
    "        prev_frame_time = current_time\n",
    "        \n",
    "    except Exception as e:\n",
    "        conditions = [f\"Error: {str(e)}\"]\n",
    "        return False, \"error\", conditions\n",
    "    \n",
    "    return is_fall, current_state, conditions\n",
    "\n",
    "# Path to video file\n",
    "video_path = \"C:\\\\Users\\\\LENOVO\\\\Documents\\\\A Skripsi\\\\datasets\\\\FallDataset\\\\Dataset\\\\Coffee_room_01\\\\Videos\\\\video (1).avi\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video properties\n",
    "original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Calculate skip frames to achieve 25 FPS\n",
    "if original_fps > 0:\n",
    "    skip_frames = max(1, int(round(original_fps / TARGET_FPS)))\n",
    "else:\n",
    "    skip_frames = 1  # Default if FPS info not available\n",
    "\n",
    "print(f\"Original FPS: {original_fps}, Processing at {TARGET_FPS} FPS, skipping {skip_frames-1} frames between processed frames\")\n",
    "\n",
    "# Create video writer to maintain 25 FPS output\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('output.avi', fourcc, TARGET_FPS, (640, 640))\n",
    "\n",
    "frame_counter = 0\n",
    "start_time = time.time()\n",
    "processed_frames = 0\n",
    "last_frame_time = time.time()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame_counter += 1\n",
    "    \n",
    "    # Skip frames to achieve target FPS\n",
    "    if frame_counter % skip_frames != 0:\n",
    "        continue\n",
    "    \n",
    "    # Calculate time to sleep to maintain 25 FPS\n",
    "    current_time = time.time()\n",
    "    elapsed = current_time - last_frame_time\n",
    "    sleep_time = max(0, (1.0/TARGET_FPS) - elapsed)\n",
    "    time.sleep(sleep_time)\n",
    "    last_frame_time = time.time()\n",
    "    \n",
    "    processed_frames += 1\n",
    "    frame_count += 1\n",
    "    \n",
    "    image = letterbox(frame, 640, stride=64, auto=True)[0]\n",
    "    image_ = image.copy()\n",
    "    image = transforms.ToTensor()(image)\n",
    "    image = torch.tensor(np.array([image.numpy()]))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        image = image.half().to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output, _ = model(image)\n",
    "        output = non_max_suppression_kpt(output, 0.25, 0.65, nc=model.yaml['nc'], nkpt=model.yaml['nkpt'], kpt_label=True)\n",
    "        output = output_to_keypoint(output)\n",
    "\n",
    "    nimg = image[0].permute(1, 2, 0) * 255\n",
    "    nimg = nimg.cpu().numpy().astype(np.uint8)\n",
    "    nimg = cv2.cvtColor(nimg, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    for idx in range(output.shape[0]):\n",
    "        keypoints = output[idx, 7:].T\n",
    "        plot_skeleton_kpts(nimg, keypoints, 3)\n",
    "    \n",
    "        # Updated to handle 3 return values\n",
    "        fall_detected, state, conditions = detect_fall(keypoints)\n",
    "        \n",
    "        if fall_detected:\n",
    "            cv2.putText(nimg, f\"FALL: {state}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            # Display all triggered conditions\n",
    "            for i, condition in enumerate(conditions):\n",
    "                cv2.putText(nimg, condition, (50, 90 + i*30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "        else:\n",
    "            cv2.putText(nimg, f\"State: {state}\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "            if conditions:  # Show conditions even when no fall\n",
    "                cv2.putText(nimg, f\"Conditions: {', '.join(conditions)}\", (50, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
    "    \n",
    "    # Calculate and display actual processing FPS\n",
    "    elapsed_time = time.time() - start_time\n",
    "    current_fps = processed_frames / elapsed_time if elapsed_time > 0 else 0\n",
    "    cv2.putText(nimg, f\"Processing FPS: {current_fps:.1f}\", (50, 110), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "    \n",
    "    # Write frame to output video\n",
    "    out.write(nimg)\n",
    "    \n",
    "    cv2.imshow(\"Fall Detection\", nimg)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Calculate final processing statistics\n",
    "total_time = time.time() - start_time\n",
    "actual_fps = processed_frames / total_time if total_time > 0 else 0\n",
    "print(f\"Processing complete. Actual FPS: {actual_fps:.2f}\")\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4447272e-7e13-4c6c-8a17-5ef851beccb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading annotations from: C:\\Users\\LENOVO\\Documents\\A Skripsi\\datasets\\FallDataset\\Datasets sudah Grouping\\A Gelap\\labels\n",
      "Successfully loaded 0 annotations\n",
      "\n",
      "Processing video: video (43)\n",
      "Fall frames in ground truth: No fall\n",
      "\n",
      "Processing video: video (44)\n",
      "Fall frames in ground truth: No fall\n",
      "\n",
      "Processing video: video (45)\n",
      "Fall frames in ground truth: No fall\n",
      "\n",
      "Processing video: video (46)\n",
      "Fall frames in ground truth: No fall\n",
      "\n",
      "Processing video: video (47)\n",
      "Fall frames in ground truth: No fall\n",
      "\n",
      "Processing video: video (48)\n",
      "Fall frames in ground truth: No fall\n",
      "\n",
      "Processing video: video (49)\n",
      "Fall frames in ground truth: No fall\n",
      "\n",
      "Processing video: video (50)\n",
      "Fall frames in ground truth: No fall\n",
      "\n",
      "Processing video: video (51)\n",
      "Fall frames in ground truth: No fall\n",
      "\n",
      "Evaluation Complete!\n",
      "Metrics saved to 'results/' directory\n",
      "\n",
      "=== Final Metrics ===\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "F1 Score: 0.0000\n",
      "Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "from models.yolo import Model\n",
    "import math\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load YOLOv7-pose model\n",
    "weights = torch.load('yolov7-w6-pose.pt', map_location=device, weights_only=False)\n",
    "model = weights['model'].float().eval().to(device)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.half()\n",
    "\n",
    "# Constants\n",
    "SPEED_THRESHOLD = 0.5\n",
    "ANGLE_THRESHOLD = 45\n",
    "TARGET_FPS = 25\n",
    "\n",
    "# ====== FIXED PATHS FOR YOUR DATASET ======\n",
    "BASE_PATH = r\"C:\\Users\\LENOVO\\Documents\\A Skripsi\\datasets\\FallDataset\\Datasets sudah Grouping\\A Gelap\"\n",
    "LABELS_PATH = os.path.join(BASE_PATH, \"labels\")\n",
    "VIDEOS_PATH = os.path.join(BASE_PATH, \"video\")\n",
    "\n",
    "# ================== Evaluation Metrics ==================\n",
    "class FallDetectionMetrics:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.true_positives = 0\n",
    "        self.false_positives = 0\n",
    "        self.true_negatives = 0\n",
    "        self.false_negatives = 0\n",
    "        self.frame_results = []\n",
    "    \n",
    "    def update(self, predicted, actual, frame_info=None):\n",
    "        if predicted and actual:\n",
    "            self.true_positives += 1\n",
    "        elif predicted and not actual:\n",
    "            self.false_positives += 1\n",
    "        elif not predicted and not actual:\n",
    "            self.true_negatives += 1\n",
    "        elif not predicted and actual:\n",
    "            self.false_negatives += 1\n",
    "        \n",
    "        if frame_info:\n",
    "            self.frame_results.append(frame_info)\n",
    "    \n",
    "    def calculate_metrics(self):\n",
    "        precision = self.true_positives / (self.true_positives + self.false_positives + 1e-9)\n",
    "        recall = self.true_positives / (self.true_positives + self.false_negatives + 1e-9)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
    "        accuracy = (self.true_positives + self.true_negatives) / (self.true_positives + self.false_positives + self.true_negatives + self.false_negatives + 1e-9)\n",
    "        \n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'accuracy': accuracy,\n",
    "            'true_positives': self.true_positives,\n",
    "            'false_positives': self.false_positives,\n",
    "            'true_negatives': self.true_negatives,\n",
    "            'false_negatives': self.false_negatives\n",
    "        }\n",
    "    \n",
    "    def save_results(self, output_dir=\"results\"):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        with open(os.path.join(output_dir, \"metrics.txt\"), \"w\") as f:\n",
    "            metrics = self.calculate_metrics()\n",
    "            for k, v in metrics.items():\n",
    "                f.write(f\"{k}: {v:.4f}\\n\")\n",
    "        \n",
    "        with open(os.path.join(output_dir, \"frame_results.csv\"), \"w\") as f:\n",
    "            f.write(\"video,frame,predicted,actual,state,conditions\\n\")\n",
    "            for res in self.frame_results:\n",
    "                f.write(f\"{res['video']},{res['frame_num']},{res['predicted']},{res['actual']},{res['state']},\\\"{'|'.join(res['conditions'])}\\\"\\n\")\n",
    "\n",
    "# ================== Annotation Loader ==================\n",
    "def load_annotations(annotation_dir):\n",
    "    annotations = {}\n",
    "    print(f\"Loading annotations from: {annotation_dir}\")\n",
    "    \n",
    "    try:\n",
    "        for ann_file in os.listdir(annotation_dir):\n",
    "            if ann_file.endswith(\".txt\"):\n",
    "                video_name = os.path.splitext(ann_file)[0]\n",
    "                annotation_path = os.path.join(annotation_dir, ann_file)\n",
    "                \n",
    "                with open(annotation_path, \"r\") as f:\n",
    "                    lines = f.readlines()\n",
    "                    if len(lines) >= 2:\n",
    "                        start = int(lines[0].strip())\n",
    "                        end = int(lines[1].strip())\n",
    "                        annotations[video_name] = (start, end) if (start != 0 or end != 0) else None\n",
    "        print(f\"Successfully loaded {len(annotations)} annotations\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading annotations: {str(e)}\")\n",
    "    \n",
    "    return annotations\n",
    "\n",
    "# ================== Fall Detection Logic ==================\n",
    "def detect_fall(keypoints, threshold=0.5):\n",
    "    NOSE = 0\n",
    "    L_SHOULDER, R_SHOULDER = 5, 6\n",
    "    L_HIP, R_HIP = 11, 12\n",
    "    L_ANKLE, R_ANKLE = 15, 16\n",
    "    \n",
    "    try:\n",
    "        kp = {\n",
    "            'nose': keypoints[NOSE*3:(NOSE+1)*3],\n",
    "            'l_shoulder': keypoints[L_SHOULDER*3:(L_SHOULDER+1)*3],\n",
    "            'r_shoulder': keypoints[R_SHOULDER*3:(R_SHOULDER+1)*3],\n",
    "            'l_hip': keypoints[L_HIP*3:(L_HIP+1)*3],\n",
    "            'r_hip': keypoints[R_HIP*3:(R_HIP+1)*3],\n",
    "            'l_ankle': keypoints[L_ANKLE*3:(L_ANKLE+1)*3],\n",
    "            'r_ankle': keypoints[R_ANKLE*3:(R_ANKLE+1)*3]\n",
    "        }\n",
    "        \n",
    "        for k, v in kp.items():\n",
    "            if v[2] < threshold:\n",
    "                return False, \"low_confidence\", [\"Low confidence\"]\n",
    "        \n",
    "        shoulder_y = (kp['l_shoulder'][1] + kp['r_shoulder'][1]) / 2\n",
    "        feet_y = (kp['l_ankle'][1] + kp['r_ankle'][1]) / 2\n",
    "        torso_length = np.sqrt((kp['l_shoulder'][0]-kp['l_hip'][0])**2 + (kp['l_shoulder'][1]-kp['l_hip'][1])**2)\n",
    "        \n",
    "        conditions = []\n",
    "        if shoulder_y > feet_y - 0.8 * torso_length:\n",
    "            conditions.append(\"shoulders_near_feet\")\n",
    "        \n",
    "        body_width = abs(kp['l_shoulder'][0] - kp['r_shoulder'][0])\n",
    "        body_height = abs(kp['nose'][1] - feet_y)\n",
    "        if body_width / (body_height + 1e-5) > 1.2:\n",
    "            conditions.append(\"horizontal_posture\")\n",
    "        \n",
    "        if len(conditions) >= 2:\n",
    "            return True, \"fallen\", conditions\n",
    "        return False, \"normal\", conditions\n",
    "    \n",
    "    except Exception as e:\n",
    "        return False, \"error\", [f\"Error: {str(e)}\"]\n",
    "\n",
    "# ================== Video Processing ==================\n",
    "def process_video(video_path, annotations, metrics):\n",
    "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "    fall_range = annotations.get(video_name)\n",
    "    \n",
    "    print(f\"\\nProcessing video: {video_name}\")\n",
    "    print(f\"Fall frames in ground truth: {fall_range if fall_range else 'No fall'}\")\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening {video_path}\")\n",
    "        return\n",
    "    \n",
    "    frame_count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        \n",
    "        # Process frame\n",
    "        img = letterbox(frame, 640, stride=64, auto=True)[0]\n",
    "        img = transforms.ToTensor()(img)\n",
    "        img = img.unsqueeze(0).to(device)\n",
    "        if torch.cuda.is_available():\n",
    "            img = img.half()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, _ = model(img)\n",
    "            output = non_max_suppression_kpt(output, 0.25, 0.65, nc=model.yaml['nc'], nkpt=model.yaml['nkpt'], kpt_label=True)\n",
    "            output = output_to_keypoint(output)\n",
    "        \n",
    "        # Visualize\n",
    "        nimg = img[0].permute(1, 2, 0).cpu().numpy() * 255\n",
    "        nimg = nimg.astype(np.uint8)\n",
    "        nimg = cv2.cvtColor(nimg, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        for idx in range(output.shape[0]):\n",
    "            kpts = output[idx, 7:].T\n",
    "            plot_skeleton_kpts(nimg, kpts, 3)\n",
    "            \n",
    "            # Detect fall\n",
    "            fall_detected, state, conditions = detect_fall(kpts)\n",
    "            actual_fall = fall_range is not None and fall_range[0] <= frame_count <= fall_range[1]\n",
    "            \n",
    "            # Update metrics\n",
    "            metrics.update(\n",
    "                fall_detected, \n",
    "                actual_fall,\n",
    "                {\n",
    "                    'video': video_name,\n",
    "                    'frame_num': frame_count,\n",
    "                    'predicted': fall_detected,\n",
    "                    'actual': actual_fall,\n",
    "                    'state': state,\n",
    "                    'conditions': conditions\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Display info\n",
    "            color = (0, 255, 0) if fall_detected == actual_fall else (0, 0, 255)\n",
    "            status = \"CORRECT\" if fall_detected == actual_fall else \"WRONG\"\n",
    "            cv2.putText(nimg, f\"Frame: {frame_count} | State: {state} | {status}\", (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            \n",
    "            if fall_range is None:\n",
    "                gt_text = \"GT: No fall\"\n",
    "            else:\n",
    "                gt_text = f\"GT Fall: {fall_range[0]}-{fall_range[1]}\"\n",
    "            cv2.putText(nimg, gt_text, (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "        \n",
    "        cv2.imshow(\"Fall Detection\", nimg)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# ================== Main Execution ==================\n",
    "if __name__ == \"__main__\":\n",
    "    # Load annotations\n",
    "    annotations = load_annotations(LABELS_PATH)\n",
    "    \n",
    "    # Initialize metrics\n",
    "    metrics = FallDetectionMetrics()\n",
    "    \n",
    "    # Process all videos in the test set\n",
    "    test_video_dir = os.path.join(VIDEOS_PATH, \"test\")\n",
    "    if not os.path.exists(test_video_dir):\n",
    "        test_video_dir = VIDEOS_PATH  # Fallback if no test subfolder exists\n",
    "    \n",
    "    for video_file in os.listdir(test_video_dir):\n",
    "        if video_file.endswith(\".avi\"):\n",
    "            video_path = os.path.join(test_video_dir, video_file)\n",
    "            process_video(video_path, annotations, metrics)\n",
    "    \n",
    "    # Save results\n",
    "    metrics.save_results()\n",
    "    print(\"\\nEvaluation Complete!\")\n",
    "    print(\"Metrics saved to 'results/' directory\")\n",
    "    \n",
    "    # Print summary\n",
    "    results = metrics.calculate_metrics()\n",
    "    print(\"\\n=== Final Metrics ===\")\n",
    "    print(f\"Precision: {results['precision']:.4f}\")\n",
    "    print(f\"Recall: {results['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {results['f1_score']:.4f}\")\n",
    "    print(f\"Accuracy: {results['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8474e200-89e8-44ad-9f4c-0882094a1f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Complete!\n",
      "Metrics saved to 'results/' directory\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "from models.yolo import Model\n",
    "import math\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load YOLOv7-pose model\n",
    "weights = torch.load('yolov7-w6-pose.pt', map_location=device, weights_only=False)\n",
    "model = weights['model'].float().eval().to(device)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.half()\n",
    "\n",
    "# Constants\n",
    "SPEED_THRESHOLD = 0.5  # Vertical speed threshold (pixels/frame)\n",
    "ANGLE_THRESHOLD = 45    # Degrees threshold between torso and legs\n",
    "TARGET_FPS = 25         # Target processing FPS\n",
    "\n",
    "# Dataset Paths (Update these!)\n",
    "LABELS_PATH = r\"C:\\Users\\LENOVO\\Documents\\A Skripsi\\datasets\\FallDataset\\Datasets sudah Grouping\\A Gelap\\labels\\test\"\n",
    "VIDEOS_PATH = r\"C:\\Users\\LENOVO\\Documents\\A Skripsi\\datasets\\FallDataset\\Datasets sudah Grouping\\A Gelap\\video\\test\"\n",
    "\n",
    "# ================== Evaluation Metrics Class ==================\n",
    "class FallDetectionMetrics:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.true_positives = 0\n",
    "        self.false_positives = 0\n",
    "        self.true_negatives = 0\n",
    "        self.false_negatives = 0\n",
    "        self.frame_results = []\n",
    "    \n",
    "    def update(self, predicted, actual, frame_info=None):\n",
    "        if predicted and actual:\n",
    "            self.true_positives += 1\n",
    "        elif predicted and not actual:\n",
    "            self.false_positives += 1\n",
    "        elif not predicted and not actual:\n",
    "            self.true_negatives += 1\n",
    "        elif not predicted and actual:\n",
    "            self.false_negatives += 1\n",
    "        \n",
    "        if frame_info:\n",
    "            self.frame_results.append(frame_info)\n",
    "    \n",
    "    def calculate_metrics(self):\n",
    "        precision = self.true_positives / (self.true_positives + self.false_positives + 1e-9)\n",
    "        recall = self.true_positives / (self.true_positives + self.false_negatives + 1e-9)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
    "        accuracy = (self.true_positives + self.true_negatives) / (self.true_positives + self.false_positives + self.true_negatives + self.false_negatives + 1e-9)\n",
    "        \n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'accuracy': accuracy,\n",
    "            'true_positives': self.true_positives,\n",
    "            'false_positives': self.false_positives,\n",
    "            'true_negatives': self.true_negatives,\n",
    "            'false_negatives': self.false_negatives\n",
    "        }\n",
    "    \n",
    "    def save_results(self, output_dir=\"results\"):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Save metrics\n",
    "        with open(os.path.join(output_dir, \"metrics.txt\"), \"w\") as f:\n",
    "            metrics = self.calculate_metrics()\n",
    "            for k, v in metrics.items():\n",
    "                f.write(f\"{k}: {v:.4f}\\n\")\n",
    "        \n",
    "        # Save frame-level results\n",
    "        with open(os.path.join(output_dir, \"frame_results.csv\"), \"w\") as f:\n",
    "            f.write(\"video,frame,predicted,actual,state,conditions\\n\")\n",
    "            for res in self.frame_results:\n",
    "                f.write(f\"{res['video']},{res['frame_num']},{res['predicted']},{res['actual']},{res['state']},\\\"{'|'.join(res['conditions'])}\\\"\\n\")\n",
    "\n",
    "# ================== Annotation Loader ==================\n",
    "def load_annotations(annotation_dir):\n",
    "    annotations = {}\n",
    "    for ann_file in os.listdir(annotation_dir):\n",
    "        if ann_file.endswith(\".txt\"):\n",
    "            video_name = os.path.splitext(ann_file)[0]\n",
    "            with open(os.path.join(annotation_dir, ann_file), \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "                if len(lines) >= 2:\n",
    "                    start = int(lines[0].strip())\n",
    "                    end = int(lines[1].strip())\n",
    "                    annotations[video_name] = (start, end) if (start != 0 or end != 0) else None\n",
    "    return annotations\n",
    "\n",
    "# ================== Fall Detection Logic ==================\n",
    "def detect_fall(keypoints, threshold=0.5):\n",
    "    # Keypoint indices\n",
    "    NOSE = 0\n",
    "    L_SHOULDER, R_SHOULDER = 5, 6\n",
    "    L_HIP, R_HIP = 11, 12\n",
    "    L_ANKLE, R_ANKLE = 15, 16\n",
    "    \n",
    "    try:\n",
    "        # Extract keypoints\n",
    "        kp = {\n",
    "            'nose': keypoints[NOSE*3:(NOSE+1)*3],\n",
    "            'l_shoulder': keypoints[L_SHOULDER*3:(L_SHOULDER+1)*3],\n",
    "            'r_shoulder': keypoints[R_SHOULDER*3:(R_SHOULDER+1)*3],\n",
    "            'l_hip': keypoints[L_HIP*3:(L_HIP+1)*3],\n",
    "            'r_hip': keypoints[R_HIP*3:(R_HIP+1)*3],\n",
    "            'l_ankle': keypoints[L_ANKLE*3:(L_ANKLE+1)*3],\n",
    "            'r_ankle': keypoints[R_ANKLE*3:(R_ANKLE+1)*3]\n",
    "        }\n",
    "        \n",
    "        # Check confidence\n",
    "        for k, v in kp.items():\n",
    "            if v[2] < threshold:\n",
    "                return False, \"low_confidence\", [\"Low confidence\"]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        shoulder_y = (kp['l_shoulder'][1] + kp['r_shoulder'][1]) / 2\n",
    "        feet_y = (kp['l_ankle'][1] + kp['r_ankle'][1]) / 2\n",
    "        torso_length = np.sqrt((kp['l_shoulder'][0]-kp['l_hip'][0])**2 + (kp['l_shoulder'][1]-kp['l_hip'][1])**2)\n",
    "        \n",
    "        # Conditions\n",
    "        conditions = []\n",
    "        if shoulder_y > feet_y - 0.8 * torso_length:\n",
    "            conditions.append(\"shoulders_near_feet\")\n",
    "        \n",
    "        body_width = abs(kp['l_shoulder'][0] - kp['r_shoulder'][0])\n",
    "        body_height = abs(kp['nose'][1] - feet_y)\n",
    "        if body_width / (body_height + 1e-5) > 1.2:\n",
    "            conditions.append(\"horizontal_posture\")\n",
    "        \n",
    "        # Determine fall state\n",
    "        if len(conditions) >= 2:\n",
    "            return True, \"fallen\", conditions\n",
    "        return False, \"normal\", conditions\n",
    "    \n",
    "    except Exception as e:\n",
    "        return False, \"error\", [f\"Error: {str(e)}\"]\n",
    "\n",
    "# ================== Video Processing ==================\n",
    "def process_video(video_path, annotations, metrics):\n",
    "    video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "    fall_range = annotations.get(video_name)\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening {video_path}\")\n",
    "        return\n",
    "    \n",
    "    frame_count = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        \n",
    "        # Process frame\n",
    "        img = letterbox(frame, 640, stride=64, auto=True)[0]\n",
    "        img = transforms.ToTensor()(img)\n",
    "        img = img.unsqueeze(0).to(device)\n",
    "        if torch.cuda.is_available():\n",
    "            img = img.half()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, _ = model(img)\n",
    "            output = non_max_suppression_kpt(output, 0.25, 0.65, nc=model.yaml['nc'], nkpt=model.yaml['nkpt'], kpt_label=True)\n",
    "            output = output_to_keypoint(output)\n",
    "        \n",
    "        # Visualize\n",
    "        nimg = img[0].permute(1, 2, 0).cpu().numpy() * 255\n",
    "        nimg = nimg.astype(np.uint8)\n",
    "        nimg = cv2.cvtColor(nimg, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        for idx in range(output.shape[0]):\n",
    "            kpts = output[idx, 7:].T\n",
    "            plot_skeleton_kpts(nimg, kpts, 3)\n",
    "            \n",
    "            # Detect fall\n",
    "            fall_detected, state, conditions = detect_fall(kpts)\n",
    "            \n",
    "            # Compare with ground truth\n",
    "            actual_fall = fall_range is not None and fall_range[0] <= frame_count <= fall_range[1]\n",
    "            \n",
    "            # Update metrics\n",
    "            metrics.update(\n",
    "                fall_detected, \n",
    "                actual_fall,\n",
    "                {\n",
    "                    'video': video_name,\n",
    "                    'frame_num': frame_count,\n",
    "                    'predicted': fall_detected,\n",
    "                    'actual': actual_fall,\n",
    "                    'state': state,\n",
    "                    'conditions': conditions\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Display info\n",
    "            color = (0, 255, 0) if fall_detected == actual_fall else (0, 0, 255)\n",
    "            status = \"CORRECT\" if fall_detected == actual_fall else \"WRONG\"\n",
    "            cv2.putText(nimg, f\"Frame: {frame_count} | State: {state} | {status}\", (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            \n",
    "            if fall_range is None:\n",
    "                gt_text = \"GT: No fall\"\n",
    "            else:\n",
    "                gt_text = f\"GT Fall: {fall_range[0]}-{fall_range[1]}\"\n",
    "            cv2.putText(nimg, gt_text, (20, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "        \n",
    "        cv2.imshow(\"Fall Detection\", nimg)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# ================== Main Execution ==================\n",
    "if __name__ == \"__main__\":\n",
    "    # Load annotations\n",
    "    annotations = load_annotations(LABELS_PATH)\n",
    "    \n",
    "    # Initialize metrics\n",
    "    metrics = FallDetectionMetrics()\n",
    "    \n",
    "    # Process all test videos\n",
    "    for video_file in os.listdir(VIDEOS_PATH):\n",
    "        if video_file.endswith(\".avi\"):\n",
    "            video_path = os.path.join(VIDEOS_PATH, video_file)\n",
    "            process_video(video_path, annotations, metrics)\n",
    "    \n",
    "    # Save results\n",
    "    metrics.save_results()\n",
    "    print(\"\\nEvaluation Complete!\")\n",
    "    print(\"Metrics saved to 'results/' directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f105979d-8f71-485b-bb07-fa2117cadc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "Processing train set (Video: 42 videos)\n",
      "Loading annotations from: C:\\Users\\LENOVO\\Documents\\A Skripsi\\datasets\\FallDataset\\Datasets sudah Grouping\\A Gelap\\labels\\train\n",
      "Successfully loaded 42 annotations\n",
      "\n",
      "Processing video: video (1)\n",
      "Fall frames in ground truth: (144, 164)\n",
      "\n",
      "Processing video: video (10)\n",
      "Fall frames in ground truth: (135, 150)\n",
      "\n",
      "Processing video: video (11)\n",
      "Fall frames in ground truth: (137, 170)\n",
      "\n",
      "Processing video: video (12)\n",
      "Fall frames in ground truth: (161, 173)\n",
      "\n",
      "Processing video: video (13)\n",
      "Fall frames in ground truth: (156, 167)\n",
      "\n",
      "Processing video: video (14)\n",
      "Fall frames in ground truth: (186, 199)\n",
      "\n",
      "Processing video: video (15)\n",
      "Fall frames in ground truth: (170, 184)\n",
      "\n",
      "Processing video: video (16)\n",
      "Fall frames in ground truth: (186, 199)\n",
      "\n",
      "Processing video: video (17)\n",
      "Fall frames in ground truth: (154, 169)\n",
      "\n",
      "Processing video: video (18)\n",
      "Fall frames in ground truth: (135, 149)\n",
      "\n",
      "Processing video: video (19)\n",
      "Fall frames in ground truth: (129, 139)\n",
      "\n",
      "Processing video: video (2)\n",
      "Fall frames in ground truth: (120, 137)\n",
      "\n",
      "Processing video: video (20)\n",
      "Fall frames in ground truth: (147, 158)\n",
      "\n",
      "Processing video: video (21)\n",
      "Fall frames in ground truth: (148, 156)\n",
      "\n",
      "Processing video: video (22)\n",
      "Fall frames in ground truth: (151, 160)\n",
      "\n",
      "Processing video: video (23)\n",
      "Fall frames in ground truth: (165, 173)\n",
      "\n",
      "Processing video: video (24)\n",
      "Fall frames in ground truth: (131, 142)\n",
      "\n",
      "Processing video: video (25)\n",
      "Fall frames in ground truth: (173, 182)\n",
      "\n",
      "Processing video: video (26)\n",
      "Fall frames in ground truth: (130, 141)\n",
      "\n",
      "Processing video: video (27)\n",
      "Fall frames in ground truth: (168, 175)\n",
      "\n",
      "Processing video: video (28)\n",
      "Fall frames in ground truth: (169, 180)\n",
      "\n",
      "Processing video: video (29)\n",
      "Fall frames in ground truth: (159, 170)\n",
      "\n",
      "Processing video: video (3)\n",
      "Fall frames in ground truth: (122, 137)\n",
      "\n",
      "Processing video: video (30)\n",
      "Fall frames in ground truth: (123, 138)\n",
      "\n",
      "Processing video: video (31)\n",
      "Fall frames in ground truth: (200, 216)\n",
      "\n",
      "Processing video: video (32)\n",
      "Fall frames in ground truth: (87, 99)\n",
      "\n",
      "Processing video: video (33)\n",
      "Fall frames in ground truth: (127, 140)\n",
      "\n",
      "Processing video: video (34)\n",
      "Fall frames in ground truth: (157, 170)\n",
      "\n",
      "Processing video: video (35)\n",
      "Fall frames in ground truth: (136, 149)\n",
      "\n",
      "Processing video: video (36)\n",
      "Fall frames in ground truth: (124, 139)\n",
      "\n",
      "Processing video: video (37)\n",
      "Fall frames in ground truth: (129, 144)\n",
      "\n",
      "Processing video: video (38)\n",
      "Fall frames in ground truth: No fall\n",
      "\n",
      "Processing video: video (39)\n",
      "Fall frames in ground truth: No fall\n",
      "\n",
      "Processing video: video (4)\n",
      "Fall frames in ground truth: (149, 164)\n",
      "\n",
      "Processing video: video (40)\n",
      "Fall frames in ground truth: No fall\n",
      "\n",
      "Processing video: video (41)\n",
      "Fall frames in ground truth: No fall\n",
      "\n",
      "Processing video: video (42)\n",
      "Fall frames in ground truth: No fall\n",
      "\n",
      "Processing video: video (5)\n",
      "Fall frames in ground truth: (109, 125)\n",
      "\n",
      "Processing video: video (6)\n",
      "Fall frames in ground truth: (116, 140)\n",
      "\n",
      "Processing video: video (7)\n",
      "Fall frames in ground truth: (139, 154)\n",
      "\n",
      "Processing video: video (8)\n",
      "Fall frames in ground truth: (125, 139)\n",
      "\n",
      "Processing video: video (9)\n",
      "Fall frames in ground truth: (149, 163)\n",
      "train metrics saved to results_train/\n",
      "\n",
      "========================================\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\LENOVO\\\\Documents\\\\A Skripsi\\\\datasets\\\\FallDataset\\\\Datasets sudah Grouping\\\\A Gelap\\\\video\\\\valid'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 111\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m phase \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m40\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 111\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mphase\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m set (Video: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(PATHS[phase][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m'\u001b[39m]))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m videos)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# Load annotations\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     annotations \u001b[38;5;241m=\u001b[39m load_annotations(PATHS[phase][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:\\\\Users\\\\LENOVO\\\\Documents\\\\A Skripsi\\\\datasets\\\\FallDataset\\\\Datasets sudah Grouping\\\\A Gelap\\\\video\\\\valid'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "from models.yolo import Model\n",
    "import math\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load YOLOv7-pose model\n",
    "weights = torch.load('yolov7-w6-pose.pt', map_location=device, weights_only=False)\n",
    "model = weights['model'].float().eval().to(device)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.half()\n",
    "\n",
    "# Dataset paths (sesuaikan dengan struktur folder Anda)\n",
    "BASE_PATH = r\"C:\\Users\\LENOVO\\Documents\\A Skripsi\\datasets\\FallDataset\\Datasets sudah Grouping\\A Gelap\"\n",
    "PATHS = {\n",
    "    'train': {'video': os.path.join(BASE_PATH, 'video', 'train'), \n",
    "              'label': os.path.join(BASE_PATH, 'labels', 'train')},\n",
    "    'valid': {'video': os.path.join(BASE_PATH, 'video', 'valid'),\n",
    "              'label': os.path.join(BASE_PATH, 'labels', 'valid')},\n",
    "    'test': {'video': os.path.join(BASE_PATH, 'video', 'test'),\n",
    "             'label': os.path.join(BASE_PATH, 'labels', 'test')}\n",
    "}\n",
    "\n",
    "# ================== Enhanced Fall Detection ==================\n",
    "def detect_fall(keypoints, threshold=0.3):\n",
    "    # Keypoint indices\n",
    "    NOSE = 0\n",
    "    L_SHOULDER, R_SHOULDER = 5, 6\n",
    "    L_HIP, R_HIP = 11, 12\n",
    "    L_ANKLE, R_ANKLE = 15, 16\n",
    "    \n",
    "    try:\n",
    "        # Extract keypoints with lower confidence threshold\n",
    "        kp = {\n",
    "            'nose': keypoints[NOSE*3:(NOSE+1)*3],\n",
    "            'l_shoulder': keypoints[L_SHOULDER*3:(L_SHOULDER+1)*3],\n",
    "            'r_shoulder': keypoints[R_SHOULDER*3:(R_SHOULDER+1)*3],\n",
    "            'l_hip': keypoints[L_HIP*3:(L_HIP+1)*3],\n",
    "            'r_hip': keypoints[R_HIP*3:(R_HIP+1)*3],\n",
    "            'l_ankle': keypoints[L_ANKLE*3:(L_ANKLE+1)*3],\n",
    "            'r_ankle': keypoints[R_ANKLE*3:(R_ANKLE+1)*3]\n",
    "        }\n",
    "        \n",
    "        # Check if keypoints are detected\n",
    "        for k, v in kp.items():\n",
    "            if v[2] < threshold:\n",
    "                return False, \"low_confidence\", [\"Keypoint low confidence\"]\n",
    "        \n",
    "        # Calculate body parameters\n",
    "        shoulder_y = (kp['l_shoulder'][1] + kp['r_shoulder'][1]) / 2\n",
    "        feet_y = (kp['l_ankle'][1] + kp['r_ankle'][1]) / 2\n",
    "        torso_length = np.sqrt((kp['l_shoulder'][0]-kp['l_hip'][0])**2 + \n",
    "                         (kp['l_shoulder'][1]-kp['l_hip'][1])**2)\n",
    "        \n",
    "        # Enhanced conditions for dark environments\n",
    "        conditions = []\n",
    "        \n",
    "        # 1. Shoulder position relative to feet (adjusted for dark)\n",
    "        if shoulder_y > feet_y - 0.5 * torso_length:  # More tolerant threshold\n",
    "            conditions.append(\"shoulder_near_feet\")\n",
    "        \n",
    "        # 2. Body orientation\n",
    "        body_width = abs(kp['l_shoulder'][0] - kp['r_shoulder'][0])\n",
    "        body_height = abs(kp['nose'][1] - feet_y)\n",
    "        if body_width / (body_height + 1e-5) > 0.8:  # Reduced threshold\n",
    "            conditions.append(\"horizontal_pose\")\n",
    "        \n",
    "        # 3. Motion detection (fall speed)\n",
    "        global prev_shoulder_y, prev_frame_time\n",
    "        if prev_shoulder_y is not None and prev_frame_time is not None:\n",
    "            time_elapsed = time.time() - prev_frame_time\n",
    "            if time_elapsed > 0:\n",
    "                speed = (shoulder_y - prev_shoulder_y) / time_elapsed\n",
    "                if speed > 0.6:  # More sensitive to fast movements\n",
    "                    conditions.append(f\"high_speed_{speed:.1f}px\")\n",
    "        \n",
    "        # Update tracking variables\n",
    "        prev_shoulder_y = shoulder_y\n",
    "        prev_frame_time = time.time()\n",
    "        \n",
    "        # Decision making (at least 2 conditions)\n",
    "        if len(conditions) >= 2:\n",
    "            return True, \"fallen\", conditions\n",
    "        return False, \"normal\", conditions\n",
    "    \n",
    "    except Exception as e:\n",
    "        return False, \"error\", [f\"Error: {str(e)}\"]\n",
    "\n",
    "# ================== Main Execution ==================\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize metrics for each phase\n",
    "    metrics = {\n",
    "        'train': FallDetectionMetrics(),\n",
    "        'valid': FallDetectionMetrics(),\n",
    "        'test': FallDetectionMetrics()\n",
    "    }\n",
    "    \n",
    "    # Process all phases\n",
    "    for phase in ['train', 'valid', 'test']:\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"Processing {phase} set (Video: {len(os.listdir(PATHS[phase]['video']))} videos)\")\n",
    "        \n",
    "        # Load annotations\n",
    "        annotations = load_annotations(PATHS[phase]['label'])\n",
    "        \n",
    "        # Process videos\n",
    "        for video_file in os.listdir(PATHS[phase]['video']):\n",
    "            if video_file.endswith(\".avi\"):\n",
    "                video_path = os.path.join(PATHS[phase]['video'], video_file)\n",
    "                process_video(video_path, annotations, metrics[phase])\n",
    "        \n",
    "        # Save results\n",
    "        metrics[phase].save_results(f\"results_{phase}\")\n",
    "        print(f\"{phase} metrics saved to results_{phase}/\")\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\n=== FINAL RESULTS ===\")\n",
    "    for phase in ['train', 'valid', 'test']:\n",
    "        res = metrics[phase].calculate_metrics()\n",
    "        print(f\"\\n{phase.upper():<6} Precision: {res['precision']:.3f} | Recall: {res['recall']:.3f} | F1: {res['f1_score']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd32ff95-3c44-44ac-af99-96f2745b7f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
