{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f47659-f100-48c6-aa69-19f581d46b43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7531dc7d-695d-4f04-af90-4db52a4916c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading YOLOv7-W6-Pose model...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "# Add the parent directory to the path so Python can find your modules\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "sys.path.append(os.getcwd())  # Also add current directory\n",
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "import math\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "# Global variables for tracking motion between frames\n",
    "prev_keypoints = None\n",
    "prev_frame_time = None\n",
    "fall_history = []\n",
    "alert_timestamp = None\n",
    "\n",
    "def detect_fall(keypoints, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Enhanced fall detection algorithm based on YOLOv7-W6-Pose keypoints\n",
    "    as described in the journal paper \"Enhanced Fall Detection Using YOLOv7-W6-Pose for Real-Time Elderly Monitoring\"\n",
    "    \n",
    "    Args:\n",
    "        keypoints: Array of keypoints from YOLOv7-W6-Pose (shape: 17*3, where each keypoint has x, y, conf)\n",
    "        threshold: Confidence threshold for keypoint detection\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (is_fall, status, conditions)\n",
    "            - is_fall: Boolean indicating whether a fall is detected\n",
    "            - status: String description of the current state\n",
    "            - conditions: List of conditions that led to the fall detection\n",
    "    \"\"\"\n",
    "    global prev_keypoints, prev_frame_time, fall_history, alert_timestamp\n",
    "    \n",
    "    # Keypoint indices as defined in YOLOv7-pose\n",
    "    NOSE = 0\n",
    "    L_EYE, R_EYE = 1, 2\n",
    "    L_EAR, R_EAR = 3, 4\n",
    "    L_SHOULDER, R_SHOULDER = 5, 6\n",
    "    L_ELBOW, R_ELBOW = 7, 8\n",
    "    L_WRIST, R_WRIST = 9, 10\n",
    "    L_HIP, R_HIP = 11, 12\n",
    "    L_KNEE, R_KNEE = 13, 14\n",
    "    L_ANKLE, R_ANKLE = 15, 16\n",
    "    \n",
    "    try:\n",
    "        # Extract keypoints and organize them for easier access\n",
    "        kp = {\n",
    "            'nose': keypoints[NOSE*3:(NOSE+1)*3],\n",
    "            'l_eye': keypoints[L_EYE*3:(L_EYE+1)*3],\n",
    "            'r_eye': keypoints[R_EYE*3:(R_EYE+1)*3],\n",
    "            'l_ear': keypoints[L_EAR*3:(L_EAR+1)*3],\n",
    "            'r_ear': keypoints[R_EAR*3:(R_EAR+1)*3],\n",
    "            'l_shoulder': keypoints[L_SHOULDER*3:(L_SHOULDER+1)*3],\n",
    "            'r_shoulder': keypoints[R_SHOULDER*3:(R_SHOULDER+1)*3],\n",
    "            'l_elbow': keypoints[L_ELBOW*3:(L_ELBOW+1)*3],\n",
    "            'r_elbow': keypoints[R_ELBOW*3:(R_ELBOW+1)*3],\n",
    "            'l_wrist': keypoints[L_WRIST*3:(L_WRIST+1)*3],\n",
    "            'r_wrist': keypoints[R_WRIST*3:(R_WRIST+1)*3],\n",
    "            'l_hip': keypoints[L_HIP*3:(L_HIP+1)*3],\n",
    "            'r_hip': keypoints[R_HIP*3:(R_HIP+1)*3],\n",
    "            'l_knee': keypoints[L_KNEE*3:(L_KNEE+1)*3],\n",
    "            'r_knee': keypoints[R_KNEE*3:(R_KNEE+1)*3],\n",
    "            'l_ankle': keypoints[L_ANKLE*3:(L_ANKLE+1)*3],\n",
    "            'r_ankle': keypoints[R_ANKLE*3:(R_ANKLE+1)*3]\n",
    "        }\n",
    "        \n",
    "        # Check if keypoints are detected with sufficient confidence\n",
    "        low_confidence_keypoints = []\n",
    "        for k, v in kp.items():\n",
    "            if v[2] < threshold:\n",
    "                low_confidence_keypoints.append(k)\n",
    "        \n",
    "        # If more than 30% of keypoints have low confidence, return early\n",
    "        if len(low_confidence_keypoints) > 0.3 * len(kp):\n",
    "            return False, \"low_confidence\", [f\"Low confidence in keypoints: {', '.join(low_confidence_keypoints)}\"]\n",
    "        \n",
    "        # 1. Calculate the length factor based on torso length to adjust thresholds\n",
    "        # as described in the journal's supplementary materials\n",
    "        shoulder_midpoint = [(kp['l_shoulder'][0] + kp['r_shoulder'][0])/2, \n",
    "                            (kp['l_shoulder'][1] + kp['r_shoulder'][1])/2]\n",
    "        hip_midpoint = [(kp['l_hip'][0] + kp['r_hip'][0])/2, \n",
    "                        (kp['l_hip'][1] + kp['r_hip'][1])/2]\n",
    "        \n",
    "        torso_length = np.sqrt((shoulder_midpoint[0] - hip_midpoint[0])**2 + \n",
    "                              (shoulder_midpoint[1] - hip_midpoint[1])**2)\n",
    "        \n",
    "        # If torso length is near zero (very rare), use a default value\n",
    "        if torso_length < 1e-5:\n",
    "            torso_length = 100  # default average torso length in pixels\n",
    "        \n",
    "        # 2. Calculate body dimensions and ratios\n",
    "        feet_midpoint = [(kp['l_ankle'][0] + kp['r_ankle'][0])/2, \n",
    "                         (kp['l_ankle'][1] + kp['r_ankle'][1])/2]\n",
    "        \n",
    "        # Height of the body (vertical distance from shoulders to ankles)\n",
    "        body_height = abs(shoulder_midpoint[1] - feet_midpoint[1])\n",
    "        \n",
    "        # Width of the body (distance between shoulders)\n",
    "        body_width = abs(kp['l_shoulder'][0] - kp['r_shoulder'][0])\n",
    "        \n",
    "        # The conditions list will store all the criteria that suggest a fall\n",
    "        conditions = []\n",
    "        \n",
    "        # 3. Analyze shoulder position relative to feet\n",
    "        # In a normal standing posture, shoulder_y < feet_y\n",
    "        # In a fall, shoulder_y approaches or exceeds feet_y\n",
    "        shoulder_relative_threshold = feet_midpoint[1] - 0.5 * torso_length\n",
    "        if shoulder_midpoint[1] > shoulder_relative_threshold:\n",
    "            conditions.append(\"shoulder_near_feet\")\n",
    "        \n",
    "        # 4. Analyze body orientation (width vs height ratio)\n",
    "        # In normal standing posture, height > width\n",
    "        # In a fall, especially to the side, width may exceed height\n",
    "        width_height_ratio = body_width / (body_height + 1e-5)  # Adding epsilon to avoid division by zero\n",
    "        if width_height_ratio > 0.8:  # Threshold from the journal\n",
    "            conditions.append(\"horizontal_orientation\")\n",
    "        \n",
    "        # 5. Calculate angle between torso and legs\n",
    "        # As mentioned in the paper - a threshold of 45 degrees is used\n",
    "        try:\n",
    "            # Calculate vectors for torso and legs\n",
    "            torso_vector = [hip_midpoint[0] - shoulder_midpoint[0], \n",
    "                           hip_midpoint[1] - shoulder_midpoint[1]]\n",
    "            \n",
    "            leg_vector = [(kp['l_knee'][0] + kp['r_knee'][0])/2 - hip_midpoint[0],\n",
    "                          (kp['l_knee'][1] + kp['r_knee'][1])/2 - hip_midpoint[1]]\n",
    "            \n",
    "            # Calculate the angle between vectors using dot product\n",
    "            dot_product = torso_vector[0]*leg_vector[0] + torso_vector[1]*leg_vector[1]\n",
    "            torso_magnitude = math.sqrt(torso_vector[0]**2 + torso_vector[1]**2)\n",
    "            leg_magnitude = math.sqrt(leg_vector[0]**2 + leg_vector[1]**2)\n",
    "            \n",
    "            if torso_magnitude > 0 and leg_magnitude > 0:\n",
    "                angle_cos = dot_product / (torso_magnitude * leg_magnitude)\n",
    "                angle_cos = max(-1, min(1, angle_cos))  # Ensure value is in [-1, 1] range\n",
    "                angle_degrees = math.degrees(math.acos(angle_cos))\n",
    "                \n",
    "                # Check if angle is below threshold (45 degrees as mentioned in paper)\n",
    "                if angle_degrees < 45:\n",
    "                    conditions.append(f\"acute_body_angle_{angle_degrees:.1f}\")\n",
    "        except Exception as e:\n",
    "            # Skip angle calculation if there's an error\n",
    "            pass\n",
    "        \n",
    "        # 6. Motion detection (fall speed)\n",
    "        # This is crucial to distinguish between a fall and intentionally lying down\n",
    "        current_time = time.time()\n",
    "        \n",
    "        if prev_keypoints is not None and prev_frame_time is not None:\n",
    "            time_elapsed = current_time - prev_frame_time\n",
    "            \n",
    "            if time_elapsed > 0:\n",
    "                # Calculate vertical movement speed of shoulder\n",
    "                prev_shoulder_y = (prev_keypoints['l_shoulder'][1] + prev_keypoints['r_shoulder'][1]) / 2\n",
    "                shoulder_y = (kp['l_shoulder'][1] + kp['r_shoulder'][1]) / 2\n",
    "                shoulder_speed = (shoulder_y - prev_shoulder_y) / time_elapsed\n",
    "                \n",
    "                # Calculate vertical movement speed of hip\n",
    "                prev_hip_y = (prev_keypoints['l_hip'][1] + prev_keypoints['r_hip'][1]) / 2\n",
    "                hip_y = (kp['l_hip'][1] + kp['r_hip'][1]) / 2\n",
    "                hip_speed = (hip_y - prev_hip_y) / time_elapsed\n",
    "                \n",
    "                # Use the maximum speed between shoulder and hip\n",
    "                # High positive speed indicates downward movement (in image coordinate system)\n",
    "                max_speed = max(shoulder_speed, hip_speed)\n",
    "                \n",
    "                # Threshold adjusted based on the literature (0.6 from your original code)\n",
    "                # scaled by the torso length for body-size independence\n",
    "                speed_threshold = 0.6 * (torso_length / 100)\n",
    "                \n",
    "                if max_speed > speed_threshold:\n",
    "                    conditions.append(f\"high_downward_speed_{max_speed:.1f}\")\n",
    "        \n",
    "        # Store current keypoints and time for next frame comparison\n",
    "        prev_keypoints = kp.copy()\n",
    "        prev_frame_time = current_time\n",
    "        \n",
    "        # 7. Fall decision logic as described in the journal's algorithm\n",
    "        # A fall is detected if:\n",
    "        # - At least 2 conditions are met (as in your original code)\n",
    "        # OR\n",
    "        # - High downward speed AND at least one other condition\n",
    "        is_fall = False\n",
    "        if len(conditions) >= 2:\n",
    "            is_fall = True\n",
    "        elif any(cond.startswith(\"high_downward_speed\") for cond in conditions) and len(conditions) >= 2:\n",
    "            is_fall = True\n",
    "            \n",
    "        # 8. Fall history tracking for more robust detection (reduce false positives)\n",
    "        # Store recent fall detections to make final decision more robust\n",
    "        fall_history.append(is_fall)\n",
    "        # Keep only the last 5 frames\n",
    "        if len(fall_history) > 5:\n",
    "            fall_history.pop(0)\n",
    "            \n",
    "        # Require at least 3 fall detections in last 5 frames for more robust detection\n",
    "        robust_fall = sum(fall_history) >= 3\n",
    "        \n",
    "        # 9. Alert frequency control\n",
    "        # Prevent alert spam by limiting frequency\n",
    "        current_time = time.time()\n",
    "        if robust_fall:\n",
    "            if alert_timestamp is None or (current_time - alert_timestamp > 120):  # 2 minutes between alerts\n",
    "                alert_timestamp = current_time\n",
    "                return True, \"fallen\", conditions\n",
    "            else:\n",
    "                # Fall detected but alert recently sent\n",
    "                return False, \"alert_cooldown\", conditions\n",
    "                \n",
    "        return False, \"normal\", conditions\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Reset tracking variables in case of error\n",
    "        prev_keypoints = None\n",
    "        prev_frame_time = None\n",
    "        fall_history = []\n",
    "        return False, \"error\", [f\"Error: {str(e)}\"]\n",
    "\n",
    "\n",
    "class FallDetectionMetrics:\n",
    "    \"\"\"\n",
    "    Class for tracking fall detection performance metrics\n",
    "    as described in the journal's evaluation methodology.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.true_positives = 0   # Correctly detected falls\n",
    "        self.false_positives = 0  # Incorrectly detected falls\n",
    "        self.true_negatives = 0   # Correctly identified non-falls\n",
    "        self.false_negatives = 0  # Missed falls\n",
    "        self.results = []\n",
    "        \n",
    "    def update(self, is_fall_detected, is_actual_fall):\n",
    "        \"\"\"Update metrics based on detection results\"\"\"\n",
    "        if is_fall_detected and is_actual_fall:\n",
    "            self.true_positives += 1\n",
    "        elif is_fall_detected and not is_actual_fall:\n",
    "            self.false_positives += 1\n",
    "        elif not is_fall_detected and is_actual_fall:\n",
    "            self.false_negatives += 1\n",
    "        else:\n",
    "            self.true_negatives += 1\n",
    "            \n",
    "        self.results.append({\n",
    "            \"detected\": is_fall_detected,\n",
    "            \"actual\": is_actual_fall,\n",
    "            \"timestamp\": time.time()\n",
    "        })\n",
    "    \n",
    "    def calculate_metrics(self):\n",
    "        \"\"\"Calculate performance metrics from confusion matrix\"\"\"\n",
    "        # Handle division by zero\n",
    "        precision = self.true_positives / (self.true_positives + self.false_positives) if (self.true_positives + self.false_positives) > 0 else 0\n",
    "        recall = self.true_positives / (self.true_positives + self.false_negatives) if (self.true_positives + self.false_negatives) > 0 else 0\n",
    "        specificity = self.true_negatives / (self.true_negatives + self.false_positives) if (self.true_negatives + self.false_positives) > 0 else 0\n",
    "        accuracy = (self.true_positives + self.true_negatives) / (self.true_positives + self.true_negatives + self.false_positives + self.false_negatives) if (self.true_positives + self.true_negatives + self.false_positives + self.false_negatives) > 0 else 0\n",
    "        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"specificity\": specificity,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"f1_score\": f1_score,\n",
    "            \"true_positives\": self.true_positives,\n",
    "            \"false_positives\": self.false_positives,\n",
    "            \"true_negatives\": self.true_negatives,\n",
    "            \"false_negatives\": self.false_negatives\n",
    "        }\n",
    "    \n",
    "    def save_results(self, filename_prefix):\n",
    "        \"\"\"Save detection results and metrics to files\"\"\"\n",
    "        import json\n",
    "        import os\n",
    "        \n",
    "        # Create results directory if it doesn't exist\n",
    "        os.makedirs(filename_prefix, exist_ok=True)\n",
    "        \n",
    "        # Save raw results\n",
    "        with open(os.path.join(filename_prefix, \"results.json\"), \"w\") as f:\n",
    "            json.dump(self.results, f, indent=2)\n",
    "        \n",
    "        # Save metrics\n",
    "        with open(os.path.join(filename_prefix, \"metrics.json\"), \"w\") as f:\n",
    "            json.dump(self.calculate_metrics(), f, indent=2)\n",
    "\n",
    "\n",
    "def load_annotations(label_dir):\n",
    "    \"\"\"\n",
    "    Load ground truth annotations from label files\n",
    "    \n",
    "    Args:\n",
    "        label_dir: Directory containing label files\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary of annotations by video file\n",
    "    \"\"\"\n",
    "    import os\n",
    "    \n",
    "    annotations = {}\n",
    "    for file in os.listdir(label_dir):\n",
    "        if file.endswith(\".txt\"):\n",
    "            video_name = file.replace(\".txt\", \".avi\")\n",
    "            with open(os.path.join(label_dir, file), \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "                \n",
    "            # Parse annotations\n",
    "            fall_frames = []\n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 3 and parts[0] == \"fall\":\n",
    "                    start_frame = int(parts[1])\n",
    "                    end_frame = int(parts[2])\n",
    "                    fall_frames.extend(list(range(start_frame, end_frame + 1)))\n",
    "            \n",
    "            annotations[video_name] = set(fall_frames)\n",
    "    \n",
    "    return annotations\n",
    "\n",
    "\n",
    "def process_video(video_path, annotations, metrics, model=None):\n",
    "    \"\"\"\n",
    "    Process a video file for fall detection\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to the video file\n",
    "        annotations: Dictionary of ground truth annotations\n",
    "        metrics: FallDetectionMetrics instance for tracking performance\n",
    "        model: Optional pre-loaded YOLOv7 model\n",
    "    \"\"\"\n",
    "    import cv2\n",
    "    import os\n",
    "    import torch\n",
    "    from torchvision import transforms\n",
    "    from utils.datasets import letterbox\n",
    "    from utils.general import non_max_suppression_kpt\n",
    "    from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "    \n",
    "    # Initialize device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Check if model is provided, otherwise load it\n",
    "    if model is None:\n",
    "        # Ensure models directory exists\n",
    "        os.makedirs('./models', exist_ok=True)\n",
    "        \n",
    "        # Check if model file exists, if not download it\n",
    "        if not os.path.exists('./models/yolov7-w6-pose.pt'):\n",
    "            print(\"Model file not found. Please download yolov7-w6-pose.pt and place it in the models directory.\")\n",
    "            return\n",
    "            \n",
    "        # Load YOLOv7-pose model\n",
    "        weights = torch.load('./models/yolov7-w6-pose.pt', map_location=device, weights_only=False)\n",
    "        model = weights['model'].float().eval().to(device)\n",
    "        if torch.cuda.is_available():\n",
    "            model = model.half()\n",
    "    \n",
    "    # Get video file name\n",
    "    video_file = os.path.basename(video_path)\n",
    "    \n",
    "    # Get ground truth fall frames (if available)\n",
    "    fall_frames = annotations.get(video_file, set())\n",
    "    \n",
    "    # Open video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file: {video_path}\")\n",
    "        return\n",
    "        \n",
    "    frame_count = 0\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            # Process frame for pose estimation\n",
    "            # Resize and preprocess\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image = letterbox(image, 960, stride=64, auto=True)[0]\n",
    "            image = transforms.ToTensor()(image)\n",
    "            image = torch.tensor(np.array([image.numpy()]))\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                image = image.half().to(device)\n",
    "            else:\n",
    "                image = image.to(device)\n",
    "                \n",
    "            # Run model inference\n",
    "            with torch.no_grad():\n",
    "                output, _ = model(image)\n",
    "            \n",
    "            # Process output for keypoints\n",
    "            output = non_max_suppression_kpt(output, 0.25, 0.65, nc=model.yaml['nc'], nkpt=model.yaml['nkpt'])\n",
    "            \n",
    "            # Check if person detected\n",
    "            if len(output) > 0 and len(output[0]) > 0:\n",
    "                # Get keypoints for first person detected\n",
    "                keypoints = output[0][0, 7:].clone().cpu().numpy()\n",
    "                \n",
    "                # Check if keypoints array is valid\n",
    "                if len(keypoints) > 0:\n",
    "                    # Check for fall\n",
    "                    is_fall_detected, status, conditions = detect_fall(keypoints)\n",
    "                    \n",
    "                    # Ground truth - is this frame a fall according to annotations?\n",
    "                    is_actual_fall = frame_count in fall_frames\n",
    "                    \n",
    "                    # Update metrics\n",
    "                    metrics.update(is_fall_detected, is_actual_fall)\n",
    "                    \n",
    "                    # Display results\n",
    "                    if is_fall_detected:\n",
    "                        cv2.putText(frame, f\"FALL DETECTED! {status}\", (10, 30), \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                    \n",
    "                    # Draw skeleton - Add error handling\n",
    "                    try:\n",
    "                        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                        with_skeleton = plot_skeleton_kpts(frame, keypoints, 3)\n",
    "                        frame = cv2.cvtColor(with_skeleton, cv2.COLOR_RGB2BGR)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error plotting skeleton: {str(e)}\")\n",
    "                        # Continue without drawing skeleton\n",
    "            \n",
    "            # Display frame\n",
    "            cv2.imshow('Fall Detection', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "                \n",
    "            frame_count += 1\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing frame {frame_count}: {str(e)}\")\n",
    "            frame_count += 1\n",
    "            continue\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def run_fall_detection_system(camera_index=0):\n",
    "    \"\"\"\n",
    "    Run real-time fall detection system using webcam\n",
    "    \n",
    "    Args:\n",
    "        camera_index: Index of camera to use\n",
    "    \"\"\"\n",
    "    import cv2\n",
    "    import torch\n",
    "    import os\n",
    "    from torchvision import transforms\n",
    "    from utils.datasets import letterbox\n",
    "    from utils.general import non_max_suppression_kpt\n",
    "    from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "    \n",
    "    # Initialize device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Ensure models directory exists\n",
    "    os.makedirs('./models', exist_ok=True)\n",
    "    \n",
    "    # Check if model file exists, if not inform user\n",
    "    if not os.path.exists('./models/yolov7-w6-pose.pt'):\n",
    "        print(\"Model file not found. Please download yolov7-w6-pose.pt and place it in the models directory.\")\n",
    "        return\n",
    "    \n",
    "    # Load YOLOv7-pose model\n",
    "    weights = torch.load('./models/yolov7-w6-pose.pt', map_location=device, weights_only=False)\n",
    "    model = weights['model'].float().eval().to(device)\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.half()\n",
    "    \n",
    "    # Open webcam\n",
    "    cap = cv2.VideoCapture(camera_index)\n",
    "    \n",
    "    print(\"Fall detection system running. Press 'q' to quit.\")\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Process frame for pose estimation\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image = letterbox(image, 960, stride=64, auto=True)[0]\n",
    "        image = transforms.ToTensor()(image)\n",
    "        image = torch.tensor(np.array([image.numpy()]))\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            image = image.half().to(device)\n",
    "        else:\n",
    "            image = image.to(device)\n",
    "            \n",
    "        # Run model inference\n",
    "        with torch.no_grad():\n",
    "            output, _ = model(image)\n",
    "        \n",
    "        # Process output for keypoints\n",
    "        output = non_max_suppression_kpt(output, 0.25, 0.65, nc=model.yaml['nc'], nkpt=model.yaml['nkpt'])\n",
    "        \n",
    "        # Check if person detected\n",
    "        if len(output) > 0 and len(output[0]) > 0:\n",
    "            # Get keypoints for first person detected\n",
    "            keypoints = output[0][0, 7:].clone().cpu().numpy()\n",
    "            \n",
    "            # Check for fall\n",
    "            is_fall_detected, status, conditions = detect_fall(keypoints)\n",
    "            \n",
    "            # Display results\n",
    "            if is_fall_detected:\n",
    "                cv2.putText(frame, f\"FALL DETECTED! {status}\", (10, 30), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "                print(f\"Fall detected! Conditions: {conditions}\")\n",
    "            else:\n",
    "                cv2.putText(frame, f\"Status: {status}\", (10, 30), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            \n",
    "            # Draw skeleton\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            with_skeleton = plot_skeleton_kpts(frame, keypoints, 3)\n",
    "            frame = cv2.cvtColor(with_skeleton, cv2.COLOR_RGB2BGR)\n",
    "        else:\n",
    "            cv2.putText(frame, \"No person detected\", (10, 30), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "        \n",
    "        # Display frame\n",
    "        cv2.imshow('Fall Detection System', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "def evaluate_on_dataset(base_path, phase='test'):\n",
    "    \"\"\"\n",
    "    Evaluate fall detection system on a dataset\n",
    "    \n",
    "    Args:\n",
    "        base_path: Base path to dataset\n",
    "        phase: Dataset phase to evaluate (train, valid, test)\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import torch\n",
    "    \n",
    "    # Fix path issues - use proper path joining\n",
    "    video_path = os.path.join(base_path, phase, 'video')\n",
    "    label_path = os.path.join(base_path, phase, 'labels')\n",
    "    \n",
    "    # Print the actual paths to help debug\n",
    "    print(f\"Looking for videos in: {os.path.abspath(video_path)}\")\n",
    "    print(f\"Looking for labels in: {os.path.abspath(label_path)}\")\n",
    "    \n",
    "    # Check if directory exists\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"Error: Video directory not found at {video_path}\")\n",
    "        # List parent directory contents to help debugging\n",
    "        parent_dir = os.path.dirname(video_path)\n",
    "        if os.path.exists(parent_dir):\n",
    "            print(f\"Contents of {parent_dir}:\")\n",
    "            for item in os.listdir(parent_dir):\n",
    "                print(f\"  - {item}\")\n",
    "        return\n",
    "    \n",
    "    if not os.path.exists(label_path):\n",
    "        print(f\"Error: Labels directory not found at {label_path}\")\n",
    "        return\n",
    "    \n",
    "    # Initialize metrics\n",
    "    metrics = FallDetectionMetrics()\n",
    "    \n",
    "    # Load model once for all videos\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Ensure models directory exists\n",
    "    os.makedirs('./models', exist_ok=True)\n",
    "    \n",
    "    # Check if model file exists\n",
    "    if not os.path.exists('./models/yolov7-w6-pose.pt'):\n",
    "        print(\"Model file not found. Please download yolov7-w6-pose.pt and place it in the models directory.\")\n",
    "        return\n",
    "    \n",
    "    # Load YOLOv7-pose model\n",
    "    print(f\"Loading YOLOv7-W6-Pose model...\")\n",
    "    weights = torch.load('./models/yolov7-w6-pose.pt', map_location=device, weights_only=False)\n",
    "    model = weights['model'].float().eval().to(device)\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.half()\n",
    "    print(f\"Model loaded successfully!\")\n",
    "    \n",
    "    # Count video files\n",
    "    video_files = [f for f in os.listdir(video_path) if f.endswith(('.avi', '.mp4'))]\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Processing {phase} set (Videos: {len(video_files)})\")\n",
    "    \n",
    "    # Load annotations\n",
    "    annotations = load_annotations(label_path)\n",
    "    \n",
    "    # Process videos\n",
    "    for video_file in video_files:\n",
    "        video_file_path = os.path.join(video_path, video_file)\n",
    "        print(f\"Processing {video_file}...\")\n",
    "        process_video(video_file_path, annotations, metrics, model)\n",
    "    \n",
    "    # Save results\n",
    "    results_dir = os.path.join('./results', phase)\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    metrics.save_results(results_dir)\n",
    "    print(f\"{phase} metrics saved to {results_dir}\")\n",
    "    \n",
    "    # Print results\n",
    "    res = metrics.calculate_metrics()\n",
    "    print(f\"\\n{phase.upper():<6} Results:\")\n",
    "    print(f\"Precision: {res['precision']:.3f}\")\n",
    "    print(f\"Recall: {res['recall']:.3f}\")\n",
    "    print(f\"F1 Score: {res['f1_score']:.3f}\")\n",
    "    print(f\"Accuracy: {res['accuracy']:.3f}\")\n",
    "    print(f\"Specificity: {res['specificity']:.3f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Download the YOLOv7-W6-Pose model\n",
    "def download_model():\n",
    "    \"\"\"\n",
    "    Download the YOLOv7-W6-Pose model if it doesn't exist\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import subprocess\n",
    "    \n",
    "    # Create models directory if it doesn't exist\n",
    "    os.makedirs('./models', exist_ok=True)\n",
    "    \n",
    "    # Check if model already exists\n",
    "    if not os.path.exists('./models/yolov7-w6-pose.pt'):\n",
    "        print(\"Downloading YOLOv7-W6-Pose model...\")\n",
    "        try:\n",
    "            # Try wget first\n",
    "            subprocess.run([\"wget\", \"-O\", \"./models/yolov7-w6-pose.pt\", \"https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-w6-pose.pt\"], check=True)\n",
    "        except:\n",
    "            # If wget fails, try curl\n",
    "            try:\n",
    "                subprocess.run([\"curl\", \"-L\", \"https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-w6-pose.pt\", \"-o\", \"./models/yolov7-w6-pose.pt\"], check=True)\n",
    "            except:\n",
    "                print(\"Failed to download model. Please download manually from:\")\n",
    "                print(\"https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-w6-pose.pt\")\n",
    "                print(\"and place it in the ./models/ directory.\")\n",
    "                return False\n",
    "        \n",
    "        print(\"Download complete!\")\n",
    "    else:\n",
    "        print(\"Model already exists!\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def evaluate_all_phases(base_path):\n",
    "    \"\"\"\n",
    "    Evaluate fall detection system on all dataset phases\n",
    "    \n",
    "    Args:\n",
    "        base_path: Base path to dataset\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import torch\n",
    "    \n",
    "    # Initialize metrics for each phase\n",
    "    metrics = {\n",
    "        'train': FallDetectionMetrics(),\n",
    "        'valid': FallDetectionMetrics(),\n",
    "        'test': FallDetectionMetrics()\n",
    "    }\n",
    "    \n",
    "    # Load model once for all videos\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Ensure models directory exists\n",
    "    os.makedirs('./models', exist_ok=True)\n",
    "    \n",
    "    # Check if model file exists\n",
    "    if not os.path.exists('./models/yolov7-w6.pt'):\n",
    "        print(\"Model file not found. Please download yolov7-w6-pose.pt and place it in the models directory.\")\n",
    "        return\n",
    "    \n",
    "    # Load YOLOv7-pose model\n",
    "    print(f\"Loading YOLOv7-W6-Pose model...\")\n",
    "    weights = torch.load('./models/yolov7-w6-pose.pt', map_location=device, weights_only=False)\n",
    "    model = weights['model'].float().eval().to(device)\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.half()\n",
    "    print(f\"Model loaded successfully!\")\n",
    "    \n",
    "    # Process all phases\n",
    "    for phase in ['train', 'valid', 'test']:\n",
    "        # Fix path issues - make sure path is normalized\n",
    "        video_path = os.path.normpath(os.path.join(base_path, phase, 'video'))\n",
    "        label_path = os.path.normpath(os.path.join(base_path, phase, 'labels'))\n",
    "        \n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"Processing {phase} set\")\n",
    "        print(f\"Looking for videos in: {video_path}\")\n",
    "        \n",
    "        # Check if directory exists\n",
    "        if not os.path.exists(video_path):\n",
    "            print(f\"Error: Video directory not found at {video_path}\")\n",
    "            # List parent directory to help debugging\n",
    "            parent_dir = os.path.dirname(video_path)\n",
    "            if os.path.exists(parent_dir):\n",
    "                print(f\"Contents of {parent_dir}:\")\n",
    "                for item in os.listdir(parent_dir):\n",
    "                    print(f\"  - {item}\")\n",
    "            continue\n",
    "        \n",
    "        if not os.path.exists(label_path):\n",
    "            print(f\"Error: Labels directory not found at {label_path}\")\n",
    "            continue\n",
    "        \n",
    "        # Load annotations\n",
    "        annotations = load_annotations(label_path)\n",
    "        \n",
    "        # Process videos\n",
    "        video_files = [f for f in os.listdir(video_path) if f.endswith(('.avi', '.mp4'))]\n",
    "        print(f\"Found {len(video_files)} video files\")\n",
    "        \n",
    "        for video_file in video_files:\n",
    "            video_file_path = os.path.join(video_path, video_file)\n",
    "            print(f\"Processing {video_file}...\")\n",
    "            try:\n",
    "                process_video(video_file_path, annotations, metrics[phase], model)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {video_file}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        # Save results\n",
    "        results_dir = os.path.normpath(os.path.join('./results', phase))\n",
    "        os.makedirs(results_dir, exist_ok=True)\n",
    "        metrics[phase].save_results(results_dir)\n",
    "        print(f\"{phase} metrics saved to {results_dir}\")\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\n=== FINAL RESULTS ===\")\n",
    "    for phase in ['train', 'valid', 'test']:\n",
    "        res = metrics[phase].calculate_metrics()\n",
    "        print(f\"\\n{phase.upper():<6} Precision: {res['precision']:.3f} | Recall: {res['recall']:.3f} | F1: {res['f1_score']:.3f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# For running in Jupyter notebook\n",
    "# 1. Download the model first\n",
    "download_success = download_model()\n",
    "\n",
    "# 2. You can now choose what you want to do:\n",
    "# Uncomment one of these lines to run:\n",
    "\n",
    "# For webcam detection:\n",
    "# run_fall_detection_system(camera_index=0)\n",
    "\n",
    "# For dataset evaluation:\n",
    "evaluate_on_dataset('./datasets/FallDataset', phase='test')\n",
    "\n",
    "# Evaluate all phases at once\n",
    "# evaluate_all_phases('../datasets/FallDataset')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
