{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29f47659-f100-48c6-aa69-19f581d46b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. YOLOv7 modules imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup paths and import dependencies\n",
    "import sys\n",
    "import os\n",
    "from collections import deque, defaultdict\n",
    "\n",
    "# Add parent directory to path to find YOLOv7 modules\n",
    "sys.path.append('..')\n",
    "sys.path.append(os.path.abspath('..'))  # Absolute path to be extra safe\n",
    "\n",
    "# Import dependencies\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import time\n",
    "import math\n",
    "from torchvision import transforms\n",
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "from models.yolo import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "# Add the custom class to the safe globals list\n",
    "torch.serialization.add_safe_globals([Model])\n",
    "\n",
    "print(\"Setup complete. YOLOv7 modules imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58c11703-dfde-4fec-8fe6-fb7d55e2e5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Loading model from ../yolov7-w6-pose.pt\n",
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Set up the device and load model\n",
    "# Initialize device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define paths for YOLOv7-W6-Pose weights\n",
    "# Try multiple potential locations\n",
    "potential_paths = [\n",
    "    '../yolov7-w6-pose.pt',  # Root directory\n",
    "    'yolov7-w6-pose.pt',     # Current directory\n",
    "    '../models/yolov7-w6-pose.pt' # Models directory\n",
    "]\n",
    "\n",
    "# Find the first path that exists\n",
    "model_path = None\n",
    "for path in potential_paths:\n",
    "    if os.path.exists(path):\n",
    "        model_path = path\n",
    "        break\n",
    "\n",
    "# If model not found, attempt to download\n",
    "if model_path is None:\n",
    "    download_path = '../yolov7-w6-pose.pt'  # Download to root directory\n",
    "    print(f\"YOLOv7-W6-Pose weights not found. Downloading to {download_path}...\")\n",
    "    \n",
    "    # GitHub release URL for YOLOv7-W6-Pose weights\n",
    "    url = 'https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-w6-pose.pt'\n",
    "    \n",
    "    try:\n",
    "        import requests\n",
    "        print(f\"Downloading {url}...\")\n",
    "        response = requests.get(url)\n",
    "        with open(download_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded to {download_path}\")\n",
    "        model_path = download_path\n",
    "    except Exception as e:\n",
    "        print(f\"Download failed: {e}\")\n",
    "        print(\"Please download the weights manually from:\")\n",
    "        print(\"https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-w6-pose.pt\")\n",
    "        raise FileNotFoundError(\"Model weights not found and download failed\")\n",
    "\n",
    "print(f\"Loading model from {model_path}\")\n",
    "\n",
    "# Load YOLOv7-pose model\n",
    "weights = torch.load(model_path, map_location=device, weights_only=False)\n",
    "model = weights['model']\n",
    "_ = model.float().eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.half().to(device)\n",
    "    \n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be7fbdac-fa7b-420b-953b-ea47d767e578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FallDetector class defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Define the FallDetector class\n",
    "class FallDetector:\n",
    "    def __init__(self):\n",
    "        # Threshold parameters based on the paper\n",
    "        self.LENGTH_FACTOR_ALPHA = 0.5  # ALPHA value from paper\n",
    "        self.VELOCITY_THRESHOLD = 0.5   # SPEED_THRESHOLD from paper\n",
    "        self.ANGLE_THRESHOLD = 45       # ANGLE_THRESHOLD from paper\n",
    "        self.TORSO_ANGLE_THRESHOLD = 50  # For torso orientation\n",
    "        self.CONFIDENCE_THRESHOLD = 0.5  # Confidence threshold for keypoints\n",
    "        self.TARGET_FPS = 25            # Target FPS for processing\n",
    "        \n",
    "        # State tracking with buffers for smoothing (as in the shared code)\n",
    "        self.prev_keypoints = None\n",
    "        self.velocity_buffer = deque(maxlen=3)  # Buffer for velocity smoothing\n",
    "        self.fall_buffer = deque(maxlen=2)      # Buffer for fall detection smoothing\n",
    "        self.prev_frame_time = None\n",
    "        self.fall_start_time = None\n",
    "        self.prev_shoulder_y = None\n",
    "        self.current_state = \"normal\"  # State tracking: \"normal\", \"falling\", \"fallen\"\n",
    "\n",
    "    def calculate_angle(self, a, b, c):\n",
    "        \"\"\"Calculate angle between three points (used for joint angles).\"\"\"\n",
    "        try:\n",
    "            ba = np.array([a[0]-b[0], a[1]-b[1]])\n",
    "            bc = np.array([c[0]-b[0], c[1]-b[1]])\n",
    "            cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc) + 1e-6)\n",
    "            return np.degrees(np.arccos(np.clip(cosine_angle, -1.0, 1.0)))\n",
    "        except:\n",
    "            return 180\n",
    "\n",
    "    def calculate_torso_angle(self, shoulders, hips):\n",
    "        \"\"\"Calculate angle of torso relative to vertical.\"\"\"\n",
    "        shoulder_center = np.mean(shoulders, axis=0)\n",
    "        hip_center = np.mean(hips, axis=0)\n",
    "        vertical_vector = np.array([0, 1])\n",
    "        torso_vector = np.array([hip_center[0]-shoulder_center[0], hip_center[1]-shoulder_center[1]])\n",
    "        \n",
    "        if np.linalg.norm(torso_vector) < 1e-6:\n",
    "            return 90\n",
    "            \n",
    "        cosine = np.dot(torso_vector, vertical_vector) / (np.linalg.norm(torso_vector) + 1e-6)\n",
    "        return np.degrees(np.arccos(np.clip(cosine, -1.0, 1.0)))\n",
    "\n",
    "    def detect_fall(self, keypoints):\n",
    "        \"\"\"\n",
    "        Detect falls using keypoints from YOLOv7-W6-Pose.\n",
    "        Implements the approach described in the paper with smoothing buffers.\n",
    "        \n",
    "        Args:\n",
    "            keypoints: Keypoints array from YOLOv7-W6-Pose model\n",
    "            \n",
    "        Returns:\n",
    "            is_fall: Boolean indicating if a fall is detected\n",
    "            current_state: Current state of the fall detection state machine\n",
    "            conditions: List of conditions that contributed to the fall detection\n",
    "        \"\"\"\n",
    "        # Keypoint indices based on YOLOv7-W6-Pose output\n",
    "        NOSE = 0\n",
    "        LEFT_SHOULDER = 5\n",
    "        RIGHT_SHOULDER = 6\n",
    "        LEFT_HIP = 11\n",
    "        RIGHT_HIP = 12\n",
    "        LEFT_KNEE = 13\n",
    "        RIGHT_KNEE = 14\n",
    "        LEFT_ANKLE = 15\n",
    "        RIGHT_ANKLE = 16\n",
    "        \n",
    "        try:\n",
    "            # Extract and validate keypoints\n",
    "            kp = {\n",
    "                'nose': keypoints[NOSE*3:(NOSE+1)*3],\n",
    "                'left_shoulder': keypoints[LEFT_SHOULDER*3:(LEFT_SHOULDER+1)*3],\n",
    "                'right_shoulder': keypoints[RIGHT_SHOULDER*3:(RIGHT_SHOULDER+1)*3],\n",
    "                'left_hip': keypoints[LEFT_HIP*3:(LEFT_HIP+1)*3],\n",
    "                'right_hip': keypoints[RIGHT_HIP*3:(RIGHT_HIP+1)*3],\n",
    "                'left_knee': keypoints[LEFT_KNEE*3:(LEFT_KNEE+1)*3],\n",
    "                'right_knee': keypoints[RIGHT_KNEE*3:(RIGHT_KNEE+1)*3],\n",
    "                'left_ankle': keypoints[LEFT_ANKLE*3:(LEFT_ANKLE+1)*3],\n",
    "                'right_ankle': keypoints[RIGHT_ANKLE*3:(RIGHT_ANKLE+1)*3]\n",
    "            }\n",
    "            \n",
    "            # Check confidence of keypoints\n",
    "            if any(point[2] < self.CONFIDENCE_THRESHOLD for point in kp.values()):\n",
    "                return False, \"low_confidence\", [\"Low confidence in keypoints\"]\n",
    "\n",
    "            # Get coordinates (x,y) for each keypoint\n",
    "            nose = (kp['nose'][0], kp['nose'][1])\n",
    "            ls = (kp['left_shoulder'][0], kp['left_shoulder'][1])\n",
    "            rs = (kp['right_shoulder'][0], kp['right_shoulder'][1])\n",
    "            lh = (kp['left_hip'][0], kp['left_hip'][1])\n",
    "            rh = (kp['right_hip'][0], kp['right_hip'][1])\n",
    "            lk = (kp['left_knee'][0], kp['left_knee'][1])\n",
    "            rk = (kp['right_knee'][0], kp['right_knee'][1])\n",
    "            la = (kp['left_ankle'][0], kp['left_ankle'][1])\n",
    "            ra = (kp['right_ankle'][0], kp['right_ankle'][1])\n",
    "\n",
    "            # 1. Height condition: shoulders near feet (as described in paper)\n",
    "            torso_mid = ((lh[0] + rh[0])/2, (lh[1] + rh[1])/2)\n",
    "            Lfactor = math.hypot(ls[0] - torso_mid[0], ls[1] - torso_mid[1])\n",
    "            max_feet_y = max(la[1], ra[1])\n",
    "            min_shoulder_y = min(ls[1], rs[1])\n",
    "            height_cond = min_shoulder_y >= (max_feet_y - self.LENGTH_FACTOR_ALPHA * Lfactor)\n",
    "            \n",
    "            # 2. Speed calculation for rapid downward movement\n",
    "            current_time = time.time()\n",
    "            vertical_speed = 0\n",
    "            current_min_y = min(ls[1], rs[1])\n",
    "            \n",
    "            if self.prev_shoulder_y is not None and self.prev_frame_time is not None:\n",
    "                time_elapsed = current_time - self.prev_frame_time\n",
    "                if time_elapsed > 0:\n",
    "                    vertical_speed = (current_min_y - self.prev_shoulder_y) / time_elapsed\n",
    "                    self.velocity_buffer.append(abs(vertical_speed))\n",
    "            \n",
    "            # Use smoothed velocity for more stable detection\n",
    "            avg_speed = sum(self.velocity_buffer)/len(self.velocity_buffer) if self.velocity_buffer else 0\n",
    "            speed_cond = avg_speed >= self.VELOCITY_THRESHOLD\n",
    "            \n",
    "            # 3. Angle conditions for leg collapse\n",
    "            left_leg_angle = self.calculate_angle(lh, lk, la)\n",
    "            right_leg_angle = self.calculate_angle(rh, rk, ra)\n",
    "            leg_angle_cond = min(left_leg_angle, right_leg_angle) < self.ANGLE_THRESHOLD\n",
    "            \n",
    "            # 4. Torso orientation for horizontal posture\n",
    "            torso_angle = self.calculate_torso_angle([ls, rs], [lh, rh])\n",
    "            torso_cond = torso_angle > self.TORSO_ANGLE_THRESHOLD\n",
    "            \n",
    "            # 5. Body orientation ratio (width vs height)\n",
    "            head_to_feet = abs(nose[1] - max_feet_y)\n",
    "            body_width = abs(ls[0] - rs[0])\n",
    "            orientation_ratio = body_width / (head_to_feet + 1e-6)\n",
    "            aspect_cond = orientation_ratio > 0.8  # Similar to ORIENTATION_RATIO_THRESHOLD in paper\n",
    "            \n",
    "            # Count how many conditions are met\n",
    "            conditions_met = sum([height_cond, speed_cond, leg_angle_cond, torso_cond, aspect_cond])\n",
    "            \n",
    "            # State machine logic as described in the paper\n",
    "            if self.current_state == \"normal\":\n",
    "                # Paper: \"If 'shoulders_near_feet' in conditions and 'rapid_downward' movement\"\n",
    "                if height_cond and speed_cond:\n",
    "                    self.current_state = \"falling\"\n",
    "                    self.fall_start_time = current_time\n",
    "            \n",
    "            elif self.current_state == \"falling\":\n",
    "                # Paper: \"if 'horizontal_posture' in conditions and time < 1.0 seconds\"\n",
    "                if torso_cond and self.fall_start_time and (current_time - self.fall_start_time < 1.0):\n",
    "                    self.current_state = \"fallen\"\n",
    "                # Reset if not completed within timeframe\n",
    "                elif current_time - self.fall_start_time > 1.0:\n",
    "                    self.current_state = \"normal\"\n",
    "            \n",
    "            elif self.current_state == \"fallen\":\n",
    "                # Stay in fallen state for some time before checking if person recovered\n",
    "                if current_time - self.fall_start_time > 5.0:\n",
    "                    if not height_cond and not torso_cond:\n",
    "                        self.current_state = \"normal\"\n",
    "            \n",
    "            # Prepare conditions info for display\n",
    "            conditions_info = []\n",
    "            if height_cond:\n",
    "                conditions_info.append(\"shoulders_near_feet\")\n",
    "            if speed_cond:\n",
    "                conditions_info.append(f\"rapid_downward_{avg_speed:.1f}px/s\")\n",
    "            if torso_cond:\n",
    "                conditions_info.append(\"horizontal_posture\")\n",
    "            if leg_angle_cond:\n",
    "                conditions_info.append(f\"legs_collapsed_{min(left_leg_angle, right_leg_angle):.0f}°\")\n",
    "            if aspect_cond:\n",
    "                conditions_info.append(f\"orientation_ratio:{orientation_ratio:.2f}\")\n",
    "            \n",
    "            # Fall detection logic - using both state machine and conditions count\n",
    "            # Paper states \"If 'shoulders_near_feet' in conditions and time < 1.0 seconds\"\n",
    "            is_fall = (self.current_state == \"fallen\") or (conditions_met >= 2 and height_cond)\n",
    "            \n",
    "            # Use buffer for temporal consistency (from shared code)\n",
    "            self.fall_buffer.append(is_fall)\n",
    "            final_detection = sum(self.fall_buffer) >= 1 if len(self.fall_buffer) >= 1 else is_fall\n",
    "            \n",
    "            if final_detection and self.current_state != \"fallen\":\n",
    "                self.current_state = \"fallen\"\n",
    "            \n",
    "            # Update tracking variables\n",
    "            self.prev_keypoints = {'left_shoulder': kp['left_shoulder'], 'right_shoulder': kp['right_shoulder']}\n",
    "            self.prev_shoulder_y = current_min_y\n",
    "            self.prev_frame_time = current_time\n",
    "            \n",
    "            return final_detection, self.current_state, conditions_info\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in fall detection: {str(e)}\")\n",
    "            return False, \"error\", [f\"Error: {str(e)}\"]\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the detector state for a new video.\"\"\"\n",
    "        self.prev_keypoints = None\n",
    "        self.velocity_buffer.clear()\n",
    "        self.fall_buffer.clear()\n",
    "        self.prev_frame_time = None\n",
    "        self.fall_start_time = None\n",
    "        self.prev_shoulder_y = None\n",
    "        self.current_state = \"normal\"\n",
    "\n",
    "print(\"FallDetector class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c581e24-bf0c-491a-9a2a-0420e2694a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Define utilities for annotation parsing\n",
    "def parse_annotation(annotation_path):\n",
    "    \"\"\"Parse ground truth annotation file containing start and end frame numbers of falls.\"\"\"\n",
    "    try:\n",
    "        with open(annotation_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            if len(lines) >= 2:\n",
    "                return (int(lines[0].strip()), int(lines[1].strip()))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading annotation file: {str(e)}\")\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def send_telegram_alert(bot_token, chat_id, message):\n",
    "    \"\"\"Send alert via Telegram when a fall is detected.\"\"\"\n",
    "    current_time = time.time()\n",
    "    \n",
    "    # Throttle alerts to avoid spam (max one alert per 2 minutes)\n",
    "    if hasattr(send_telegram_alert, 'last_alert_time') and current_time - send_telegram_alert.last_alert_time < 120:\n",
    "        return False\n",
    "    \n",
    "    url = f\"https://api.telegram.org/bot{bot_token}/sendMessage\"\n",
    "    payload = {\n",
    "        \"chat_id\": chat_id,\n",
    "        \"text\": message,\n",
    "        \"parse_mode\": \"HTML\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, data=payload)\n",
    "        if response.status_code == 200:\n",
    "            send_telegram_alert.last_alert_time = current_time\n",
    "            print(f\"Alert sent: {message}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Failed to send alert: {response.status_code} {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error sending Telegram alert: {str(e)}\")\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Initialize the last alert time\n",
    "send_telegram_alert.last_alert_time = 0\n",
    "\n",
    "print(\"Utility functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed0d0b03-640b-493c-8843-4b5e28bef61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Define video processing function\n",
    "def process_video(video_path, fall_detector, annotation_path=None, bot_token=None, chat_id=None, display=True):\n",
    "    \"\"\"\n",
    "    Process video for fall detection using the FallDetector class.\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to video file\n",
    "        fall_detector: Instance of FallDetector class\n",
    "        annotation_path: Path to ground truth annotation file (optional)\n",
    "        bot_token: Telegram bot token (optional)\n",
    "        chat_id: Telegram chat ID (optional)\n",
    "        display: Whether to display frames (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        detected_frames: List of frames where falls were detected\n",
    "    \"\"\"\n",
    "    # Reset the detector for new video\n",
    "    fall_detector.reset()\n",
    "    \n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video {video_path}\")\n",
    "        return []\n",
    "\n",
    "    # Get video properties\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    # Determine frame skip rate to achieve target FPS\n",
    "    skip_frames = max(1, int(round(original_fps / fall_detector.TARGET_FPS)))\n",
    "    \n",
    "    # Initialize result variables\n",
    "    detected_frames = []\n",
    "    frame_counter = 0\n",
    "    \n",
    "    # Get ground truth annotation if available\n",
    "    annotation_range = parse_annotation(annotation_path) if annotation_path else None\n",
    "\n",
    "    print(f\"Processing video: {video_path}\")\n",
    "    print(f\"Original FPS: {original_fps}, Target FPS: {fall_detector.TARGET_FPS}, Skipping every {skip_frames} frames\")\n",
    "    \n",
    "    # Process video frames\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_counter += 1\n",
    "        \n",
    "        # Skip frames to achieve target FPS\n",
    "        if frame_counter % skip_frames != 0:\n",
    "            continue\n",
    "\n",
    "        # Preprocess frame for YOLO model\n",
    "        img = letterbox(frame, 640, stride=64, auto=True)[0]\n",
    "        img_tensor = transforms.ToTensor()(img)\n",
    "        img_tensor = torch.tensor(np.array([img_tensor.numpy()]))\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            img_tensor = img_tensor.half().to(device)\n",
    "\n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            output, _ = model(img_tensor)\n",
    "            output = non_max_suppression_kpt(output, 0.25, 0.65, \n",
    "                                          nc=model.yaml['nc'], \n",
    "                                          nkpt=model.yaml['nkpt'], \n",
    "                                          kpt_label=True)\n",
    "            output = output_to_keypoint(output)\n",
    "\n",
    "        # Visualization and fall detection\n",
    "        display_frame = img.copy()\n",
    "        display_frame = cv2.cvtColor(display_frame, cv2.COLOR_RGB2BGR)\n",
    "        fall_detected = False\n",
    "        current_state = \"normal\"\n",
    "        conditions = []\n",
    "\n",
    "        # Process detected persons\n",
    "        if len(output) > 0:\n",
    "            for idx in range(output.shape[0]):\n",
    "                # Get keypoints for the person\n",
    "                keypoints = output[idx, 7:].T\n",
    "                \n",
    "                # Draw skeleton on the frame\n",
    "                plot_skeleton_kpts(display_frame, keypoints, 3)\n",
    "                \n",
    "                # Detect fall using the FallDetector\n",
    "                is_fall, state, conds = fall_detector.detect_fall(keypoints)\n",
    "                current_state = state\n",
    "                conditions = conds\n",
    "                \n",
    "                if is_fall:\n",
    "                    fall_detected = True\n",
    "                    if frame_counter not in detected_frames:\n",
    "                        detected_frames.append(frame_counter)\n",
    "                        \n",
    "                        # Send alert if Telegram credentials are provided\n",
    "                        if bot_token and chat_id:\n",
    "                            alert_msg = f\"⚠️ <b>ALERT:</b> Fall detected!\\nTime: {time.strftime('%H:%M:%S')}\\nState: {current_state}\\nConditions: {', '.join(conditions)}\"\n",
    "                            send_telegram_alert(bot_token, chat_id, alert_msg)\n",
    "\n",
    "        # Draw detection status on frame\n",
    "        color = (0, 0, 255) if fall_detected else (0, 255, 0)\n",
    "        cv2.putText(display_frame, f\"State: {current_state}\", (20, 30), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "        \n",
    "        # Draw conditions\n",
    "        y_offset = 60\n",
    "        for cond in conditions:\n",
    "            cv2.putText(display_frame, cond, (20, y_offset), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "            y_offset += 25\n",
    "\n",
    "        # Draw ground truth if available\n",
    "        if annotation_range:\n",
    "            start, end = annotation_range\n",
    "            gt_text = f\"GT: {start}-{end} {'(FALL)' if start <= frame_counter <= end else ''}\"\n",
    "            cv2.putText(display_frame, gt_text, (20, y_offset+25), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "\n",
    "        if display:\n",
    "            # For Jupyter: display image directly\n",
    "            rgb_frame = cv2.cvtColor(display_frame, cv2.COLOR_BGR2RGB)\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.imshow(rgb_frame)\n",
    "            plt.title(f\"Frame {frame_counter} - State: {current_state}\")\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            clear_output(wait=True)\n",
    "            display(plt.gcf())\n",
    "            plt.close()\n",
    "            \n",
    "            # Add small delay to simulate video\n",
    "            time.sleep(0.05)\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    print(f\"Video processing complete. Detected falls in {len(detected_frames)} frames.\")\n",
    "    return detected_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f115b842-16a1-495c-ad7c-e4cdd723a70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Define evaluation functions\n",
    "def evaluate_videos(video_dir, label_dir, bot_token=None, chat_id=None):\n",
    "    \"\"\"\n",
    "    Evaluate fall detection on a directory of videos with ground truth labels.\n",
    "    \n",
    "    Args:\n",
    "        video_dir: Directory containing video files\n",
    "        label_dir: Directory containing ground truth label files\n",
    "        bot_token: Telegram bot token (optional)\n",
    "        chat_id: Telegram chat ID (optional)\n",
    "    \n",
    "    Returns:\n",
    "        metrics: Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    metrics = defaultdict(int)\n",
    "    results = []\n",
    "    \n",
    "    # Create fall detector instance\n",
    "    fall_detector = FallDetector()\n",
    "\n",
    "    # Process each video file\n",
    "    for video_file in os.listdir(video_dir):\n",
    "        if not video_file.endswith(('.avi', '.mp4', '.mov')):\n",
    "            continue\n",
    "\n",
    "        video_path = os.path.join(video_dir, video_file)\n",
    "        video_name = os.path.splitext(video_file)[0]\n",
    "        label_path = os.path.join(label_dir, f\"{video_name}.txt\")\n",
    "        \n",
    "        print(f\"\\nProcessing video: {video_file}\")\n",
    "        \n",
    "        # Check if annotation exists\n",
    "        if not os.path.exists(label_path):\n",
    "            print(f\"Warning: No annotation found for {video_file}\")\n",
    "            continue\n",
    "\n",
    "        # Process video with visualization\n",
    "        detected_frames = process_video(video_path, fall_detector, label_path, bot_token, chat_id, display=False)\n",
    "        \n",
    "        # Update metrics based on detection results\n",
    "        annotation_range = parse_annotation(label_path)\n",
    "        gt_fall = annotation_range is not None\n",
    "        true_detection = False\n",
    "        \n",
    "        if gt_fall and annotation_range and detected_frames:\n",
    "            start, end = annotation_range\n",
    "            \n",
    "            # Check if any detected frame is within ground truth range\n",
    "            true_detection = any(start <= frame <= end for frame in detected_frames)\n",
    "        \n",
    "        # Update confusion matrix counts\n",
    "        if gt_fall:\n",
    "            if true_detection:\n",
    "                metrics['tp'] += 1  # True positive\n",
    "            else:\n",
    "                metrics['fn'] += 1  # False negative\n",
    "        else:\n",
    "            if detected_frames:\n",
    "                metrics['fp'] += 1  # False positive\n",
    "            else:\n",
    "                metrics['tn'] += 1  # True negative\n",
    "                \n",
    "        # Record individual result\n",
    "        results.append({\n",
    "            'video': video_file,\n",
    "            'gt_fall': gt_fall,\n",
    "            'detected_fall': bool(detected_frames),\n",
    "            'true_detection': true_detection if gt_fall else None,\n",
    "            'frames_detected': detected_frames\n",
    "        })\n",
    "\n",
    "    return metrics, results\n",
    "\n",
    "def calculate_metrics(metrics):\n",
    "    \"\"\"Calculate performance metrics from confusion matrix.\"\"\"\n",
    "    tp = metrics['tp']\n",
    "    fp = metrics['fp']\n",
    "    tn = metrics['tn']\n",
    "    fn = metrics['fn']\n",
    "\n",
    "    # Calculate derived metrics\n",
    "    total = tp + tn + fp + fn\n",
    "    accuracy = (tp + tn) / total if total > 0 else 0\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'accuracy': round(accuracy * 100, 2),\n",
    "        'precision': round(precision * 100, 2),\n",
    "        'recall': round(recall * 100, 2),\n",
    "        'specificity': round(specificity * 100, 2),\n",
    "        'f1': round(f1 * 100, 2),\n",
    "        'confusion_matrix': {\n",
    "            'TP': tp,\n",
    "            'FP': fp,\n",
    "            'TN': tn,\n",
    "            'FN': fn\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d75a832-251c-425a-9e1e-8d315ed93900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webcam demo function defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Define webcam function for real-time demo\n",
    "def demo_webcam(bot_token=None, chat_id=None):\n",
    "    \"\"\"Run fall detection on webcam feed in Jupyter notebook.\"\"\"\n",
    "    print(\"Starting fall detection on webcam feed...\")\n",
    "    fall_detector = FallDetector()\n",
    "    \n",
    "    # Initialize webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            # Preprocess frame\n",
    "            img = letterbox(frame, 640, stride=64, auto=True)[0]\n",
    "            img_tensor = transforms.ToTensor()(img)\n",
    "            img_tensor = torch.tensor(np.array([img_tensor.numpy()]))\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                img_tensor = img_tensor.half().to(device)\n",
    "\n",
    "            # Inference\n",
    "            with torch.no_grad():\n",
    "                output, _ = model(img_tensor)\n",
    "                output = non_max_suppression_kpt(output, 0.25, 0.65, \n",
    "                                              nc=model.yaml['nc'], \n",
    "                                              nkpt=model.yaml['nkpt'], \n",
    "                                              kpt_label=True)\n",
    "                output = output_to_keypoint(output)\n",
    "\n",
    "            # Visualization\n",
    "            display_frame = img.copy()\n",
    "            display_frame = cv2.cvtColor(display_frame, cv2.COLOR_RGB2BGR)\n",
    "            fall_detected = False\n",
    "            current_state = \"normal\"\n",
    "            conditions = []\n",
    "\n",
    "            if len(output) > 0:\n",
    "                for idx in range(output.shape[0]):\n",
    "                    keypoints = output[idx, 7:].T\n",
    "                    plot_skeleton_kpts(display_frame, keypoints, 3)\n",
    "                    \n",
    "                    is_fall, state, conds = fall_detector.detect_fall(keypoints)\n",
    "                    current_state = state\n",
    "                    conditions = conds\n",
    "                    \n",
    "                    if is_fall:\n",
    "                        fall_detected = True\n",
    "                        if bot_token and chat_id:\n",
    "                            alert_msg = f\"⚠️ <b>ALERT:</b> Fall detected in live feed!\\nTime: {time.strftime('%H:%M:%S')}\\nState: {current_state}\\nConditions: {', '.join(conditions)}\"\n",
    "                            send_telegram_alert(bot_token, chat_id, alert_msg)\n",
    "\n",
    "            # Draw detection status\n",
    "            color = (0, 0, 255) if fall_detected else (0, 255, 0)\n",
    "            cv2.putText(display_frame, f\"State: {current_state}\", (20, 30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            \n",
    "            # Draw conditions\n",
    "            y_offset = 60\n",
    "            for cond in conditions:\n",
    "                cv2.putText(display_frame, cond, (20, y_offset), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "                y_offset += 25\n",
    "\n",
    "            # Privacy notice\n",
    "            cv2.putText(display_frame, \"Privacy-preserving: No data stored\", \n",
    "                       (20, display_frame.shape[0]-20), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "            # For Jupyter: display image directly\n",
    "            rgb_frame = cv2.cvtColor(display_frame, cv2.COLOR_BGR2RGB)\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.imshow(rgb_frame)\n",
    "            plt.title(f\"Fall Detection - State: {current_state}\")\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            clear_output(wait=True)\n",
    "            display(plt.gcf())\n",
    "            plt.close()\n",
    "            \n",
    "            # Add small delay\n",
    "            time.sleep(0.05)\n",
    "            \n",
    "            # Check for keyboard interrupt\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Stopping webcam capture...\")\n",
    "    finally:\n",
    "        cap.release()\n",
    "        print(\"Webcam released\")\n",
    "\n",
    "print(\"Webcam demo function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42efbea3-3727-4b2a-9bb0-2f629ae9aa4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fall detection on test dataset:\n",
      "Video directory: ../datasets/FallDataset\\test/video\n",
      "Label directory: ../datasets/FallDataset\\test/labels\n",
      "\n",
      "Processing video: video (100).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (100).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\naufa\\anaconda3\\envs\\cv_env\\Lib\\site-packages\\torch\\functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3596.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (101).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (101).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (102).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (102).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 7 frames.\n",
      "\n",
      "Processing video: video (103).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (103).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (104).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (104).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (105).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (105).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (106).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (106).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (107).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (107).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (108).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (108).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (71).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (71).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (72).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (72).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (73).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (73).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (74).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (74).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (75).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (75).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 2 frames.\n",
      "\n",
      "Processing video: video (76).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (76).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 14 frames.\n",
      "\n",
      "Processing video: video (77).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (77).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (78).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (78).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 73 frames.\n",
      "\n",
      "Processing video: video (79).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (79).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 68 frames.\n",
      "\n",
      "Processing video: video (80).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (80).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 72 frames.\n",
      "\n",
      "Processing video: video (81).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (81).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (82).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (82).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 48 frames.\n",
      "\n",
      "Processing video: video (83).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (83).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (84).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (84).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (85).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (85).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (86).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (86).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (87).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (87).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (88).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (88).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 3 frames.\n",
      "\n",
      "Processing video: video (89).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (89).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 3 frames.\n",
      "\n",
      "Processing video: video (90).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (90).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 51 frames.\n",
      "\n",
      "Processing video: video (91).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (91).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 51 frames.\n",
      "\n",
      "Processing video: video (92).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (92).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 3 frames.\n",
      "\n",
      "Processing video: video (93).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (93).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (94).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (94).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 5 frames.\n",
      "\n",
      "Processing video: video (95).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (95).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (96).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (96).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 52 frames.\n",
      "\n",
      "Processing video: video (97).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (97).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 15 frames.\n",
      "\n",
      "Processing video: video (98).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (98).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (99).avi\n",
      "Processing video: ../datasets/FallDataset\\test/video\\video (99).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 9 frames.\n",
      "\n",
      "Evaluation Results:\n",
      "Accuracy: 18.42%\n",
      "Precision: 100.0%\n",
      "Recall: 18.42%\n",
      "Specificity: 0%\n",
      "F1-Score: 31.11%\n",
      "\n",
      "Confusion Matrix:\n",
      "True Positives (TP): 7\n",
      "False Positives (FP): 0\n",
      "True Negatives (TN): 0\n",
      "False Negatives (FN): 31\n",
      "\n",
      "Detailed Results:\n",
      "✗ video (100).avi: Ground Truth: True, Detected: False\n",
      "✗ video (101).avi: Ground Truth: True, Detected: False\n",
      "✓ video (102).avi: Ground Truth: True, Detected: True\n",
      "✗ video (103).avi: Ground Truth: True, Detected: False\n",
      "✗ video (104).avi: Ground Truth: True, Detected: False\n",
      "✗ video (105).avi: Ground Truth: True, Detected: False\n",
      "✗ video (106).avi: Ground Truth: True, Detected: False\n",
      "✗ video (107).avi: Ground Truth: True, Detected: False\n",
      "✗ video (108).avi: Ground Truth: True, Detected: False\n",
      "✗ video (71).avi: Ground Truth: True, Detected: False\n",
      "✗ video (72).avi: Ground Truth: True, Detected: False\n",
      "✗ video (73).avi: Ground Truth: True, Detected: False\n",
      "✗ video (74).avi: Ground Truth: True, Detected: False\n",
      "✓ video (75).avi: Ground Truth: True, Detected: True\n",
      "✓ video (76).avi: Ground Truth: True, Detected: True\n",
      "✗ video (77).avi: Ground Truth: True, Detected: False\n",
      "✓ video (78).avi: Ground Truth: True, Detected: True\n",
      "✓ video (79).avi: Ground Truth: True, Detected: True\n",
      "✓ video (80).avi: Ground Truth: True, Detected: True\n",
      "✗ video (81).avi: Ground Truth: True, Detected: False\n",
      "✓ video (82).avi: Ground Truth: True, Detected: True\n",
      "✗ video (83).avi: Ground Truth: True, Detected: False\n",
      "✗ video (84).avi: Ground Truth: True, Detected: False\n",
      "✗ video (85).avi: Ground Truth: True, Detected: False\n",
      "✗ video (86).avi: Ground Truth: True, Detected: False\n",
      "✗ video (87).avi: Ground Truth: True, Detected: False\n",
      "✓ video (88).avi: Ground Truth: True, Detected: True\n",
      "✓ video (89).avi: Ground Truth: True, Detected: True\n",
      "✓ video (90).avi: Ground Truth: True, Detected: True\n",
      "✓ video (91).avi: Ground Truth: True, Detected: True\n",
      "✓ video (92).avi: Ground Truth: True, Detected: True\n",
      "✗ video (93).avi: Ground Truth: True, Detected: False\n",
      "✓ video (94).avi: Ground Truth: True, Detected: True\n",
      "✗ video (95).avi: Ground Truth: True, Detected: False\n",
      "✓ video (96).avi: Ground Truth: True, Detected: True\n",
      "✓ video (97).avi: Ground Truth: True, Detected: True\n",
      "✗ video (98).avi: Ground Truth: True, Detected: False\n",
      "✓ video (99).avi: Ground Truth: True, Detected: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Examples using your Fall Dataset structure\n",
    "# Set paths to your dataset\n",
    "FALL_DATASET_ROOT = \"../datasets/FallDataset\"\n",
    "\n",
    "# Example 1: Process a single test video file\n",
    "def run_single_video_test():\n",
    "    # Find the first video file in the test folder\n",
    "    test_video_dir = os.path.join(FALL_DATASET_ROOT, \"test/video\")\n",
    "    test_label_dir = os.path.join(FALL_DATASET_ROOT, \"test/labels\")\n",
    "    \n",
    "    # Create fall detector instance\n",
    "    fall_detector = FallDetector()\n",
    "    \n",
    "    # List all video files\n",
    "    video_files = [f for f in os.listdir(test_video_dir) \n",
    "                   if f.endswith(('.avi', '.mp4', '.mov'))]\n",
    "    \n",
    "    if not video_files:\n",
    "        print(f\"No video files found in {test_video_dir}\")\n",
    "        return\n",
    "    \n",
    "    # Use the first video file\n",
    "    video_file = video_files[0]\n",
    "    video_path = os.path.join(test_video_dir, video_file)\n",
    "    \n",
    "    # Check if label exists\n",
    "    video_name = os.path.splitext(video_file)[0]\n",
    "    label_path = os.path.join(test_label_dir, f\"{video_name}.txt\")\n",
    "    if os.path.exists(label_path):\n",
    "        print(f\"Processing {video_file} with label\")\n",
    "        detected_frames = process_video(video_path, fall_detector, label_path)\n",
    "    else:\n",
    "        print(f\"Processing {video_file} without label\")\n",
    "        detected_frames = process_video(video_path, fall_detector)\n",
    "    \n",
    "    print(f\"Fall detected in frames: {detected_frames}\")\n",
    "\n",
    "# Example 2: Run the webcam demo\n",
    "def run_webcam_demo():\n",
    "    # Replace with your Telegram credentials if you want alerts\n",
    "    telegram_bot_token = None  # \"YOUR_BOT_TOKEN\"\n",
    "    telegram_chat_id = None    # \"YOUR_CHAT_ID\"\n",
    "    \n",
    "    demo_webcam(telegram_bot_token, telegram_chat_id)\n",
    "\n",
    "# Example 3: Evaluate on your test dataset\n",
    "def evaluate_fall_dataset():\n",
    "    test_video_dir = os.path.join(FALL_DATASET_ROOT, \"test/video\")\n",
    "    test_label_dir = os.path.join(FALL_DATASET_ROOT, \"test/labels\")\n",
    "    \n",
    "    print(f\"Evaluating fall detection on test dataset:\")\n",
    "    print(f\"Video directory: {test_video_dir}\")\n",
    "    print(f\"Label directory: {test_label_dir}\")\n",
    "    \n",
    "    metrics, results = evaluate_videos(test_video_dir, test_label_dir)\n",
    "    evaluation = calculate_metrics(metrics)\n",
    "    \n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"Accuracy: {evaluation['accuracy']}%\")\n",
    "    print(f\"Precision: {evaluation['precision']}%\")\n",
    "    print(f\"Recall: {evaluation['recall']}%\")\n",
    "    print(f\"Specificity: {evaluation['specificity']}%\")\n",
    "    print(f\"F1-Score: {evaluation['f1']}%\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(f\"True Positives (TP): {evaluation['confusion_matrix']['TP']}\")\n",
    "    print(f\"False Positives (FP): {evaluation['confusion_matrix']['FP']}\")\n",
    "    print(f\"True Negatives (TN): {evaluation['confusion_matrix']['TN']}\")\n",
    "    print(f\"False Negatives (FN): {evaluation['confusion_matrix']['FN']}\")\n",
    "    \n",
    "    # Print detailed results\n",
    "    print(\"\\nDetailed Results:\")\n",
    "    for result in results:\n",
    "        status = \"✓\" if result['gt_fall'] == result['detected_fall'] else \"✗\"\n",
    "        print(f\"{status} {result['video']}: Ground Truth: {result['gt_fall']}, Detected: {result['detected_fall']}\")\n",
    "\n",
    "# Uncomment one of these to run the corresponding example\n",
    "# run_single_video_test()\n",
    "# run_webcam_demo()\n",
    "evaluate_fall_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
