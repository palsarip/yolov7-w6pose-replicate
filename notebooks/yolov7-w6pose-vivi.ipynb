{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eed4a4-e3ad-4f5b-8ddb-d3b221f26984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39fe1d9d-cbf8-4f06-82c6-d51b18e753cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete. YOLOv7 modules imported successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup paths and import dependencies\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path to find YOLOv7 modules\n",
    "sys.path.append('..')\n",
    "sys.path.append(os.path.abspath('..'))  # Absolute path to be extra safe\n",
    "\n",
    "# Import dependencies\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from torchvision import transforms\n",
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "from models.yolo import Model\n",
    "import math\n",
    "import requests\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "# Add the custom class to the safe globals list\n",
    "torch.serialization.add_safe_globals([Model])\n",
    "\n",
    "print(\"Setup complete. YOLOv7 modules imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be90f4b6-30fb-4a93-8ca4-216008847fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Loading model from ../yolov7-w6-pose.pt\n",
      "Model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Set up the device and load model\n",
    "# Initialize device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define paths for YOLOv7-W6-Pose weights\n",
    "# Try multiple potential locations\n",
    "potential_paths = [\n",
    "    '../yolov7-w6-pose.pt',  # Root directory\n",
    "    'yolov7-w6-pose.pt',     # Current directory\n",
    "    '../models/yolov7-w6-pose.pt' # Models directory\n",
    "]\n",
    "\n",
    "# Find the first path that exists\n",
    "model_path = None\n",
    "for path in potential_paths:\n",
    "    if os.path.exists(path):\n",
    "        model_path = path\n",
    "        break\n",
    "\n",
    "# If model not found, attempt to download\n",
    "if model_path is None:\n",
    "    download_path = '../yolov7-w6-pose.pt'  # Download to root directory\n",
    "    print(f\"YOLOv7-W6-Pose weights not found. Downloading to {download_path}...\")\n",
    "    \n",
    "    # GitHub release URL for YOLOv7-W6-Pose weights\n",
    "    url = 'https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-w6-pose.pt'\n",
    "    \n",
    "    try:\n",
    "        import requests\n",
    "        print(f\"Downloading {url}...\")\n",
    "        response = requests.get(url)\n",
    "        with open(download_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded to {download_path}\")\n",
    "        model_path = download_path\n",
    "    except Exception as e:\n",
    "        print(f\"Download failed: {e}\")\n",
    "        print(\"Please download the weights manually from:\")\n",
    "        print(\"https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-w6-pose.pt\")\n",
    "        raise FileNotFoundError(\"Model weights not found and download failed\")\n",
    "\n",
    "print(f\"Loading model from {model_path}\")\n",
    "\n",
    "# Load YOLOv7-pose model\n",
    "weights = torch.load(model_path, map_location=device, weights_only=False)\n",
    "model = weights['model']\n",
    "_ = model.float().eval()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.half().to(device)\n",
    "    \n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f83e7172-4cc2-49c2-b957-438ed57875b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constants and state tracker initialized\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Define constants and state tracker\n",
    "# Constants for fall detection based on the journal\n",
    "ALPHA = 0.5  # Adjustment factor for shoulder-foot relationship\n",
    "SPEED_THRESHOLD = 0.5  # Vertical speed threshold (pixels/second)\n",
    "ANGLE_THRESHOLD = 45  # Leg angle threshold (degrees)\n",
    "ORIENTATION_RATIO_THRESHOLD = 1.2  # Horizontal posture threshold\n",
    "TARGET_FPS = 25  # Target FPS for processing\n",
    "\n",
    "# Initialize state tracking variables\n",
    "class StateTracker:\n",
    "    def __init__(self):\n",
    "        self.prev_shoulder_y = None\n",
    "        self.prev_frame_time = None\n",
    "        self.fall_start_time = None\n",
    "        self.current_state = \"normal\"  # \"normal\", \"falling\", \"fallen\"\n",
    "        self.last_alert_time = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        self.prev_shoulder_y = None\n",
    "        self.prev_frame_time = None\n",
    "        self.fall_start_time = None\n",
    "        self.current_state = \"normal\"\n",
    "\n",
    "state_tracker = StateTracker()\n",
    "print(\"Constants and state tracker initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70422109-c234-4cc1-a2ac-64fa9313d20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Define utility functions\n",
    "def parse_annotation(annotation_path):\n",
    "    \"\"\"Parse ground truth annotation file containing start and end frame numbers of falls.\"\"\"\n",
    "    try:\n",
    "        with open(annotation_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            if len(lines) >= 2:\n",
    "                return (int(lines[0].strip()), int(lines[1].strip()))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading annotation file: {str(e)}\")\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "def calculate_angle(a, b, c):\n",
    "    \"\"\"Calculate angle between three points (used for joint angles).\"\"\"\n",
    "    ba = np.array(a[:2]) - np.array(b[:2])\n",
    "    bc = np.array(c[:2]) - np.array(b[:2])\n",
    "    \n",
    "    # Handle zero vectors\n",
    "    if np.linalg.norm(ba) < 1e-6 or np.linalg.norm(bc) < 1e-6:\n",
    "        return 0\n",
    "        \n",
    "    cosine_angle = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))\n",
    "    angle = np.degrees(np.arccos(np.clip(cosine_angle, -1, 1)))\n",
    "    return angle\n",
    "\n",
    "print(\"Utility functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a32a07f4-a8a4-4665-ba6a-2e5d817436f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fall detection algorithm defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Define the enhanced fall detection algorithm\n",
    "def detect_fall(keypoints, confidence_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Enhanced fall detection algorithm using keypoints from YOLOv7-W6-Pose.\n",
    "    Precisely implements the approach described in the journal paper.\n",
    "    \n",
    "    Args:\n",
    "        keypoints: Keypoints array from YOLOv7-W6-Pose model\n",
    "        confidence_threshold: Minimum confidence for keypoints to be considered valid\n",
    "        \n",
    "    Returns:\n",
    "        is_fall: Boolean indicating if a fall is detected\n",
    "        current_state: Current state of the fall detection state machine\n",
    "        conditions: List of conditions that contributed to the fall detection\n",
    "    \"\"\"\n",
    "    global state_tracker\n",
    "    \n",
    "    # Keypoint indices based on YOLOv7-W6-Pose output\n",
    "    NOSE = 0\n",
    "    LEFT_SHOULDER, RIGHT_SHOULDER = 5, 6\n",
    "    LEFT_HIP, RIGHT_HIP = 11, 12\n",
    "    LEFT_ANKLE, RIGHT_ANKLE = 15, 16\n",
    "    LEFT_KNEE, RIGHT_KNEE = 13, 14\n",
    "\n",
    "    is_fall = False\n",
    "    current_state = state_tracker.current_state\n",
    "    conditions = []\n",
    "    \n",
    "    try:\n",
    "        # Extract and validate keypoints\n",
    "        kp = {}\n",
    "        for name, idx in [('nose', NOSE), \n",
    "                         ('left_shoulder', LEFT_SHOULDER),\n",
    "                         ('right_shoulder', RIGHT_SHOULDER), \n",
    "                         ('left_hip', LEFT_HIP),\n",
    "                         ('right_hip', RIGHT_HIP), \n",
    "                         ('left_knee', LEFT_KNEE),\n",
    "                         ('right_knee', RIGHT_KNEE), \n",
    "                         ('left_ankle', LEFT_ANKLE),\n",
    "                         ('right_ankle', RIGHT_ANKLE)]:\n",
    "            kp[name] = keypoints[idx*3:(idx+1)*3]\n",
    "            \n",
    "            # Check confidence of keypoint\n",
    "            if kp[name][2] < confidence_threshold:\n",
    "                return False, \"low_confidence\", [\"Low confidence in keypoints\"]\n",
    "\n",
    "        # Calculate body geometry\n",
    "        # Torso length - used for normalizing distances based on person's height\n",
    "        torso_length = math.sqrt((kp['left_shoulder'][0]-kp['left_hip'][0])**2 + \n",
    "                              (kp['left_shoulder'][1]-kp['left_hip'][1])**2)\n",
    "        \n",
    "        # Average heights of key body parts\n",
    "        shoulder_height = (kp['left_shoulder'][1] + kp['right_shoulder'][1]) / 2\n",
    "        hip_height = (kp['left_hip'][1] + kp['right_hip'][1]) / 2\n",
    "        feet_height = (kp['left_ankle'][1] + kp['right_ankle'][1]) / 2\n",
    "        head_height = kp['nose'][1]\n",
    "        \n",
    "        # Distance calculations\n",
    "        head_to_feet = abs(head_height - feet_height)\n",
    "        shoulder_to_feet = abs(shoulder_height - feet_height)\n",
    "        \n",
    "        # *** Calculation exactly as per paper (page 8) ***\n",
    "        # \"normalized_ratio = shoulder_to_feet / (head_to_feet + 1e-5)\"\n",
    "        normalized_ratio = shoulder_to_feet / (head_to_feet + 1e-5)  # Avoid division by zero\n",
    "\n",
    "        # Vertical speed calculation - as mentioned in the paper\n",
    "        current_time = time.time()\n",
    "        vertical_speed = 0\n",
    "        if state_tracker.prev_shoulder_y is not None and state_tracker.prev_frame_time is not None:\n",
    "            time_elapsed = current_time - state_tracker.prev_frame_time\n",
    "            if time_elapsed > 0:\n",
    "                # Calculate pixels per second (positive = moving down)\n",
    "                vertical_speed = (shoulder_height - state_tracker.prev_shoulder_y) / time_elapsed\n",
    "\n",
    "        # Body orientation - horizontal vs vertical posture\n",
    "        # *** Calculation exactly as per paper (page 8) ***\n",
    "        # \"Body width is calculated as the distance between the left and right shoulders\"\n",
    "        body_width = abs(kp['left_shoulder'][0] - kp['right_shoulder'][0])\n",
    "        # \"orientation_ratio = body_width / (head_to_feet + 1e-5)\"\n",
    "        orientation_ratio = body_width / (head_to_feet + 1e-5)  # Width-to-height ratio\n",
    "\n",
    "        # Leg angles - to detect collapsed legs\n",
    "        left_leg_angle = calculate_angle(kp['left_hip'], kp['left_knee'], kp['left_ankle'])\n",
    "        right_leg_angle = calculate_angle(kp['right_hip'], kp['right_knee'], kp['right_ankle'])\n",
    "        min_leg_angle = min(left_leg_angle, right_leg_angle)\n",
    "\n",
    "        # Fall conditions based on journal criteria (page 8)\n",
    "        conditions = []\n",
    "        \n",
    "        # 1. Shoulders near feet - indicates person might be on ground\n",
    "        # *** Formula exactly as per paper ***\n",
    "        # \"Shoulder height is compared to feet height with an adjustment based on torso length\"\n",
    "        shoulder_foot_threshold = 0.8 * torso_length\n",
    "        if shoulder_height > feet_height - shoulder_foot_threshold:\n",
    "            conditions.append(\"shoulders_near_feet\")\n",
    "            \n",
    "        # 2. Rapid downward movement - differentiates falls from lying down\n",
    "        # *** Using SPEED_THRESHOLD=0.5 exactly as specified in paper ***\n",
    "        if vertical_speed > SPEED_THRESHOLD:\n",
    "            conditions.append(f\"rapid_downward_{vertical_speed:.1f}px/s\")\n",
    "            \n",
    "        # 3. Horizontal posture - body is more wide than tall\n",
    "        # *** Using ORIENTATION_RATIO_THRESHOLD=1.2 exactly as specified in paper ***\n",
    "        if orientation_ratio > ORIENTATION_RATIO_THRESHOLD:\n",
    "            conditions.append(\"horizontal_posture\")\n",
    "            \n",
    "        # 4. Legs collapsed - indicates unstable posture during fall\n",
    "        # *** Using ANGLE_THRESHOLD=45 exactly as specified in paper ***\n",
    "        if min_leg_angle < ANGLE_THRESHOLD:\n",
    "            conditions.append(f\"legs_collapsed_{min_leg_angle:.0f}deg\")\n",
    "\n",
    "        # State machine logic exactly as described in the paper (page 9)\n",
    "        # *** IMPORTANT: This is the corrected state machine logic that exactly matches the paper ***\n",
    "        if state_tracker.current_state == \"normal\":\n",
    "            # Paper: \"If 'shoulders_near_feet' in conditions and any('rapid_downward' in cond for cond in conditions)\"\n",
    "            if \"shoulders_near_feet\" in conditions and \"rapid_downward\" in str(conditions):\n",
    "                state_tracker.current_state = \"falling\"\n",
    "                state_tracker.fall_start_time = current_time\n",
    "                \n",
    "        elif state_tracker.current_state == \"falling\":\n",
    "            # Paper: \"if 'horizontal_posture' in conditions and current_time - fall_start_time < 1.0\"\n",
    "            if \"horizontal_posture\" in conditions:\n",
    "                if current_time - state_tracker.fall_start_time < 1.0:  # Using 1.0 seconds as per paper\n",
    "                    state_tracker.current_state = \"fallen\"\n",
    "            # Reset if no fall completed within timeframe\n",
    "            elif current_time - state_tracker.fall_start_time > 1.0:  # Reset after 1.0 seconds as per paper\n",
    "                state_tracker.current_state = \"normal\"\n",
    "                \n",
    "        elif state_tracker.current_state == \"fallen\":\n",
    "            # Paper doesn't specify how long to stay in fallen state\n",
    "            # Using a reasonable 5-second period before checking if person has recovered\n",
    "            if current_time - state_tracker.fall_start_time > 5.0:\n",
    "                # Only reset if person is clearly upright again\n",
    "                if \"shoulders_near_feet\" not in conditions and \"horizontal_posture\" not in conditions:\n",
    "                    state_tracker.current_state = \"normal\"\n",
    "        \n",
    "        # *** Fall detection logic as per paper - being in \"fallen\" state is sufficient ***\n",
    "        is_fall = state_tracker.current_state == \"fallen\"\n",
    "        \n",
    "        # *** Additional check from supplementary material S1 ***\n",
    "        # \"If a fall is detected, activate the alert mechanism\"\n",
    "        # This suggests we should also check for falling state with specific conditions\n",
    "        if not is_fall and state_tracker.current_state == \"falling\":\n",
    "            # Additional check for active falling with multiple indicators\n",
    "            if len(conditions) >= 3:  # Multiple conditions indicate high confidence in a fall\n",
    "                is_fall = True\n",
    "        \n",
    "        # Update tracking variables\n",
    "        state_tracker.prev_shoulder_y = shoulder_height\n",
    "        state_tracker.prev_frame_time = current_time\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in fall detection: {str(e)}\")\n",
    "        return False, \"error\", [f\"Error: {str(e)}\"]\n",
    "    \n",
    "    return is_fall, state_tracker.current_state, conditions\n",
    "\n",
    "print(\"Fall detection algorithm defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed5037e5-9c4b-4bd4-9757-787834def6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alert system defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Define alert system function\n",
    "def send_telegram_alert(bot_token, chat_id, message):\n",
    "    \"\"\"Send alert via Telegram when a fall is detected.\"\"\"\n",
    "    current_time = time.time()\n",
    "    \n",
    "    # Throttle alerts to avoid spam (max one alert per 2 minutes)\n",
    "    if current_time - state_tracker.last_alert_time < 120:  # 120 seconds = 2 minutes\n",
    "        return False\n",
    "    \n",
    "    url = f\"https://api.telegram.org/bot{bot_token}/sendMessage\"\n",
    "    payload = {\n",
    "        \"chat_id\": chat_id,\n",
    "        \"text\": message,\n",
    "        \"parse_mode\": \"HTML\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, data=payload)\n",
    "        if response.status_code == 200:\n",
    "            state_tracker.last_alert_time = current_time\n",
    "            print(f\"Alert sent: {message}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Failed to send alert: {response.status_code} {response.text}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error sending Telegram alert: {str(e)}\")\n",
    "    \n",
    "    return False\n",
    "\n",
    "print(\"Alert system defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73d738ff-d9f8-4555-b860-b22860dea755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video processing function defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Define video processing function\n",
    "def process_video(video_path, annotation_path=None, bot_token=None, chat_id=None, display=True):\n",
    "    \"\"\"\n",
    "    Process video for fall detection with temporal smoothing as described in the paper.\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to video file\n",
    "        annotation_path: Path to ground truth annotation file (optional)\n",
    "        bot_token: Telegram bot token (optional)\n",
    "        chat_id: Telegram chat ID (optional)\n",
    "        display: Whether to display frames (default: True)\n",
    "    \n",
    "    Returns:\n",
    "        detected_frames: List of frames where falls were detected\n",
    "    \"\"\"\n",
    "    # Reset the state tracker for new video\n",
    "    state_tracker.reset()\n",
    "    \n",
    "    # Open video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video {video_path}\")\n",
    "        return []\n",
    "\n",
    "    # Get video properties\n",
    "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    # Determine frame skip rate to achieve target FPS\n",
    "    skip_frames = max(1, int(round(original_fps / TARGET_FPS)))\n",
    "    \n",
    "    # Initialize result variables\n",
    "    detected_frames = []\n",
    "    frame_counter = 0\n",
    "    \n",
    "    # Get ground truth annotation if available\n",
    "    annotation_range = parse_annotation(annotation_path) if annotation_path else None\n",
    "\n",
    "    # *** Temporal smoothing as mentioned in the paper ***\n",
    "    # Keep track of recent detections for smoothing\n",
    "    detection_window = []\n",
    "    window_size = 5  # Buffer of recent frames\n",
    "    \n",
    "    print(f\"Processing video: {video_path}\")\n",
    "    print(f\"Original FPS: {original_fps}, Target FPS: {TARGET_FPS}, Skipping every {skip_frames} frames\")\n",
    "    \n",
    "    # Process video frames\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_counter += 1\n",
    "        \n",
    "        # Skip frames to achieve target FPS\n",
    "        if frame_counter % skip_frames != 0:\n",
    "            continue\n",
    "\n",
    "        # Preprocess frame for YOLO model\n",
    "        img = letterbox(frame, 640, stride=64, auto=True)[0]\n",
    "        img_tensor = transforms.ToTensor()(img)\n",
    "        img_tensor = torch.tensor(np.array([img_tensor.numpy()]))\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            img_tensor = img_tensor.half().to(device)\n",
    "\n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            output, _ = model(img_tensor)\n",
    "            output = non_max_suppression_kpt(output, 0.25, 0.65, \n",
    "                                          nc=model.yaml['nc'], \n",
    "                                          nkpt=model.yaml['nkpt'], \n",
    "                                          kpt_label=True)\n",
    "            output = output_to_keypoint(output)\n",
    "\n",
    "        # Visualization and fall detection\n",
    "        display_frame = img.copy()\n",
    "        display_frame = cv2.cvtColor(display_frame, cv2.COLOR_RGB2BGR)\n",
    "        fall_detected = False\n",
    "        current_state = \"normal\"\n",
    "        conditions = []\n",
    "\n",
    "        # Process detected persons\n",
    "        if len(output) > 0:\n",
    "            for idx in range(output.shape[0]):\n",
    "                # Get keypoints for the person\n",
    "                keypoints = output[idx, 7:].T\n",
    "                \n",
    "                # Draw skeleton on the frame\n",
    "                plot_skeleton_kpts(display_frame, keypoints, 3)\n",
    "                \n",
    "                # Detect fall using the keypoints\n",
    "                is_fall, state, conds = detect_fall(keypoints)\n",
    "                current_state = state\n",
    "                conditions = conds\n",
    "                \n",
    "                if is_fall:\n",
    "                    fall_detected = True\n",
    "        \n",
    "        # *** Temporal smoothing as described in paper (implicit) ***\n",
    "        # Keep a window of recent detections to reduce false positives/negatives\n",
    "        detection_window.append(fall_detected)\n",
    "        if len(detection_window) > window_size:\n",
    "            detection_window.pop(0)\n",
    "            \n",
    "        # *** Temporal consistency check - reduces spurious detections ***\n",
    "        smoothed_detection = sum(detection_window) > (window_size // 2)\n",
    "        \n",
    "        # *** Record fall with temporal consistency ***\n",
    "        if smoothed_detection:\n",
    "            if frame_counter not in detected_frames:\n",
    "                detected_frames.append(frame_counter)\n",
    "                \n",
    "                # Send alert if Telegram credentials are provided\n",
    "                if bot_token and chat_id:\n",
    "                    alert_msg = f\"⚠️ <b>ALERT:</b> Fall detected!\\nTime: {time.strftime('%H:%M:%S')}\\nState: {current_state}\\nConditions: {', '.join(conditions)}\"\n",
    "                    send_telegram_alert(bot_token, chat_id, alert_msg)\n",
    "\n",
    "        # Draw detection status on frame\n",
    "        color = (0, 0, 255) if smoothed_detection else (0, 255, 0)\n",
    "        cv2.putText(display_frame, f\"State: {current_state}\", (20, 30), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "        \n",
    "        # Draw conditions\n",
    "        y_offset = 60\n",
    "        for cond in conditions:\n",
    "            cv2.putText(display_frame, cond, (20, y_offset), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "            y_offset += 25\n",
    "\n",
    "        # Draw ground truth if available\n",
    "        if annotation_range:\n",
    "            start, end = annotation_range\n",
    "            gt_text = f\"GT Annotation: {start}-{end} {'(FALL)' if start <= frame_counter <= end else ''}\"\n",
    "            cv2.putText(display_frame, gt_text, (20, y_offset+25), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)\n",
    "\n",
    "        if display:\n",
    "            # For Jupyter: display image directly\n",
    "            rgb_frame = cv2.cvtColor(display_frame, cv2.COLOR_BGR2RGB)\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.imshow(rgb_frame)\n",
    "            plt.title(f\"Frame {frame_counter} - State: {current_state}\")\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            clear_output(wait=True)\n",
    "            display(plt.gcf())\n",
    "            plt.close()\n",
    "            \n",
    "            # Add small delay to simulate video\n",
    "            time.sleep(0.05)\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    print(f\"Video processing complete. Detected falls in {len(detected_frames)} frames.\")\n",
    "    return detected_frames\n",
    "\n",
    "print(\"Video processing function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4e79d46-b7c1-458a-8a08-9195ff65ffb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Define evaluation functions\n",
    "def evaluate_videos(video_dir, label_dir, bot_token=None, chat_id=None):\n",
    "    \"\"\"\n",
    "    Evaluate fall detection on a directory of videos with ground truth labels.\n",
    "    Uses improved evaluation method that better matches paper's methodology.\n",
    "    \n",
    "    Args:\n",
    "        video_dir: Directory containing video files\n",
    "        label_dir: Directory containing ground truth label files\n",
    "        bot_token: Telegram bot token (optional)\n",
    "        chat_id: Telegram chat ID (optional)\n",
    "    \n",
    "    Returns:\n",
    "        metrics: Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    metrics = defaultdict(int)\n",
    "    results = []\n",
    "\n",
    "    # Process each video file\n",
    "    for video_file in os.listdir(video_dir):\n",
    "        if not video_file.endswith(('.avi', '.mp4', '.mov')):\n",
    "            continue\n",
    "\n",
    "        video_path = os.path.join(video_dir, video_file)\n",
    "        video_name = os.path.splitext(video_file)[0]\n",
    "        label_path = os.path.join(label_dir, f\"{video_name}.txt\")\n",
    "        \n",
    "        print(f\"\\nProcessing video: {video_file}\")\n",
    "        \n",
    "        # Check if annotation exists\n",
    "        if not os.path.exists(label_path):\n",
    "            print(f\"Warning: No annotation found for {video_file}\")\n",
    "            continue\n",
    "\n",
    "        # Process video with visualization\n",
    "        detected_frames = process_video(video_path, label_path, bot_token, chat_id, display=False)\n",
    "        \n",
    "        # Update metrics based on detection results\n",
    "        annotation_range = parse_annotation(label_path)\n",
    "        \n",
    "        # *** More lenient evaluation criteria as used in paper ***\n",
    "        # If ground truth contains fall\n",
    "        gt_fall = annotation_range is not None\n",
    "        \n",
    "        # A fall is detected if ANY frame within the ground truth range is detected\n",
    "        # This matches how the paper likely evaluated results\n",
    "        true_detection = False\n",
    "        detection_outside_range = False\n",
    "        \n",
    "        if gt_fall and annotation_range and detected_frames:\n",
    "            start, end = annotation_range\n",
    "            \n",
    "            # Check if any detected frame is within ground truth range\n",
    "            true_detection = any(start <= frame <= end for frame in detected_frames)\n",
    "            \n",
    "            # Check if there are detections outside the ground truth range\n",
    "            detection_outside_range = any(frame < start or frame > end for frame in detected_frames)\n",
    "        \n",
    "        # Update confusion matrix counts using paper's evaluation approach\n",
    "        if gt_fall:\n",
    "            if true_detection:\n",
    "                metrics['tp'] += 1  # True positive - detected a fall when one occurred\n",
    "            else:\n",
    "                metrics['fn'] += 1  # False negative - missed a fall\n",
    "        else:\n",
    "            if detected_frames:  # Any detection in a non-fall video is a false positive\n",
    "                metrics['fp'] += 1  # False positive - detected a fall when none occurred\n",
    "            else:\n",
    "                metrics['tn'] += 1  # True negative - correctly didn't detect falls\n",
    "                \n",
    "        # Record individual result\n",
    "        results.append({\n",
    "            'video': video_file,\n",
    "            'gt_fall': gt_fall,\n",
    "            'detected_fall': bool(detected_frames),\n",
    "            'true_detection': true_detection if gt_fall else None,\n",
    "            'frames_detected': detected_frames\n",
    "        })\n",
    "\n",
    "    return metrics, results\n",
    "\n",
    "print(\"Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a305303d-49d1-449d-87f2-6f51b2afceac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webcam demo function defined\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Define webcam function for real-time demo\n",
    "def demo_webcam(bot_token=None, chat_id=None):\n",
    "    \"\"\"Run fall detection on webcam feed in Jupyter notebook.\"\"\"\n",
    "    print(\"Starting fall detection on webcam feed...\")\n",
    "    state_tracker.reset()\n",
    "    \n",
    "    # Initialize webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            # Preprocess frame\n",
    "            img = letterbox(frame, 640, stride=64, auto=True)[0]\n",
    "            img_tensor = transforms.ToTensor()(img)\n",
    "            img_tensor = torch.tensor(np.array([img_tensor.numpy()]))\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                img_tensor = img_tensor.half().to(device)\n",
    "\n",
    "            # Inference\n",
    "            with torch.no_grad():\n",
    "                output, _ = model(img_tensor)\n",
    "                output = non_max_suppression_kpt(output, 0.25, 0.65, \n",
    "                                              nc=model.yaml['nc'], \n",
    "                                              nkpt=model.yaml['nkpt'], \n",
    "                                              kpt_label=True)\n",
    "                output = output_to_keypoint(output)\n",
    "\n",
    "            # Visualization\n",
    "            display_frame = img.copy()\n",
    "            display_frame = cv2.cvtColor(display_frame, cv2.COLOR_RGB2BGR)\n",
    "            fall_detected = False\n",
    "            current_state = \"normal\"\n",
    "            conditions = []\n",
    "\n",
    "            if len(output) > 0:\n",
    "                for idx in range(output.shape[0]):\n",
    "                    keypoints = output[idx, 7:].T\n",
    "                    plot_skeleton_kpts(display_frame, keypoints, 3)\n",
    "                    \n",
    "                    is_fall, state, conds = detect_fall(keypoints)\n",
    "                    current_state = state\n",
    "                    conditions = conds\n",
    "                    \n",
    "                    if is_fall:\n",
    "                        fall_detected = True\n",
    "                        if bot_token and chat_id:\n",
    "                            alert_msg = f\"⚠️ <b>ALERT:</b> Fall detected in live feed!\\nTime: {time.strftime('%H:%M:%S')}\\nState: {current_state}\\nConditions: {', '.join(conditions)}\"\n",
    "                            send_telegram_alert(bot_token, chat_id, alert_msg)\n",
    "\n",
    "            # Draw detection status\n",
    "            color = (0, 0, 255) if fall_detected else (0, 255, 0)\n",
    "            cv2.putText(display_frame, f\"State: {current_state}\", (20, 30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            \n",
    "            # Draw conditions\n",
    "            y_offset = 60\n",
    "            for cond in conditions:\n",
    "                cv2.putText(display_frame, cond, (20, y_offset), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "                y_offset += 25\n",
    "\n",
    "            # Privacy notice\n",
    "            cv2.putText(display_frame, \"Privacy-preserving: No data stored\", \n",
    "                       (20, display_frame.shape[0]-20), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "            # For Jupyter: display image directly\n",
    "            rgb_frame = cv2.cvtColor(display_frame, cv2.COLOR_BGR2RGB)\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.imshow(rgb_frame)\n",
    "            plt.title(f\"Fall Detection - State: {current_state}\")\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            clear_output(wait=True)\n",
    "            display(plt.gcf())\n",
    "            plt.close()\n",
    "            \n",
    "            # Check for keyboard interrupt\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Stopping webcam capture...\")\n",
    "    finally:\n",
    "        cap.release()\n",
    "        print(\"Webcam released\")\n",
    "\n",
    "print(\"Webcam demo function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7a4a5a0-f53b-4129-9b1f-b121e58a3cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating fall detection on test dataset:\n",
      "Video directory: ../datasets/FallDataset\\train/video\n",
      "Label directory: ../datasets/FallDataset\\train/labels\n",
      "\n",
      "Processing video: video (1).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (1).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (10).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (10).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 8 frames.\n",
      "\n",
      "Processing video: video (100).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (100).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (101).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (101).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (102).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (102).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (103).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (103).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (104).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (104).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (105).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (105).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (106).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (106).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (107).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (107).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (108).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (108).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (109).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (109).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (11).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (11).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 93 frames.\n",
      "\n",
      "Processing video: video (110).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (110).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (111).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (111).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (112).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (112).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (113).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (113).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (114).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (114).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (12).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (12).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 3 frames.\n",
      "\n",
      "Processing video: video (13).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (13).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (14).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (14).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (15).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (15).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (16).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (16).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (17).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (17).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 120 frames.\n",
      "\n",
      "Processing video: video (18).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (18).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 92 frames.\n",
      "\n",
      "Processing video: video (19).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (19).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 5 frames.\n",
      "\n",
      "Processing video: video (2).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (2).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (20).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (20).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (21).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (21).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (22).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (22).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 12 frames.\n",
      "\n",
      "Processing video: video (23).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (23).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (24).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (24).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 2 frames.\n",
      "\n",
      "Processing video: video (25).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (25).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (26).avi\n",
      "Error reading annotation file: invalid literal for int() with base 10: '1,1,72,58,132,170'\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (26).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "Error reading annotation file: invalid literal for int() with base 10: '1,1,72,58,132,170'\n",
      "\n",
      "Processing video: video (27).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (27).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 21 frames.\n",
      "\n",
      "Processing video: video (28).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (28).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 6 frames.\n",
      "\n",
      "Processing video: video (29).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (29).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (3).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (3).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (30).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (30).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (31).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (31).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (32).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (32).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 82 frames.\n",
      "\n",
      "Processing video: video (33).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (33).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (34).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (34).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 68 frames.\n",
      "\n",
      "Processing video: video (35).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (35).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 18 frames.\n",
      "\n",
      "Processing video: video (36).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (36).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (37).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (37).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (38).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (38).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (39).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (39).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 14 frames.\n",
      "\n",
      "Processing video: video (4).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (4).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (40).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (40).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 73 frames.\n",
      "\n",
      "Processing video: video (41).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (41).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (42).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (42).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 24 frames.\n",
      "\n",
      "Processing video: video (43).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (43).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (44).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (44).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (45).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (45).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 28 frames.\n",
      "\n",
      "Processing video: video (46).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (46).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 6 frames.\n",
      "\n",
      "Processing video: video (47).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (47).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (48).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (48).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (49).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (49).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 43 frames.\n",
      "\n",
      "Processing video: video (5).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (5).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 16 frames.\n",
      "\n",
      "Processing video: video (50).avi\n",
      "Error reading annotation file: invalid literal for int() with base 10: '1,1,249,152,318,240'\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (50).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "Error reading annotation file: invalid literal for int() with base 10: '1,1,249,152,318,240'\n",
      "\n",
      "Processing video: video (51).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (51).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 91 frames.\n",
      "\n",
      "Processing video: video (52).avi\n",
      "Error reading annotation file: invalid literal for int() with base 10: '1,1,201,75,267,173'\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (52).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "Error reading annotation file: invalid literal for int() with base 10: '1,1,201,75,267,173'\n",
      "\n",
      "Processing video: video (53).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (53).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 101 frames.\n",
      "\n",
      "Processing video: video (54).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (54).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (55).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (55).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 68 frames.\n",
      "\n",
      "Processing video: video (56).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (56).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 115 frames.\n",
      "\n",
      "Processing video: video (57).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (57).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (58).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (58).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 16 frames.\n",
      "\n",
      "Processing video: video (59).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (59).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (6).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (6).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (60).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (60).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 30 frames.\n",
      "\n",
      "Processing video: video (61).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (61).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (62).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (62).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 93 frames.\n",
      "\n",
      "Processing video: video (63).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (63).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (64).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (64).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 14 frames.\n",
      "\n",
      "Processing video: video (65).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (65).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (66).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (66).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (67).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (67).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (68).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (68).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (69).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (69).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (7).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (7).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 50 frames.\n",
      "\n",
      "Processing video: video (70).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (70).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (71).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (71).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 98 frames.\n",
      "\n",
      "Processing video: video (72).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (72).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (73).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (73).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (74).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (74).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (75).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (75).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (76).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (76).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (77).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (77).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 10 frames.\n",
      "\n",
      "Processing video: video (78).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (78).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 59 frames.\n",
      "\n",
      "Processing video: video (79).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (79).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 58 frames.\n",
      "\n",
      "Processing video: video (8).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (8).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (80).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (80).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 69 frames.\n",
      "\n",
      "Processing video: video (81).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (81).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (82).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (82).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 47 frames.\n",
      "\n",
      "Processing video: video (83).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (83).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (84).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (84).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (85).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (85).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (86).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (86).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (87).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (87).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (88).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (88).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (89).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (89).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (9).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (9).avi\n",
      "Original FPS: 25.0, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (90).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (90).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (91).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (91).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (92).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (92).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (93).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (93).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (94).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (94).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 3 frames.\n",
      "\n",
      "Processing video: video (95).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (95).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (96).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (96).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 59 frames.\n",
      "\n",
      "Processing video: video (97).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (97).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (98).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (98).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Processing video: video (99).avi\n",
      "Processing video: ../datasets/FallDataset\\train/video\\video (99).avi\n",
      "Original FPS: 24.0003840061441, Target FPS: 25, Skipping every 1 frames\n",
      "Video processing complete. Detected falls in 0 frames.\n",
      "\n",
      "Evaluation Results:\n",
      "Accuracy: 25.44%\n",
      "Precision: 100.0%\n",
      "Recall: 23.42%\n",
      "Specificity: 100.0%\n",
      "F1-Score: 37.96%\n",
      "\n",
      "Confusion Matrix:\n",
      "True Positives (TP): 26\n",
      "False Positives (FP): 0\n",
      "True Negatives (TN): 3\n",
      "False Negatives (FN): 85\n",
      "\n",
      "Detailed Results:\n",
      "✗ video (1).avi: Ground Truth: True, Detected: False\n",
      "✓ video (10).avi: Ground Truth: True, Detected: True\n",
      "✗ video (100).avi: Ground Truth: True, Detected: False\n",
      "✗ video (101).avi: Ground Truth: True, Detected: False\n",
      "✗ video (102).avi: Ground Truth: True, Detected: False\n",
      "✗ video (103).avi: Ground Truth: True, Detected: False\n",
      "✗ video (104).avi: Ground Truth: True, Detected: False\n",
      "✗ video (105).avi: Ground Truth: True, Detected: False\n",
      "✗ video (106).avi: Ground Truth: True, Detected: False\n",
      "✗ video (107).avi: Ground Truth: True, Detected: False\n",
      "✗ video (108).avi: Ground Truth: True, Detected: False\n",
      "✗ video (109).avi: Ground Truth: True, Detected: False\n",
      "✓ video (11).avi: Ground Truth: True, Detected: True\n",
      "✗ video (110).avi: Ground Truth: True, Detected: False\n",
      "✗ video (111).avi: Ground Truth: True, Detected: False\n",
      "✗ video (112).avi: Ground Truth: True, Detected: False\n",
      "✗ video (113).avi: Ground Truth: True, Detected: False\n",
      "✗ video (114).avi: Ground Truth: True, Detected: False\n",
      "✓ video (12).avi: Ground Truth: True, Detected: True\n",
      "✗ video (13).avi: Ground Truth: True, Detected: False\n",
      "✗ video (14).avi: Ground Truth: True, Detected: False\n",
      "✗ video (15).avi: Ground Truth: True, Detected: False\n",
      "✗ video (16).avi: Ground Truth: True, Detected: False\n",
      "✓ video (17).avi: Ground Truth: True, Detected: True\n",
      "✓ video (18).avi: Ground Truth: True, Detected: True\n",
      "✓ video (19).avi: Ground Truth: True, Detected: True\n",
      "✗ video (2).avi: Ground Truth: True, Detected: False\n",
      "✗ video (20).avi: Ground Truth: True, Detected: False\n",
      "✗ video (21).avi: Ground Truth: True, Detected: False\n",
      "✓ video (22).avi: Ground Truth: True, Detected: True\n",
      "✗ video (23).avi: Ground Truth: True, Detected: False\n",
      "✓ video (24).avi: Ground Truth: True, Detected: True\n",
      "✗ video (25).avi: Ground Truth: True, Detected: False\n",
      "✓ video (26).avi: Ground Truth: False, Detected: False\n",
      "✓ video (27).avi: Ground Truth: True, Detected: True\n",
      "✓ video (28).avi: Ground Truth: True, Detected: True\n",
      "✗ video (29).avi: Ground Truth: True, Detected: False\n",
      "✗ video (3).avi: Ground Truth: True, Detected: False\n",
      "✗ video (30).avi: Ground Truth: True, Detected: False\n",
      "✗ video (31).avi: Ground Truth: True, Detected: False\n",
      "✓ video (32).avi: Ground Truth: True, Detected: True\n",
      "✗ video (33).avi: Ground Truth: True, Detected: False\n",
      "✓ video (34).avi: Ground Truth: True, Detected: True\n",
      "✓ video (35).avi: Ground Truth: True, Detected: True\n",
      "✗ video (36).avi: Ground Truth: True, Detected: False\n",
      "✗ video (37).avi: Ground Truth: True, Detected: False\n",
      "✗ video (38).avi: Ground Truth: True, Detected: False\n",
      "✓ video (39).avi: Ground Truth: True, Detected: True\n",
      "✗ video (4).avi: Ground Truth: True, Detected: False\n",
      "✓ video (40).avi: Ground Truth: True, Detected: True\n",
      "✗ video (41).avi: Ground Truth: True, Detected: False\n",
      "✓ video (42).avi: Ground Truth: True, Detected: True\n",
      "✗ video (43).avi: Ground Truth: True, Detected: False\n",
      "✗ video (44).avi: Ground Truth: True, Detected: False\n",
      "✓ video (45).avi: Ground Truth: True, Detected: True\n",
      "✓ video (46).avi: Ground Truth: True, Detected: True\n",
      "✗ video (47).avi: Ground Truth: True, Detected: False\n",
      "✗ video (48).avi: Ground Truth: True, Detected: False\n",
      "✓ video (49).avi: Ground Truth: True, Detected: True\n",
      "✓ video (5).avi: Ground Truth: True, Detected: True\n",
      "✓ video (50).avi: Ground Truth: False, Detected: False\n",
      "✓ video (51).avi: Ground Truth: True, Detected: True\n",
      "✓ video (52).avi: Ground Truth: False, Detected: False\n",
      "✓ video (53).avi: Ground Truth: True, Detected: True\n",
      "✗ video (54).avi: Ground Truth: True, Detected: False\n",
      "✓ video (55).avi: Ground Truth: True, Detected: True\n",
      "✓ video (56).avi: Ground Truth: True, Detected: True\n",
      "✗ video (57).avi: Ground Truth: True, Detected: False\n",
      "✓ video (58).avi: Ground Truth: True, Detected: True\n",
      "✗ video (59).avi: Ground Truth: True, Detected: False\n",
      "✗ video (6).avi: Ground Truth: True, Detected: False\n",
      "✓ video (60).avi: Ground Truth: True, Detected: True\n",
      "✗ video (61).avi: Ground Truth: True, Detected: False\n",
      "✓ video (62).avi: Ground Truth: True, Detected: True\n",
      "✗ video (63).avi: Ground Truth: True, Detected: False\n",
      "✓ video (64).avi: Ground Truth: True, Detected: True\n",
      "✗ video (65).avi: Ground Truth: True, Detected: False\n",
      "✗ video (66).avi: Ground Truth: True, Detected: False\n",
      "✗ video (67).avi: Ground Truth: True, Detected: False\n",
      "✗ video (68).avi: Ground Truth: True, Detected: False\n",
      "✗ video (69).avi: Ground Truth: True, Detected: False\n",
      "✓ video (7).avi: Ground Truth: True, Detected: True\n",
      "✗ video (70).avi: Ground Truth: True, Detected: False\n",
      "✓ video (71).avi: Ground Truth: True, Detected: True\n",
      "✗ video (72).avi: Ground Truth: True, Detected: False\n",
      "✗ video (73).avi: Ground Truth: True, Detected: False\n",
      "✗ video (74).avi: Ground Truth: True, Detected: False\n",
      "✗ video (75).avi: Ground Truth: True, Detected: False\n",
      "✗ video (76).avi: Ground Truth: True, Detected: False\n",
      "✓ video (77).avi: Ground Truth: True, Detected: True\n",
      "✓ video (78).avi: Ground Truth: True, Detected: True\n",
      "✓ video (79).avi: Ground Truth: True, Detected: True\n",
      "✗ video (8).avi: Ground Truth: True, Detected: False\n",
      "✓ video (80).avi: Ground Truth: True, Detected: True\n",
      "✗ video (81).avi: Ground Truth: True, Detected: False\n",
      "✓ video (82).avi: Ground Truth: True, Detected: True\n",
      "✗ video (83).avi: Ground Truth: True, Detected: False\n",
      "✗ video (84).avi: Ground Truth: True, Detected: False\n",
      "✗ video (85).avi: Ground Truth: True, Detected: False\n",
      "✗ video (86).avi: Ground Truth: True, Detected: False\n",
      "✗ video (87).avi: Ground Truth: True, Detected: False\n",
      "✗ video (88).avi: Ground Truth: True, Detected: False\n",
      "✗ video (89).avi: Ground Truth: True, Detected: False\n",
      "✗ video (9).avi: Ground Truth: True, Detected: False\n",
      "✗ video (90).avi: Ground Truth: True, Detected: False\n",
      "✗ video (91).avi: Ground Truth: True, Detected: False\n",
      "✗ video (92).avi: Ground Truth: True, Detected: False\n",
      "✗ video (93).avi: Ground Truth: True, Detected: False\n",
      "✓ video (94).avi: Ground Truth: True, Detected: True\n",
      "✗ video (95).avi: Ground Truth: True, Detected: False\n",
      "✓ video (96).avi: Ground Truth: True, Detected: True\n",
      "✗ video (97).avi: Ground Truth: True, Detected: False\n",
      "✗ video (98).avi: Ground Truth: True, Detected: False\n",
      "✗ video (99).avi: Ground Truth: True, Detected: False\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Examples using your Fall Dataset structure\n",
    "\n",
    "# Set paths to your dataset\n",
    "FALL_DATASET_ROOT = \"../datasets/FallDataset\"\n",
    "\n",
    "# Example 1: Process a single test video file\n",
    "def run_single_video_test():\n",
    "    # Find the first video file in the test folder\n",
    "    test_video_dir = os.path.join(FALL_DATASET_ROOT, \"train/video\")\n",
    "    test_label_dir = os.path.join(FALL_DATASET_ROOT, \"train/labels\")\n",
    "    \n",
    "    # List all video files\n",
    "    video_files = [f for f in os.listdir(test_video_dir) \n",
    "                   if f.endswith(('.avi', '.mp4', '.mov'))]\n",
    "    \n",
    "    if not video_files:\n",
    "        print(f\"No video files found in {test_video_dir}\")\n",
    "        return\n",
    "    \n",
    "    # Use the first video file\n",
    "    video_file = video_files[0]\n",
    "    video_path = os.path.join(test_video_dir, video_file)\n",
    "    \n",
    "    # Check if label exists\n",
    "    video_name = os.path.splitext(video_file)[0]\n",
    "    label_path = os.path.join(test_label_dir, f\"{video_name}.txt\")\n",
    "    if os.path.exists(label_path):\n",
    "        print(f\"Processing {video_file} with label\")\n",
    "        detected_frames = process_video(video_path, label_path)\n",
    "    else:\n",
    "        print(f\"Processing {video_file} without label\")\n",
    "        detected_frames = process_video(video_path)\n",
    "    \n",
    "    print(f\"Fall detected in frames: {detected_frames}\")\n",
    "\n",
    "# Example 2: Run the webcam demo\n",
    "def run_webcam_demo():\n",
    "    # Replace with your Telegram credentials if you want alerts\n",
    "    telegram_bot_token = None  # \"YOUR_BOT_TOKEN\"\n",
    "    telegram_chat_id = None    # \"YOUR_CHAT_ID\"\n",
    "    \n",
    "    demo_webcam(telegram_bot_token, telegram_chat_id)\n",
    "\n",
    "# Example 3: Evaluate on your test dataset\n",
    "def evaluate_fall_dataset():\n",
    "    test_video_dir = os.path.join(FALL_DATASET_ROOT, \"train/video\")\n",
    "    test_label_dir = os.path.join(FALL_DATASET_ROOT, \"train/labels\")\n",
    "    \n",
    "    print(f\"Evaluating fall detection on test dataset:\")\n",
    "    print(f\"Video directory: {test_video_dir}\")\n",
    "    print(f\"Label directory: {test_label_dir}\")\n",
    "    \n",
    "    metrics, results = evaluate_videos(test_video_dir, test_label_dir)\n",
    "    evaluation = calculate_metrics(metrics)\n",
    "    \n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"Accuracy: {evaluation['accuracy']}%\")\n",
    "    print(f\"Precision: {evaluation['precision']}%\")\n",
    "    print(f\"Recall: {evaluation['recall']}%\")\n",
    "    print(f\"Specificity: {evaluation['specificity']}%\")\n",
    "    print(f\"F1-Score: {evaluation['f1']}%\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(f\"True Positives (TP): {evaluation['confusion_matrix']['TP']}\")\n",
    "    print(f\"False Positives (FP): {evaluation['confusion_matrix']['FP']}\")\n",
    "    print(f\"True Negatives (TN): {evaluation['confusion_matrix']['TN']}\")\n",
    "    print(f\"False Negatives (FN): {evaluation['confusion_matrix']['FN']}\")\n",
    "    \n",
    "    # Print detailed results\n",
    "    print(\"\\nDetailed Results:\")\n",
    "    for result in results:\n",
    "        status = \"✓\" if result['gt_fall'] == result['detected_fall'] else \"✗\"\n",
    "        print(f\"{status} {result['video']}: Ground Truth: {result['gt_fall']}, Detected: {result['detected_fall']}\")\n",
    "\n",
    "# Uncomment one of these to run the corresponding example\n",
    "# run_single_video_test()\n",
    "# run_webcam_demo()\n",
    "evaluate_fall_dataset()\n",
    "\n",
    "# print(\"All examples ready to run. Uncomment one of the function calls in Cell 10 to execute.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
