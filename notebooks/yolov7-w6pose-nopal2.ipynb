{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f72a11a9-9c33-4353-90ab-23c323bd8f63",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "707a1d00-f913-4688-be6c-76233bd17281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Model file found at ../models/yolov7-w6-pose.pt\n",
      "Camera 0 is available\n",
      "System setup completed!\n"
     ]
    }
   ],
   "source": [
    "# 1. System Setup for Fall Detection Application (without Telegram integration)\n",
    "\n",
    "# Import necessary standard libraries\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import argparse\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Import PyTorch libraries\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Set up GPU if available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define constants and configuration\n",
    "CONFIG = {\n",
    "    'model_path': '../models/yolov7-w6-pose.pt',  # Path to the pre-trained YOLOv7-w6-pose model\n",
    "    'input_size': (960, 960),           # Input size for the model (width, height)\n",
    "    'confidence_threshold': 0.25,       # Confidence threshold for detection\n",
    "    'iou_threshold': 0.45,              # IoU threshold for NMS\n",
    "    'device': device,                   # Device to run inference on\n",
    "}\n",
    "\n",
    "# Check if the model file exists\n",
    "if not os.path.exists(CONFIG['model_path']):\n",
    "    print(f\"Model file not found at {CONFIG['model_path']}. Please download it first.\")\n",
    "    print(\"You can download it from: https://github.com/WongKinYiu/yolov7/releases\")\n",
    "else:\n",
    "    print(f\"Model file found at {CONFIG['model_path']}\")\n",
    "\n",
    "# Function to check camera availability\n",
    "def check_camera(camera_id=0):\n",
    "    cap = cv2.VideoCapture(camera_id)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Camera {camera_id} is not available\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"Camera {camera_id} is available\")\n",
    "        cap.release()\n",
    "        return True\n",
    "\n",
    "# Check camera\n",
    "camera_available = check_camera()\n",
    "\n",
    "# Function to download YOLOv7-W6-Pose model if not present\n",
    "def download_model(model_path='yolov7-w6-pose.pt'):\n",
    "    # This is a placeholder. In a real implementation,\n",
    "    # you would use requests or similar to download from the GitHub repo\n",
    "    print(\"Note: In a real implementation, this function would download the model.\")\n",
    "    print(\"Please download manually from: https://github.com/WongKinYiu/yolov7/releases\")\n",
    "    return False\n",
    "\n",
    "# If model doesn't exist, try to download it\n",
    "if not os.path.exists(CONFIG['model_path']):\n",
    "    download_successful = download_model(CONFIG['model_path'])\n",
    "    if not download_successful:\n",
    "        print(\"Warning: Could not automatically download the model.\")\n",
    "        print(\"Please download it manually before proceeding.\")\n",
    "\n",
    "print(\"System setup completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2574d478-1ce3-46de-b345-cc45f376a3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Video Processing Pipeline\n",
    "\n",
    "def preprocess_frame(frame, input_size=(960, 960)):\n",
    "    \"\"\"\n",
    "    Preprocess a frame for input to YOLOv7-W6-Pose.\n",
    "    \n",
    "    Args:\n",
    "        frame (numpy.ndarray): Input frame from video/camera\n",
    "        input_size (tuple): Target size for model input (width, height)\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Preprocessed frame tensor ready for model input\n",
    "    \"\"\"\n",
    "    # Convert BGR to RGB (OpenCV uses BGR by default)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Create a PIL image from the numpy array\n",
    "    image = Image.fromarray(rgb_frame)\n",
    "    \n",
    "    # Original dimensions\n",
    "    orig_width, orig_height = image.size\n",
    "    \n",
    "    # Calculate the letterbox dimensions to maintain aspect ratio\n",
    "    ratio = min(input_size[0] / orig_width, input_size[1] / orig_height)\n",
    "    new_width = int(orig_width * ratio)\n",
    "    new_height = int(orig_height * ratio)\n",
    "    \n",
    "    # Resize the image\n",
    "    resized_image = image.resize((new_width, new_height), Image.LANCZOS)\n",
    "    \n",
    "    # Create a new image with the target size and paste the resized image\n",
    "    letterboxed_image = Image.new(\"RGB\", input_size, (114, 114, 114))\n",
    "    letterboxed_image.paste(resized_image, ((input_size[0] - new_width) // 2, \n",
    "                                           (input_size[1] - new_height) // 2))\n",
    "    \n",
    "    # Convert to tensor and normalize\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Apply transforms\n",
    "    tensor = transform(letterboxed_image)\n",
    "    \n",
    "    # Add batch dimension\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    \n",
    "    return tensor, ratio, (input_size[0] - new_width) // 2, (input_size[1] - new_height) // 2\n",
    "\n",
    "\n",
    "def create_video_capture(source=0):\n",
    "    \"\"\"\n",
    "    Create a video capture object for the specified source.\n",
    "    \n",
    "    Args:\n",
    "        source: Camera index or video file path\n",
    "        \n",
    "    Returns:\n",
    "        cv2.VideoCapture: VideoCapture object\n",
    "    \"\"\"\n",
    "    # Create video capture object\n",
    "    cap = cv2.VideoCapture(source)\n",
    "    \n",
    "    # Check if camera/video opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video source {source}\")\n",
    "        return None\n",
    "    \n",
    "    # Get frame dimensions\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    print(f\"Video source opened: {frame_width}x{frame_height} at {fps} FPS\")\n",
    "    \n",
    "    return cap\n",
    "\n",
    "\n",
    "def process_video_source(source=0, process_frame_func=None, display=True, output_file=None):\n",
    "    \"\"\"\n",
    "    Process frames from a video source (camera or file).\n",
    "    \n",
    "    Args:\n",
    "        source: Camera index or video file path\n",
    "        process_frame_func: Function to process each frame\n",
    "        display: Whether to display the output frame\n",
    "        output_file: Path to save output video (if None, no saving)\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Create video capture\n",
    "    cap = create_video_capture(source)\n",
    "    if cap is None:\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Create video writer if output file is specified\n",
    "    out = None\n",
    "    if output_file is not None:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_file, fourcc, fps, (frame_width, frame_height))\n",
    "    \n",
    "    # Process video frames\n",
    "    frame_count = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while True:\n",
    "        # Read frame\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # Break if end of video or error\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        frame_count += 1\n",
    "        \n",
    "        # Process the frame if a processing function is provided\n",
    "        if process_frame_func is not None:\n",
    "            processed_frame, fall_detected = process_frame_func(frame)\n",
    "        else:\n",
    "            processed_frame = frame\n",
    "            fall_detected = False\n",
    "        \n",
    "        # Write frame to output video if specified\n",
    "        if out is not None:\n",
    "            out.write(processed_frame)\n",
    "        \n",
    "        # Display the frame\n",
    "        if display:\n",
    "            # Add fall detection status to the frame\n",
    "            if fall_detected:\n",
    "                cv2.putText(processed_frame, \"FALL DETECTED\", (50, 50), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            \n",
    "            cv2.imshow('Video Processing', processed_frame)\n",
    "            \n",
    "            # Break if 'q' is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    \n",
    "    # Calculate and print processing stats\n",
    "    elapsed_time = time.time() - start_time\n",
    "    if frame_count > 0 and elapsed_time > 0:\n",
    "        fps_processing = frame_count / elapsed_time\n",
    "        print(f\"Processed {frame_count} frames in {elapsed_time:.2f} seconds ({fps_processing:.2f} FPS)\")\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    if out is not None:\n",
    "        out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Test the video processing pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # This will just display the camera feed without any processing\n",
    "    process_video_source(0, None, True, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2089854b-eaa6-4fb0-b41e-0ac58de8b78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Fall Detection Algorithm - Including Fall vs. Lying Down Differentiation\n",
    "\n",
    "class FallDetector:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the fall detector with tracking for speed calculation.\"\"\"\n",
    "        self.prev_keypoints = None\n",
    "    \n",
    "    def calculate_length_factor(self, keypoints):\n",
    "        \"\"\"\n",
    "        Calculate length factor based on the shoulder-to-torso distance.\n",
    "        \n",
    "        This implements Equation 1 from page 9 of the journal:\n",
    "        Lfactor = √(xl - xTl)² + (yl - yTl)²\n",
    "        \n",
    "        Args:\n",
    "            keypoints (list): List of keypoint tuples (x, y, confidence)\n",
    "            \n",
    "        Returns:\n",
    "            float: Length factor for distance calculations\n",
    "        \"\"\"\n",
    "        # Get left shoulder and left hip (torso) keypoints\n",
    "        left_shoulder = keypoints[KEYPOINT_DICT['left_shoulder']]\n",
    "        left_hip = keypoints[KEYPOINT_DICT['left_hip']]\n",
    "        \n",
    "        # Calculate Euclidean distance as in Equation 1\n",
    "        shoulder_x, shoulder_y = left_shoulder[0], left_shoulder[1]\n",
    "        hip_x, hip_y = left_hip[0], left_hip[1]\n",
    "        \n",
    "        length_factor = math.sqrt((shoulder_x - hip_x)**2 + (shoulder_y - hip_y)**2)\n",
    "        return length_factor\n",
    "    \n",
    "    def calculate_vertical_speed(self, prev_keypoints, curr_keypoints):\n",
    "        \"\"\"\n",
    "        Calculate vertical speed of movement to differentiate falls from lying down.\n",
    "        \n",
    "        As described on page 10: \"The speed of key body points is calculated by\n",
    "        measuring the displacement between their positions in consecutive frames.\"\n",
    "        \n",
    "        Args:\n",
    "            prev_keypoints (list): Previous frame keypoints\n",
    "            curr_keypoints (list): Current frame keypoints\n",
    "            \n",
    "        Returns:\n",
    "            float: Vertical speed (displacement between frames)\n",
    "        \"\"\"\n",
    "        if prev_keypoints is None:\n",
    "            return 0\n",
    "        \n",
    "        # Use shoulders to calculate vertical speed as mentioned in the paper\n",
    "        prev_left_shoulder = prev_keypoints[KEYPOINT_DICT['left_shoulder']]\n",
    "        curr_left_shoulder = curr_keypoints[KEYPOINT_DICT['left_shoulder']]\n",
    "        \n",
    "        # Calculate vertical displacement\n",
    "        vertical_displacement = abs(curr_left_shoulder[1] - prev_left_shoulder[1])\n",
    "        return vertical_displacement\n",
    "    \n",
    "    def calculate_torso_angle(self, keypoints):\n",
    "        \"\"\"\n",
    "        Calculate angle between torso and vertical.\n",
    "        \n",
    "        As mentioned on page 10: \"A threshold of 45 degrees is used in the code.\n",
    "        If the angle between the torso and legs drops below this value, it indicates\n",
    "        that the person's body is approaching a horizontal position...\"\n",
    "        \n",
    "        Args:\n",
    "            keypoints (list): List of keypoint tuples (x, y, confidence)\n",
    "            \n",
    "        Returns:\n",
    "            float: Angle in degrees\n",
    "        \"\"\"\n",
    "        # Get shoulder and hip keypoints to define torso\n",
    "        left_shoulder = keypoints[KEYPOINT_DICT['left_shoulder']]\n",
    "        right_shoulder = keypoints[KEYPOINT_DICT['right_shoulder']]\n",
    "        left_hip = keypoints[KEYPOINT_DICT['left_hip']]\n",
    "        right_hip = keypoints[KEYPOINT_DICT['right_hip']]\n",
    "        \n",
    "        # Calculate midpoints\n",
    "        mid_shoulder_x = (left_shoulder[0] + right_shoulder[0]) / 2\n",
    "        mid_shoulder_y = (left_shoulder[1] + right_shoulder[1]) / 2\n",
    "        mid_hip_x = (left_hip[0] + right_hip[0]) / 2\n",
    "        mid_hip_y = (left_hip[1] + right_hip[1]) / 2\n",
    "        \n",
    "        # Calculate torso vector\n",
    "        torso_x = mid_shoulder_x - mid_hip_x\n",
    "        torso_y = mid_shoulder_y - mid_hip_y\n",
    "        \n",
    "        # Calculate angle with vertical (y-axis in image coordinates)\n",
    "        # Note: In image coordinates, y increases downward\n",
    "        angle_rad = math.atan2(torso_x, torso_y)  # Angle with vertical\n",
    "        angle_deg = math.degrees(abs(angle_rad))\n",
    "        \n",
    "        return angle_deg\n",
    "    \n",
    "    def detect_fall(self, people):\n",
    "        \"\"\"\n",
    "        Detect if a fall has occurred based on pose keypoints.\n",
    "        \n",
    "        This implements the fall detection algorithm described in the paper\n",
    "        on pages 9-10, including the differentiation between falls and lying down.\n",
    "        \n",
    "        Args:\n",
    "            people (list): List of dictionaries containing keypoints and bounding boxes\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if fall detected, False otherwise\n",
    "        \"\"\"\n",
    "        # If no people detected, return False\n",
    "        if not people:\n",
    "            self.prev_keypoints = None\n",
    "            return False\n",
    "        \n",
    "        # Use the first person detected\n",
    "        person = people[0]\n",
    "        keypoints = person['keypoints']\n",
    "        \n",
    "        # Get relevant keypoints\n",
    "        left_shoulder = keypoints[KEYPOINT_DICT['left_shoulder']]\n",
    "        right_shoulder = keypoints[KEYPOINT_DICT['right_shoulder']]\n",
    "        left_hip = keypoints[KEYPOINT_DICT['left_hip']]\n",
    "        right_hip = keypoints[KEYPOINT_DICT['right_hip']]\n",
    "        left_ankle = keypoints[KEYPOINT_DICT['left_ankle']]\n",
    "        right_ankle = keypoints[KEYPOINT_DICT['right_ankle']]\n",
    "        \n",
    "        # 1. Calculate the length factor (Equation 1, page 9)\n",
    "        length_factor = self.calculate_length_factor(keypoints)\n",
    "        \n",
    "        # 2. Check if shoulders are lower than feet with adjustment (Equation 2, page 9)\n",
    "        # yl ≤ yFl + α·Lfactor\n",
    "        # Note: In image coordinates, y increases downward, so we flip the inequality\n",
    "        alpha = 0.1  # Small adjustment factor as mentioned in the paper\n",
    "        shoulder_below_feet = (\n",
    "            left_shoulder[1] >= left_ankle[1] - alpha * length_factor or\n",
    "            right_shoulder[1] >= right_ankle[1] - alpha * length_factor\n",
    "        )\n",
    "        \n",
    "        # 3. Calculate body height and width (Equations 3 & 4, page 10)\n",
    "        # Hbody = |yl - yFl|\n",
    "        body_height = abs(left_shoulder[1] - left_ankle[1])\n",
    "        # Wbody = |xl - xr|\n",
    "        body_width = abs(left_shoulder[0] - right_shoulder[0])\n",
    "        \n",
    "        # 4. Check fall condition based on body dimensions (Equation 5, page 10)\n",
    "        # Hbody < Wbody\n",
    "        orientation_fallen = body_height < body_width\n",
    "        \n",
    "        # 5. Differentiate between fall and lying down (as described on page 10)\n",
    "        # Calculate vertical speed between frames\n",
    "        vertical_speed = 0\n",
    "        if self.prev_keypoints is not None:\n",
    "            vertical_speed = self.calculate_vertical_speed(self.prev_keypoints, keypoints)\n",
    "        \n",
    "        # Calculate torso angle\n",
    "        torso_angle = self.calculate_torso_angle(keypoints)\n",
    "        \n",
    "        # From page 10: \"If the vertical speed exceeds a specific threshold, it indicates a fall...\"\n",
    "        # From page 10: \"A threshold of 45 degrees is used in the code.\"\n",
    "        is_rapid_movement = vertical_speed > 15  # Threshold for rapid movement\n",
    "        is_horizontal_position = torso_angle > 45  # Angle threshold as mentioned in paper\n",
    "        \n",
    "        # Fall is detected if basic conditions are met AND\n",
    "        # either the movement is rapid OR the body is in a horizontal position\n",
    "        fall_detected = (\n",
    "            shoulder_below_feet and \n",
    "            orientation_fallen and \n",
    "            (is_rapid_movement or is_horizontal_position)\n",
    "        )\n",
    "        \n",
    "        # Store current keypoints for next frame's speed calculation\n",
    "        self.prev_keypoints = keypoints\n",
    "        \n",
    "        return fall_detected\n",
    "    \n",
    "    def annotate_frame(self, frame, fall_detected):\n",
    "        \"\"\"\n",
    "        Annotate the frame with fall detection status.\n",
    "        \n",
    "        Args:\n",
    "            frame (numpy.ndarray): Frame to annotate\n",
    "            fall_detected (bool): Whether a fall is detected\n",
    "            \n",
    "        Returns:\n",
    "            numpy.ndarray: Annotated frame\n",
    "        \"\"\"\n",
    "        if fall_detected:\n",
    "            # Draw red text for fall detection\n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                \"FALL DETECTED!\",\n",
    "                (50, 50),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                1,\n",
    "                (0, 0, 255),  # Red color\n",
    "                2\n",
    "            )\n",
    "        \n",
    "        return frame\n",
    "\n",
    "# Function to process a frame with fall detection\n",
    "def process_frame_with_fall_detection(frame, model, device, fall_detector):\n",
    "    \"\"\"\n",
    "    Process a frame with pose estimation and fall detection.\n",
    "    \n",
    "    Args:\n",
    "        frame (numpy.ndarray): Input frame\n",
    "        model: YOLOv7-W6-Pose model\n",
    "        device (torch.device): Device to run inference on\n",
    "        fall_detector (FallDetector): Fall detector instance\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (Processed frame with annotations, fall detection status)\n",
    "    \"\"\"\n",
    "    # Perform pose estimation\n",
    "    frame_with_pose, people = process_frame_with_pose(frame, model, device)\n",
    "    \n",
    "    # Detect falls using the algorithm from the journal\n",
    "    fall_detected = fall_detector.detect_fall(people)\n",
    "    \n",
    "    # Annotate frame with fall detection status\n",
    "    annotated_frame = fall_detector.annotate_frame(frame_with_pose, fall_detected)\n",
    "    \n",
    "    return annotated_frame, fall_detected\n",
    "\n",
    "# Test fall detection if running this cell directly\n",
    "if __name__ == \"__main__\":\n",
    "    # Only import if this file is run directly\n",
    "    from video_processing import process_video_source\n",
    "    \n",
    "    # Load the model\n",
    "    model = load_model(CONFIG['model_path'], CONFIG['device'])\n",
    "    \n",
    "    # Create fall detector\n",
    "    fall_detector = FallDetector()\n",
    "    \n",
    "    # Define a function to process each frame with the model and fall detection\n",
    "    def process_frame_func(frame):\n",
    "        return process_frame_with_fall_detection(frame, model, device, fall_detector)\n",
    "    \n",
    "    # Process video from webcam\n",
    "    process_video_source(0, process_frame_func, True, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8c42ce-d6f2-4a6f-b652-a48b965c0f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Main Program Integration\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function that integrates all components of the fall detection system.\n",
    "    \"\"\"\n",
    "    print(\"Fall Detection System using YOLOv7-W6-Pose\")\n",
    "    print(\"------------------------------------------\")\n",
    "    \n",
    "    # Parse command line arguments\n",
    "    parser = argparse.ArgumentParser(description='Fall Detection System using YOLOv7-W6-Pose')\n",
    "    parser.add_argument('--source', type=str, default='0', \n",
    "                        help='Source for video input (0 for webcam, or path to video file)')\n",
    "    parser.add_argument('--output', type=str, default=None, \n",
    "                        help='Path to save processed video (None for no saving)')\n",
    "    parser.add_argument('--display', action='store_true', \n",
    "                        help='Display video processing in real-time')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Convert source to int if it's a digit (camera index)\n",
    "    if args.source.isdigit():\n",
    "        args.source = int(args.source)\n",
    "    \n",
    "    # Check if model file exists\n",
    "    if not os.path.exists(CONFIG['model_path']):\n",
    "        print(f\"Error: Model file not found at {CONFIG['model_path']}\")\n",
    "        print(\"Please download it from: https://github.com/WongKinYiu/yolov7/releases\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Load YOLOv7-W6-Pose model\n",
    "        print(f\"Loading model from {CONFIG['model_path']}...\")\n",
    "        model = load_model(CONFIG['model_path'], CONFIG['device'])\n",
    "        print(\"Model loaded successfully.\")\n",
    "        \n",
    "        # Create fall detector\n",
    "        fall_detector = FallDetector()\n",
    "        \n",
    "        # Define frame processing function\n",
    "        def process_frame_func(frame):\n",
    "            return process_frame_with_fall_detection(frame, model, CONFIG['device'], fall_detector)\n",
    "        \n",
    "        # Process video source\n",
    "        print(f\"Processing video from source: {args.source}\")\n",
    "        if args.output:\n",
    "            print(f\"Output will be saved to: {args.output}\")\n",
    "        \n",
    "        process_video_source(args.source, process_frame_func, args.display, args.output)\n",
    "        \n",
    "        print(\"Processing completed.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run the main program if this script is executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26604664-5430-4923-a112-43dc4f31b71c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
