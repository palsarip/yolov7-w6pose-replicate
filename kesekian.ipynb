{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b8d85b1-6734-4a2e-8b2f-cacc64429605",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL models.yolo.Model was not an allowed global by default. Please use `torch.serialization.add_safe_globals([Model])` or the `torch.serialization.safe_globals([Model])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Inisialisasi model YOLOv7\u001b[39;00m\n\u001b[0;32m     14\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mattempt_load\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myolov7-w6-pose.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "File \u001b[1;32m~\\yolov7\\models\\experimental.py:252\u001b[0m, in \u001b[0;36mattempt_load\u001b[1;34m(weights, map_location)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m weights \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(weights, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [weights]:\n\u001b[0;32m    251\u001b[0m     attempt_download(w)\n\u001b[1;32m--> 252\u001b[0m     ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load\u001b[39;00m\n\u001b[0;32m    253\u001b[0m     model\u001b[38;5;241m.\u001b[39mappend(ckpt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mema\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mema\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mfuse()\u001b[38;5;241m.\u001b[39meval())  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m# Compatibility updates\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolov7_env\\lib\\site-packages\\torch\\serialization.py:1470\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1462\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1463\u001b[0m                     opened_zipfile,\n\u001b[0;32m   1464\u001b[0m                     map_location,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1467\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1468\u001b[0m                 )\n\u001b[0;32m   1469\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1470\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1471\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[0;32m   1472\u001b[0m             opened_zipfile,\n\u001b[0;32m   1473\u001b[0m             map_location,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1476\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[0;32m   1477\u001b[0m         )\n\u001b[0;32m   1478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[1;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL models.yolo.Model was not an allowed global by default. Please use `torch.serialization.add_safe_globals([Model])` or the `torch.serialization.safe_globals([Model])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from collections import deque\n",
    "\n",
    "# Fungsi tambahan (pastikan fungsi-fungsi ini didefinisikan atau diimpor)\n",
    "from models.experimental import attempt_load\n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.datasets import letterbox\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "\n",
    "# Inisialisasi model YOLOv7\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = attempt_load('yolov7-w6-pose.pt', map_location=device)\n",
    "_ = model.float().eval()\n",
    "if torch.cuda.is_available():\n",
    "    model.half().to(device)\n",
    "\n",
    "class FallDetector:\n",
    "    def __init__(self, fps=30):\n",
    "        self.LEFT_SHOULDER, self.RIGHT_SHOULDER = 5, 6\n",
    "        self.LEFT_HIP, self.RIGHT_HIP = 11, 12\n",
    "        self.LEFT_ANKLE, self.RIGHT_ANKLE = 13, 14\n",
    "        \n",
    "        self.ALPHA = 0.4  # Diperbesar untuk toleransi lebih\n",
    "        self.ANGLE_THRESHOLD = 120  # Diperbarui untuk mendeteksi sudut lebih besar\n",
    "        self.VELOCITY_THRESHOLD = 0.5  # Diperbesar\n",
    "        self.CONFIRMATION_FRAMES = 10  # Diperpanjang untuk mengurangi false positive\n",
    "        \n",
    "        self.previous_positions = deque(maxlen=5)\n",
    "        self.fall_frames = 0\n",
    "        self.is_fallen = False\n",
    "        self.fps = fps\n",
    "\n",
    "    def calculate_length_factor(self, shoulder, hip):\n",
    "        return np.sqrt((shoulder[0] - hip[0])**2 + (shoulder[1] - hip[1])**2)\n",
    "\n",
    "    def calculate_body_dimensions(self, keypoints):\n",
    "        ls = keypoints[self.LEFT_SHOULDER*3:(self.LEFT_SHOULDER+1)*3]\n",
    "        rs = keypoints[self.RIGHT_SHOULDER*3:(self.RIGHT_SHOULDER+1)*3]\n",
    "        la = keypoints[self.LEFT_ANKLE*3:(self.LEFT_ANKLE+1)*3]\n",
    "        \n",
    "        if ls[2] > 0.5 and rs[2] > 0.5 and la[2] > 0.5:\n",
    "            body_height = abs(((ls[1] + rs[1])/2) - la[1])\n",
    "            body_width = abs(ls[0] - rs[0])\n",
    "            return body_height, body_width\n",
    "        return None, None\n",
    "\n",
    "    def calculate_velocity(self, current_pos):\n",
    "        if len(self.previous_positions) >= 2:\n",
    "            prev_pos = self.previous_positions[-1]\n",
    "            dx = current_pos[0] - prev_pos[0]\n",
    "            dy = current_pos[1] - prev_pos[1]\n",
    "            distance = np.sqrt(dx**2 + dy**2)\n",
    "            time_elapsed = 1 / self.fps  # Waktu antara frame\n",
    "            return distance / time_elapsed  # Kecepatan dalam pixel/detik\n",
    "        return 0\n",
    "\n",
    "    def calculate_torso_leg_angle(self, keypoints):\n",
    "        ls = keypoints[self.LEFT_SHOULDER*3:(self.LEFT_SHOULDER+1)*3]\n",
    "        lh = keypoints[self.LEFT_HIP*3:(self.LEFT_HIP+1)*3]\n",
    "        la = keypoints[self.LEFT_ANKLE*3:(self.LEFT_ANKLE+1)*3]\n",
    "        \n",
    "        if all(kp[2] > 0.5 for kp in [ls, lh, la]):\n",
    "            torso_vec = np.array([lh[0] - ls[0], lh[1] - ls[1]])\n",
    "            leg_vec = np.array([la[0] - lh[0], la[1] - lh[1]])\n",
    "            \n",
    "            # Hindari vektor nol\n",
    "            if np.linalg.norm(torso_vec) < 1e-6 or np.linalg.norm(leg_vec) < 1e-6:\n",
    "                return None\n",
    "                \n",
    "            unit_torso = torso_vec / np.linalg.norm(torso_vec)\n",
    "            unit_leg = leg_vec / np.linalg.norm(leg_vec)\n",
    "            angle = np.degrees(np.arccos(np.clip(np.dot(unit_torso, unit_leg), -1.0, 1.0)))\n",
    "            return angle\n",
    "        return None\n",
    "\n",
    "    def detect_fall(self, keypoints):\n",
    "        ls = keypoints[self.LEFT_SHOULDER*3:(self.LEFT_SHOULDER+1)*3]\n",
    "        rs = keypoints[self.RIGHT_SHOULDER*3:(self.RIGHT_SHOULDER+1)*3]\n",
    "        lh = keypoints[self.LEFT_HIP*3:(self.LEFT_HIP+1)*3]\n",
    "        rh = keypoints[self.RIGHT_HIP*3:(self.RIGHT_HIP+1)*3]\n",
    "        la = keypoints[self.LEFT_ANKLE*3:(self.LEFT_ANKLE+1)*3]\n",
    "        ra = keypoints[self.RIGHT_ANKLE*3:(self.RIGHT_ANKLE+1)*3]\n",
    "        \n",
    "        # Validasi keypoints\n",
    "        if any(kp[2] < 0.5 for kp in [ls, rs, lh, rh, la, ra]):\n",
    "            self.fall_frames = max(0, self.fall_frames - 1)\n",
    "            return False\n",
    "        \n",
    "        shoulder_center = ((ls[0] + rs[0])/2, (ls[1] + rs[1])/2)\n",
    "        self.previous_positions.append(shoulder_center)\n",
    "        \n",
    "        length_factor = self.calculate_length_factor(ls, lh)\n",
    "        body_height, body_width = self.calculate_body_dimensions(keypoints)\n",
    "        \n",
    "        # Kondisi ketinggian yang diperbaiki\n",
    "        vertical_diff_left = ls[1] - la[1]\n",
    "        vertical_diff_right = rs[1] - ra[1]\n",
    "        height_condition = (vertical_diff_left > self.ALPHA * length_factor) or \\\n",
    "                          (vertical_diff_right > self.ALPHA * length_factor)\n",
    "        \n",
    "        # Kondisi dimensi lebih toleran\n",
    "        dimension_condition = (body_height is not None) and (body_height < body_width * 1.2)\n",
    "        \n",
    "        velocity = self.calculate_velocity(shoulder_center)\n",
    "        angle = self.calculate_torso_leg_angle(keypoints)\n",
    "        \n",
    "        is_fall = False\n",
    "        if height_condition and dimension_condition:\n",
    "            angle_condition = (angle is not None) and (angle < self.ANGLE_THRESHOLD)\n",
    "            velocity_condition = velocity > self.VELOCITY_THRESHOLD\n",
    "            is_fall = angle_condition or velocity_condition  # Menggunakan OR logic\n",
    "        \n",
    "        if is_fall:\n",
    "            self.fall_frames += 1\n",
    "            if self.fall_frames >= self.CONFIRMATION_FRAMES:\n",
    "                self.is_fallen = True\n",
    "                return True\n",
    "        else:\n",
    "            self.fall_frames = max(0, self.fall_frames - 2)\n",
    "            if self.fall_frames == 0:\n",
    "                self.is_fallen = False\n",
    "        \n",
    "        return self.is_fallen\n",
    "\n",
    "def process_video(video_path, output_path=None, rotation=0):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "    \n",
    "    # Mendapatkan properti video\n",
    "    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Menyesuaikan dimensi untuk rotasi\n",
    "    if rotation in [90, 270]:\n",
    "        width, height = original_height, original_width\n",
    "    else:\n",
    "        width, height = original_width, original_height\n",
    "    \n",
    "    if output_path:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    fall_detector = FallDetector(fps=fps)\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Rotasi frame\n",
    "        if rotation == 90:\n",
    "            frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)\n",
    "        elif rotation == 180:\n",
    "            frame = cv2.rotate(frame, cv2.ROTATE_180)\n",
    "        elif rotation == 270:\n",
    "            frame = cv2.rotate(frame, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "        \n",
    "        # Preprocessing dengan letterbox\n",
    "        # Ganti baris letterbox:\n",
    "        img, (pad_w, pad_h), (ratio_w, ratio_h) = letterbox(frame, new_shape=640, stride=64, auto=True)\n",
    "        img_tensor = transforms.ToTensor()(img)\n",
    "        img_tensor = img_tensor.unsqueeze(0).to(device)\n",
    "        if torch.cuda.is_available():\n",
    "            img_tensor = img_tensor.half()\n",
    "        \n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            output, _ = model(img_tensor)\n",
    "            output = non_max_suppression_kpt(output, 0.25, 0.65, nc=model.yaml['nc'], nkpt=model.yaml['nkpt'], kpt_label=True)\n",
    "            output = output_to_keypoint(output)\n",
    "        \n",
    "        # Menyiapkan frame output\n",
    "        display_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Memproses setiap orang yang terdeteksi\n",
    "        if output is not None and len(output) > 0:\n",
    "            for idx in range(output.shape[0]):\n",
    "                keypoints = output[idx, 7:].T\n",
    "                \n",
    "                # Menyesuaikan keypoints ke koordinat asli\n",
    "                keypoints[0::3] = (keypoints[0::3] - pad_w) / ratio_w  # X\n",
    "                keypoints[1::3] = (keypoints[1::3] - pad_h) / ratio_h  # Y\n",
    "                \n",
    "                # Plot skeleton\n",
    "                plot_skeleton_kpts(display_img, keypoints, 3)\n",
    "                \n",
    "                # Deteksi jatuh\n",
    "                if fall_detector.detect_fall(keypoints):\n",
    "                    cv2.putText(display_img, \"FALL DETECTED!\", (50, 80), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 3)\n",
    "                    \n",
    "                    # Menampilkan metrik\n",
    "                    body_height, body_width = fall_detector.calculate_body_dimensions(keypoints)\n",
    "                    angle = fall_detector.calculate_torso_leg_angle(keypoints)\n",
    "                    \n",
    "                    cv2.putText(display_img, f\"H/W Ratio: {body_height/body_width:.2f}\", (50, 120),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)\n",
    "                    cv2.putText(display_img, f\"Angle: {angle:.1f}Â°\" if angle else \"Angle: N/A\", (50, 150),\n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)\n",
    "        \n",
    "        # Konversi ke BGR untuk output\n",
    "        display_img = cv2.cvtColor(display_img, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Menampilkan dan menyimpan\n",
    "        cv2.imshow(\"Fall Detection\", display_img)\n",
    "        if output_path:\n",
    "            out.write(display_img)\n",
    "            \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    if output_path:\n",
    "        out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Contoh penggunaan\n",
    "process_video('C:/Users/LENOVO/Documents/A Skripsi/datasets/FallDataset/Dataset/Coffee_room_01/Videos/video (1).avi', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7f984a3-2809-4125-9347-4b1028a3d93b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "trailing comma not allowed without surrounding parentheses (741100123.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 9\u001b[1;36m\u001b[0m\n\u001b[1;33m    from utils.general import non_max_suppression_kpt,\u001b[0m\n\u001b[1;37m                                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m trailing comma not allowed without surrounding parentheses\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea062e8-080b-43fc-9c06-95ce68d27fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
