{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e160714f-ba4d-4ed7-93e3-8e19246b81f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cuDNN error: CUDNN_STATUS_INTERNAL_ERROR",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 241\u001b[0m\n\u001b[0;32m    238\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n\u001b[0;32m    240\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:/Users/LENOVO/Documents/A Skripsi/datasets/FallDataset/Dataset/Coffee_room_01/Videos/video (1).avi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrotation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m90\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 196\u001b[0m, in \u001b[0;36mprocess_video\u001b[1;34m(video_path, output_path, rotation)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 196\u001b[0m     output, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m     output \u001b[38;5;241m=\u001b[39m non_max_suppression_kpt(\n\u001b[0;32m    198\u001b[0m         output, \n\u001b[0;32m    199\u001b[0m         \u001b[38;5;241m0.25\u001b[39m, \u001b[38;5;241m0.65\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    202\u001b[0m         kpt_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    203\u001b[0m     )\n\u001b[0;32m    204\u001b[0m     output \u001b[38;5;241m=\u001b[39m output_to_keypoint(output)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolov7_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\yolov7\\models\\yolo.py:599\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, x, augment, profile)\u001b[0m\n\u001b[0;32m    597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# augmented inference, train\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\yolov7\\models\\yolo.py:625\u001b[0m, in \u001b[0;36mModel.forward_once\u001b[1;34m(self, x, profile)\u001b[0m\n\u001b[0;32m    622\u001b[0m         dt\u001b[38;5;241m.\u001b[39mappend((time_synchronized() \u001b[38;5;241m-\u001b[39m t) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m    623\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%10.1f\u001b[39;00m\u001b[38;5;132;01m%10.0f\u001b[39;00m\u001b[38;5;132;01m%10.1f\u001b[39;00m\u001b[38;5;124mms \u001b[39m\u001b[38;5;132;01m%-40s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (o, m\u001b[38;5;241m.\u001b[39mnp, dt[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], m\u001b[38;5;241m.\u001b[39mtype))\n\u001b[1;32m--> 625\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolov7_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\yolov7\\models\\common.py:108\u001b[0m, in \u001b[0;36mConv.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolov7_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolov7_env\\lib\\site-packages\\torch\\nn\\modules\\conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\yolov7_env\\lib\\site-packages\\torch\\nn\\modules\\conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    451\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    452\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from collections import deque\n",
    "from models.experimental import attempt_load\n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.datasets import letterbox\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = torch.load('yolov7-w6-pose.pt', map_location=device)\n",
    "model = weights['model'].float().eval()\n",
    "if torch.cuda.is_available():\n",
    "    model.half().to(device)\n",
    "\n",
    "class FallDetector:\n",
    "    def __init__(self, fps=30):\n",
    "        self.KEYPOINT_IDS = {\n",
    "            'LEFT_SHOULDER': 5, \n",
    "            'RIGHT_SHOULDER': 6,\n",
    "            'LEFT_HIP': 11,\n",
    "            'RIGHT_HIP': 12,\n",
    "            'LEFT_ANKLE': 13,\n",
    "            'RIGHT_ANKLE': 14\n",
    "        }\n",
    "        \n",
    "        self.THRESHOLDS = {\n",
    "            'ALPHA': 0.4,\n",
    "            'ANGLE': 120,\n",
    "            'VELOCITY': 0.5,\n",
    "            'CONFIRMATION_FRAMES': 10\n",
    "        }\n",
    "        \n",
    "        self.previous_positions = deque(maxlen=5)\n",
    "        self.fall_frames = 0\n",
    "        self.is_fallen = False\n",
    "        self.fps = fps\n",
    "\n",
    "    def get_valid_keypoints(self, keypoints):\n",
    "        \"\"\"Filter and validate keypoints with proper confidence\"\"\"\n",
    "        MIN_CONFIDENCE = 0.5\n",
    "        SAFE_COORD_THRESH = 1e6\n",
    "        kps = []\n",
    "        for i in range(0, len(keypoints), 3):\n",
    "            x, y, conf = keypoints[i:i+3]\n",
    "            if conf > MIN_CONFIDENCE and abs(x) < SAFE_COORD_THRESH and abs(y) < SAFE_COORD_THRESH:\n",
    "                kps.extend([x, y, conf])\n",
    "            else:\n",
    "                kps.extend([0.0, 0.0, 0.0])\n",
    "        return np.array(kps)\n",
    "\n",
    "    def calculate_length_factor(self, shoulder, hip):\n",
    "        EPSILON = 1e-6\n",
    "        return np.sqrt((shoulder[0] - hip[0])**2 + (shoulder[1] - hip[1])**2 + EPSILON)\n",
    "\n",
    "    def calculate_body_dimensions(self, keypoints):\n",
    "        MIN_CONFIDENCE = 0.5\n",
    "        EPSILON = 1e-6\n",
    "        ls = keypoints[self.KEYPOINT_IDS['LEFT_SHOULDER']*3:(self.KEYPOINT_IDS['LEFT_SHOULDER']+1)*3]\n",
    "        rs = keypoints[self.KEYPOINT_IDS['RIGHT_SHOULDER']*3:(self.KEYPOINT_IDS['RIGHT_SHOULDER']+1)*3]\n",
    "        la = keypoints[self.KEYPOINT_IDS['LEFT_ANKLE']*3:(self.KEYPOINT_IDS['LEFT_ANKLE']+1)*3]\n",
    "        \n",
    "        if all(kp[2] > MIN_CONFIDENCE for kp in [ls, rs, la]):\n",
    "            body_height = abs(((ls[1] + rs[1])/2) - la[1])\n",
    "            body_width = abs(ls[0] - rs[0])\n",
    "            return max(body_height, EPSILON), max(body_width, EPSILON)\n",
    "        return None, None\n",
    "\n",
    "    # Rest of the methods remain with consistent 4-space indentation...\n",
    "\n",
    "    def calculate_velocity(self, current_pos):\n",
    "        if len(self.previous_positions) >= 2:\n",
    "            prev_pos = self.previous_positions[-1]\n",
    "            dx = current_pos[0] - prev_pos[0]\n",
    "            dy = current_pos[1] - prev_pos[1]\n",
    "            distance = np.sqrt(dx**2 + dy**2 + EPSILON)\n",
    "            return distance * self.fps  # pixels/second\n",
    "        return 0\n",
    "\n",
    "    def calculate_torso_leg_angle(self, keypoints):\n",
    "        ls = keypoints[self.KEYPOINT_IDS['LEFT_SHOULDER']*3:(self.KEYPOINT_IDS['LEFT_SHOULDER']+1)*3]\n",
    "        lh = keypoints[self.KEYPOINT_IDS['LEFT_HIP']*3:(self.KEYPOINT_IDS['LEFT_HIP']+1)*3]\n",
    "        la = keypoints[self.KEYPOINT_IDS['LEFT_ANKLE']*3:(self.KEYPOINT_IDS['LEFT_ANKLE']+1)*3]\n",
    "        \n",
    "        if all(kp[2] > MIN_CONFIDENCE for kp in [ls, lh, la]):\n",
    "            torso_vec = np.array([lh[0] - ls[0], lh[1] - ls[1]])\n",
    "            leg_vec = np.array([la[0] - lh[0], la[1] - lh[1]])\n",
    "            \n",
    "            if np.linalg.norm(torso_vec) < EPSILON or np.linalg.norm(leg_vec) < EPSILON:\n",
    "                return None\n",
    "                \n",
    "            unit_torso = torso_vec / np.linalg.norm(torso_vec)\n",
    "            unit_leg = leg_vec / np.linalg.norm(leg_vec)\n",
    "            angle = np.degrees(np.arccos(np.clip(np.dot(unit_torso, unit_leg), -1.0, 1.0)))\n",
    "            return angle\n",
    "        return None\n",
    "\n",
    "    def detect_fall(self, keypoints):\n",
    "        keypoints = self.get_valid_keypoints(keypoints)\n",
    "        \n",
    "        # Get required keypoints with validation\n",
    "        required_kps = [\n",
    "            keypoints[self.KEYPOINT_IDS[k]*3:(self.KEYPOINT_IDS[k]+1)*3] \n",
    "            for k in ['LEFT_SHOULDER', 'RIGHT_SHOULDER', \n",
    "                    'LEFT_HIP', 'RIGHT_HIP',\n",
    "                    'LEFT_ANKLE', 'RIGHT_ANKLE']\n",
    "        ]\n",
    "        \n",
    "        if any(kp[2] < MIN_CONFIDENCE for kp in required_kps):\n",
    "            self.fall_frames = max(0, self.fall_frames - 1)\n",
    "            return False\n",
    "        \n",
    "        ls, rs, lh, rh, la, ra = required_kps\n",
    "        shoulder_center = ((ls[0] + rs[0])/2, (ls[1] + rs[1])/2)\n",
    "        self.previous_positions.append(shoulder_center)\n",
    "        \n",
    "        length_factor = self.calculate_length_factor(ls, lh)\n",
    "        body_height, body_width = self.calculate_body_dimensions(keypoints)\n",
    "        \n",
    "        vertical_diff_left = abs(ls[1] - la[1])\n",
    "        vertical_diff_right = abs(rs[1] - ra[1])\n",
    "        height_condition = (vertical_diff_left > self.THRESHOLDS['ALPHA'] * length_factor) or \\\n",
    "                          (vertical_diff_right > self.THRESHOLDS['ALPHA'] * length_factor)\n",
    "        \n",
    "        dimension_condition = (body_height is not None) and (body_height < body_width * 1.2)\n",
    "        \n",
    "        velocity = self.calculate_velocity(shoulder_center)\n",
    "        angle = self.calculate_torso_leg_angle(keypoints)\n",
    "        \n",
    "        is_fall = False\n",
    "        if height_condition and dimension_condition:\n",
    "            angle_condition = (angle is not None) and (angle < self.THRESHOLDS['ANGLE'])\n",
    "            velocity_condition = velocity > self.THRESHOLDS['VELOCITY']\n",
    "            is_fall = angle_condition or velocity_condition\n",
    "        \n",
    "        if is_fall:\n",
    "            self.fall_frames += 1\n",
    "            if self.fall_frames >= self.THRESHOLDS['CONFIRMATION_FRAMES']:\n",
    "                self.is_fallen = True\n",
    "                return True\n",
    "        else:\n",
    "            self.fall_frames = max(0, self.fall_frames - 2)\n",
    "            if self.fall_frames == 0:\n",
    "                self.is_fallen = False\n",
    "        \n",
    "        return self.is_fallen\n",
    "\n",
    "def process_video(video_path, output_path=None, rotation=0):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error opening video file\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Rotation handling\n",
    "    if rotation in [90, 270]:\n",
    "        width, height = original_height, original_width\n",
    "    else:\n",
    "        width, height = original_width, original_height\n",
    "    \n",
    "    # Video writer setup\n",
    "    if output_path:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    fall_detector = FallDetector(fps=fps)\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret or frame is None:\n",
    "            break\n",
    "        \n",
    "        # Rotate frame if needed\n",
    "        if rotation == 90:\n",
    "            frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)\n",
    "        elif rotation == 180:\n",
    "            frame = cv2.rotate(frame, cv2.ROTATE_180)\n",
    "        elif rotation == 270:\n",
    "            frame = cv2.rotate(frame, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "        \n",
    "        # Preprocess frame (same as photo example)\n",
    "        img = letterbox(frame, 960, stride=64, auto=True)[0]\n",
    "        img_tensor = transforms.ToTensor()(img)\n",
    "        img_tensor = img_tensor.unsqueeze(0)\n",
    "        if torch.cuda.is_available():\n",
    "            img_tensor = img_tensor.half().to(device)\n",
    "        \n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            output, _ = model(img_tensor)\n",
    "            output = non_max_suppression_kpt(\n",
    "                output, \n",
    "                0.25, 0.65, \n",
    "                nc=model.yaml['nc'], \n",
    "                nkpt=model.yaml['nkpt'], \n",
    "                kpt_label=True\n",
    "            )\n",
    "            output = output_to_keypoint(output)\n",
    "        \n",
    "        # Prepare display image (same as photo example)\n",
    "        display_img = img_tensor[0].permute(1, 2, 0).cpu().numpy()\n",
    "        display_img = (display_img * 255).astype(np.uint8)\n",
    "        display_img = cv2.cvtColor(display_img, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Process detections\n",
    "        if output is not None and len(output) > 0:\n",
    "            for idx in range(output.shape[0]):\n",
    "                keypoints = output[idx, 7:].T\n",
    "                \n",
    "                # Plot skeleton directly on processed image\n",
    "                plot_skeleton_kpts(display_img, keypoints, 3)\n",
    "                \n",
    "                # Fall detection logic\n",
    "                if fall_detector.detect_fall(keypoints):\n",
    "                    cv2.putText(display_img, \"FALL DETECTED!\", (50, 80), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 3)\n",
    "        \n",
    "        # Resize back to original dimensions\n",
    "        display_img = cv2.resize(display_img, (width, height))\n",
    "        \n",
    "        # Show and save results\n",
    "        cv2.imshow(\"Fall Detection\", display_img)\n",
    "        if output_path:\n",
    "            out.write(display_img)\n",
    "            \n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    if output_path:\n",
    "        out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "process_video('C:/Users/LENOVO/Documents/A Skripsi/datasets/FallDataset/Dataset/Coffee_room_01/Videos/video (1).avi', rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb7d248-15c0-4a6b-9638-4f1039785520",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
